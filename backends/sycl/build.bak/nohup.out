UpdateCTestConfiguration  from :/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/DartConfiguration.tcl
UpdateCTestConfiguration  from :/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/DartConfiguration.tcl
Test project /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build
Constructing a list of tests
Done constructing a list of tests
Updating test list for fixtures
Added 0 tests to meet fixture requirements
Checking test dependency graph...
Checking test dependency graph end
test 5
    Start 5: test_elementwise_mul_op

5: Test command: /usr/local/bin/cmake "-E" "env" "CUSTOM_DEVICE_ROOT=/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/python/paddle_custom_device/" "PYTHONPATH=/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/../../Paddle:/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/../../Paddle/test:/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/../../Paddle/test/legacy_test:/home/mlx/repos/tvm-sycl/python:/usr/local/:" "python" "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/../../Paddle/tools/test_runner.py" "test_elementwise_mul_op"
5: Working Directory: /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/tests/unittests
5: Test timeout computed to be: 10000000
5: I0722 17:16:35.892511 3402940 init.cc:233] ENV [CUSTOM_DEVICE_ROOT]=/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/python/paddle_custom_device/
5: I0722 17:16:35.892558 3402940 init.cc:142] Try loading custom device libs from: [/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/python/paddle_custom_device/]
5: I0722 17:16:35.933986 3402940 custom_device.cc:1108] Successed in loading custom runtime in lib: /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/python/paddle_custom_device//libpaddle-custom-sycl.so
5: I0722 17:16:35.934029 3402940 custom_kernel.cc:39] No custom kernel info found in loaded lib(s).
5: I0722 17:16:35.934034 3402940 init.cc:154] Finished in LoadCustomDevice with libs_path: [/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/python/paddle_custom_device/]
5: I0722 17:16:35.934048 3402940 init.cc:239] CustomDevice: SYCL, visible devices count: 4
5: I0722 17:16:36.312711 3402940 program_interpreter.cc:212] New Executor is Running.
5: I0722 17:16:36.613929 3402940 interpreter_util.cc:624] Standalone Executor is Used.
5: /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py:2910: ComplexWarning: Casting complex values to real discards the imaginary part
5:   return (
5: /home/mlx/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:127: RuntimeWarning: invalid value encountered in reduce
5:   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
5: /home/mlx/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide
5:   ret = ret.dtype.type(ret / rcount)
5: W0722 17:16:37.081701 3402940 tensor.cc:399] The `is_initialized` method is deprecated since version 2.3, and will be removed in version 2.4! Please use `initialized` method instead.
5: custom_cpu plugin compiled with clang
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.108383 3402940 pass.cc:38] --- detected [0] subgraphs!
5: I0722 17:16:37.108604 3402940 pir_interpreter.cc:1198] New Executor is Running ...
5: I0722 17:16:37.108739 3402940 pir_interpreter.cc:1225] pir interpreter is running by multi-thread mode ...
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.110039 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.122839 3402940 pass.cc:38] --- detected [0] subgraphs!
5: I0722 17:16:37.122980 3402940 pir_interpreter.cc:1281] New Executor is Running ...
5: I0722 17:16:37.123137 3402940 pir_interpreter.cc:1308] pir interpreter is running by multi-thread mode ...
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.124094 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.140638 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.193662 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.194900 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.205876 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.207072 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.224659 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.336165 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.337410 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.353595 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.354833 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.541254 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.542572 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.555822 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:37.557025 3402940 pass.cc:38] --- detected [0] subgraphs!
5: /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py:317: RuntimeWarning: invalid value encountered in scalar subtract
5:   df_over_dr = y_pos - y_neg
5: /home/mlx/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:127: RuntimeWarning: invalid value encountered in reduce
5:   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.385418 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.386715 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.400707 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.402031 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.440960 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.597227 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.598532 3402940 pass.cc:38] --- detected [1] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.613226 3402940 pass.cc:38] --- detected [0] subgraphs!
5: [1m[35m--- Running PIR pass [inplace_pass][0m
5: I0722 17:16:39.614377 3402940 pass.cc:38] --- detected [0] subgraphs!
5: W0722 17:16:39.720496 3402940 tensor.cc:64] The Tensor(place) constructor is deprecated since version 2.3, and will be removed in version 2.4! Please use `paddle::empty/full` method to create a new Tensor instead. Reason: A legal tensor cannot be constructed only based on the `place`, and datatype, shape, layout, etc. is also required.
5: /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py:2910: ComplexWarning: Casting complex values to real discards the imaginary part
5:   return (
5: test_elementwise_mul_op failed
5:  FFFFFFFFssssFFFFFFFFssssFFFF..FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF.FFFFFFFFFFF
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.ElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[4.11244346e-294 8.30987220e-246 5.63268755e-241 3.72097437e-294
5:   4.12005877e-294 8.30987202e-246 7.85987587e-251 3.72097445e-294
5:   3.72097135e-294 1.22416778e-250 5.67658968e-299 5.67775187e-299
5:   5.37794075e-299 5.37794075e-299 5.37794077e-299 5.68829665e-299
5:   5.37794075e-299]
5:  [5.37794075e-299 5.37794531e-299 3.52448725e-294 5.37794075e-299
5:   5.37794075e-299 5.37910733e-299 8.20608635e-304 9.98049563e-217
5:   1.08808896e-245 3.73555737e-294 2.81776901e-202 2.81327780e-202
5:   5.43821042e-246 5.68584529e-294 8.30987220e-246 8.30982838e-246
5:   3.72100433e-294]
5:  [2.58656327e-231 5.37794075e-299 8.36556460e-299 1.60176989e-284
5:   5.25923086e-241 1.22246280e-250 5.25114441e-241 2.52644252e-284
5:   8.30987220e-246 1.22433826e-250 8.30152017e-251 1.22416778e-250
5:   5.63414935e-241 5.37910735e-299 5.68829665e-299 1.22416778e-250
5:   1.22416110e-250]
5:  [5.37794549e-299 3.52451712e-294 1.22416778e-250 1.22245614e-250
5:   5.37915308e-299 1.22416778e-250 1.22416776e-250 5.37794077e-299
5:   5.67546428e-299 1.22416776e-250 1.22245614e-250 5.37794529e-299
5:   3.32876768e-294 5.38960655e-299 7.85986921e-251 5.37910295e-299
5:   1.40157839e-309]
5:  [1.76692488e-284 4.11394851e-294 1.60342072e-284 2.81776901e-202
5:   2.80829420e-202 3.72250363e-294 5.68584529e-294 8.30987202e-246
5:   8.30982841e-246 4.11245245e-294 2.58656327e-231 3.72097434e-294
5:   5.43821886e-246 1.77088325e-284 8.30987219e-246 5.37794075e-299
5:   1.22262866e-250]
5:  [2.52644252e-284 3.72097434e-294 8.29865477e-246 3.72253339e-294
5:   8.30987220e-246 1.22416778e-250 3.72097435e-294 3.72864951e-294
5:   8.30982838e-246 5.37794075e-299 5.67775625e-299 3.52448725e-294
5:   1.22416776e-250 5.37794075e-299 5.37910733e-299 5.37794075e-299
5:   5.37794075e-299]
5:  [5.37794077e-299 5.67541853e-299 5.37794075e-299 5.37794075e-299
5:   5.37794529e-299 3.32876768e-294 5.37794075e-299 5.37794075e-299
5:   5.37910277e-299 7.75039123e-304 1.76692744e-284 1.76692743e-284
5:   1.59880561e-284 4.77602066e-217 8.64553060e-217 3.72250359e-294
5:   3.72097434e-294]
5:  [8.30987220e-246 8.30982841e-246 6.01034285e-246 8.30987220e-246
5:   5.37794533e-299 3.72097437e-294 4.11164595e-294 1.22416776e-250
5:   7.85986921e-251 1.31162746e-250 5.88233238e-294 3.72097434e-294
5:   7.86158086e-251 8.29869420e-246 1.22416778e-250 1.22416778e-250
5:   5.37910735e-299]
5:  [3.72020682e-294 8.37605013e-299 5.37794075e-299 5.37794531e-299
5:   3.52448725e-294 1.22416778e-250 5.37794075e-299 5.37910733e-299
5:   5.37794075e-299 1.22416778e-250 1.22416776e-250 5.68829665e-299
5:   1.22416778e-250 1.22416778e-250 7.85987585e-251 3.32803314e-294
5:   1.22416776e-250]
5:  [5.37794075e-299 5.37910276e-299 7.75039150e-304 2.81776304e-202
5:   4.76362001e-217 3.73555737e-294 2.81776901e-202 2.81624071e-202
5:   4.11241370e-294 6.28298185e-294 2.58656324e-231 2.33901121e-231
5:   1.59816305e-284 2.58656327e-231 5.37910735e-299 5.67776538e-299
5:   1.60176989e-284]
5:  [3.81573681e-236 5.25773189e-241 1.26661525e-250 2.52644252e-284
5:   5.43821010e-246 8.29804342e-251 5.68013060e-299 1.22416778e-250
5:   5.63414935e-241 8.01261033e-246 4.11932123e-294 1.22416778e-250
5:   5.37794075e-299 8.29634509e-251 5.48935819e-294 5.63268480e-241
5:   7.85986921e-251]
5:  [5.37915308e-299 1.22416778e-250 1.22245614e-250 5.37794077e-299
5:   5.68825108e-299 5.37794075e-299 5.37794075e-299 5.37794531e-299
5:   3.52448725e-294 5.37794075e-299 5.37794075e-299 5.37910277e-299
5:   8.20608635e-304 4.76362696e-217 1.76692743e-284 1.60342072e-284
5:   2.81776899e-202]
5:  [2.42655064e-202 3.72250363e-294 5.68584529e-294 8.30982838e-246
5:   8.30982841e-246 4.11245245e-294 2.58656327e-231 1.76692743e-284
5:   3.72250940e-294 1.76692744e-284 1.76692744e-284 5.37794075e-299
5:   7.86159415e-251 2.52644252e-284 3.72097434e-294 3.72097434e-294
5:   3.72253339e-294]]
5:  eager grad out tensor:
5: [[6.93786520e-310 6.93786520e-310 4.04313523e-316 4.04313523e-316
5:   1.06097757e-153 3.17095867e+180 2.97329889e+222 1.77789214e-286
5:   7.49778955e+247 2.35346129e+251 1.33856863e-152 7.22759583e+159
5:   6.23102990e+228 1.12592220e-153 4.90606260e-109 1.73723187e+097
5:   1.10639874e+200]
5:  [4.24819624e+180 1.05394595e-153 4.78206978e+180 1.46923448e+195
5:   7.66632853e+170 2.58643860e+185 1.27275809e+232 4.82407130e+228
5:   8.78401613e+247 3.98271083e+252 8.04430783e-095 1.46923443e+195
5:   2.32111980e-152 1.06083185e-153 1.89122297e+219 4.72209826e+164
5:   1.39806869e-152]
5:  [6.41013208e+029 1.45516433e-152 1.02122917e+277 7.20365585e+159
5:   1.17522886e+214 4.78104882e+180 1.45517163e-152 6.23537914e+228
5:   4.34471890e-153 1.69374419e+190 1.13556098e-153 1.27989333e-152
5:   4.34458453e-153 1.17549387e+214 1.27734658e-152 3.81388253e+180
5:   2.64521041e+185]
5:  [6.24024538e-085 5.84567677e+199 3.66550810e+098 5.29593218e+005
5:   5.84564809e+199 3.66550810e+098 1.10639874e+200 4.24819624e+180
5:   2.35357671e-085 1.46923443e+195 1.14073631e+243 1.32641274e+272
5:   2.35566767e+251 1.35507324e+248 5.91841500e-085 8.88254272e+247
5:   1.80339693e+044]
5:  [8.65062045e+227 4.71592278e+257 1.46921881e+195 6.02202246e+175
5:   1.10638926e+200 4.24819624e+180 1.05394595e-153 5.09225095e-294
5:   1.86441711e+232 1.06261704e+248 4.90606260e-109 1.26268739e+232
5:   1.06261704e+248 2.19995456e-152 1.71870550e+161 1.06038174e-153
5:   4.78206978e+180]
5:  [1.46923448e+195 4.44391185e+252 3.71260714e+160 3.75638152e+193
5:   1.17567347e+214 1.99329722e+161 9.07652283e+223 8.01068654e-095
5:   2.79983003e+275 2.07155585e+161 1.85815503e+142 5.84567678e+199
5:   3.66550810e+098 5.29593218e+005 5.84564809e+199 3.66550810e+098
5:   1.10639874e+200]
5:  [4.24819624e+180 1.47900242e-076 7.49779493e+247 3.17095867e+180
5:   9.75388087e+199 5.21411322e+170 1.69600772e+161 1.17567344e+214
5:   1.99329722e+161 9.07652283e+223 8.01068654e-095 8.18036199e-096
5:   8.64852963e-096 9.11669726e-096 9.75219993e+140 2.31634000e-152
5:   2.87302965e+161]
5:  [1.14073631e+243 5.28961029e+180 2.45944392e+198 4.53685210e+217
5:   1.23063808e+171 1.48511578e+161 2.87521412e+161 3.17095813e+180
5:   1.42609241e+248 1.28733234e+248 1.06263900e+248 1.27966001e-152
5:   5.14476675e+058 1.27734658e-152 1.94809318e+227 4.91461491e+252
5:   1.01158192e-153]
5:  [1.33261100e+228 8.15765354e+140 3.26121667e+199 1.05117824e-153
5:   1.46923446e+195 2.97327062e+222 4.44391185e+252 7.86131616e-085
5:   5.99236710e+175 1.17939265e+195 5.03938766e+180 9.48962472e-154
5:   1.17718562e+195 5.03938766e+180 6.62907950e+265 2.16209963e+233
5:   4.57695907e-072]
5:  [7.49779493e+247 3.17095867e+180 1.95086931e+227 4.44388389e+252
5:   7.86131616e-085 6.01718273e+175 1.46923443e+195 4.26957803e+257
5:   9.04115219e+271 4.73673866e+164 1.13556095e-153 1.26972371e+161
5:   6.02669629e+175 2.28395794e-308 1.53044315e-094 6.52016598e+252
5:   3.15474906e+180]
5:  [1.95086931e+227 1.24929172e+243 8.02628991e+165 1.27734658e-152
5:   4.72176120e+257 1.43979929e+214 2.35052608e+180 4.24359863e+175
5:   4.91437019e+252 1.05133125e-153 1.91077769e+214 1.67521283e+243
5:   1.91297081e+214 1.06399913e+248 4.86781246e-144 1.67439040e+243
5:   1.41590979e+195]
5:  [6.11168448e-114 1.72608387e+097 5.62630908e+175 6.50944291e+252
5:   4.73787217e+164 1.39806869e-152 1.10639944e+200 3.10720590e+169
5:   9.17708809e+170 3.44999157e-085 2.43861180e-152 4.23275990e+175
5:   5.03582581e+175 4.72883456e+164 5.16410254e-301 7.72197901e+228
5:   2.57832314e-301]
5:  [3.57458838e+252 4.41622066e+257 1.14073631e+243 5.28961029e+180
5:   2.35567927e+251 2.66020411e+233 7.29530117e+175 2.35345430e+251
5:   5.36566153e+199 7.22759543e+159 3.17095867e+180 8.80091454e+199
5:   6.47753011e+170 1.27423026e+161 3.59021609e+246 1.30450797e+243
5:   2.05080770e+044]]
5: 
5: Mismatched elements: 165 / 221 (74.7%)
5: Max absolute difference among violations: 1.02122917e+277
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[4.112443e-294, 8.309872e-246, 5.632688e-241, 3.720974e-294,
5:         4.120059e-294, 8.309872e-246, 7.859876e-251, 3.720974e-294,
5:         3.720971e-294, 1.224168e-250, 5.676590e-299, 5.677752e-299,...
5:  DESIRED: array([[6.937865e-310, 6.937865e-310, 4.043135e-316, 4.043135e-316,
5:         1.060978e-153, 3.170959e+180, 2.973299e+222, 1.777892e-286,
5:         7.497790e+247, 2.353461e+251, 1.338569e-152, 7.227596e+159,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.ElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[1.63608372e-311 3.72097434e-294 3.72097132e-294 4.77830978e-299
5:   4.18837432e-309 3.72097434e-294 3.72020082e-294 6.39095203e-314
5:   3.72097434e-294 3.72097434e-294 3.72250938e-294 1.76626814e-284
5:   3.72097434e-294 3.72097434e-294 3.72097434e-294 3.72097434e-294
5:   3.72097434e-294]
5:  [3.72097434e-294 3.71868076e-294 3.72097434e-294 3.72097434e-294
5:   3.72097431e-294 3.13381565e-294 3.72097434e-294 3.72097434e-294
5:   3.72096539e-294 4.18837432e-309 3.72097434e-294 3.72097434e-294
5:   3.71868076e-294 3.72097434e-294 3.72097434e-294 3.72097132e-294
5:   3.52525478e-294]
5:  [3.72097434e-294 3.72097434e-294 3.71867180e-294 4.18837432e-309
5:   3.72097434e-294 3.72097132e-294 4.77832345e-299 3.72097434e-294
5:   3.72097434e-294 3.72020082e-294 1.63608372e-311 3.72097434e-294
5:   3.72097433e-294 3.52295223e-294 4.18837432e-309 3.72097434e-294
5:   3.72097132e-294]
5:  [4.77832345e-299 3.72097434e-294 3.72097434e-294 3.72020082e-294
5:   1.63608372e-311 3.72097434e-294 3.72097433e-294 3.52295223e-294
5:   4.18837432e-309 3.72097434e-294 3.72020679e-294 4.77832345e-299
5:   3.72097434e-294 4.11394853e-294 4.11241947e-294 1.76626814e-284
5:   3.72097437e-294]
5:  [3.72098034e-294 3.72097434e-294 3.72097434e-294 3.72097434e-294
5:   3.72097434e-294 3.71868076e-294 3.72097434e-294 3.72097434e-294
5:   3.72097132e-294 4.78182319e-299 3.72097434e-294 3.72097434e-294
5:   3.72020086e-294 4.18837432e-309 3.72097434e-294 3.72097433e-294
5:   3.13152206e-294]
5:  [3.72097434e-294 3.72097434e-294 3.72097132e-294 4.78182319e-299
5:   3.72097434e-294 3.72097433e-294 3.52295223e-294 4.18837432e-309
5:   3.72097434e-294 3.72097433e-294 3.52296119e-294 3.72097434e-294
5:   3.72097433e-294 3.52295220e-294 1.63608372e-311 3.72097434e-294
5:   3.72097132e-294]
5:  [4.77830978e-299 4.18837432e-309 3.72097434e-294 3.72020082e-294
5:   6.39095203e-314 3.72097434e-294 3.72097433e-294 3.52295220e-294
5:   1.63608372e-311 3.72097434e-294 3.72096535e-294 2.49646564e-316
5:   4.18837432e-309 3.72097434e-294 3.71867176e-294 6.39095203e-314
5:   3.72097434e-294]
5:  [3.72250940e-294 4.11394853e-294 1.59814631e-284 3.72097434e-294
5:   3.72097434e-294 4.11241049e-294 4.18837432e-309 3.72097434e-294
5:   3.72097434e-294 3.71868076e-294 3.72097434e-294 3.72097434e-294
5:   3.72097431e-294 4.78182319e-299 3.72097434e-294 3.72097434e-294
5:   3.72020086e-294]
5:  [4.18837432e-309 3.72097434e-294 3.72097433e-294 3.52296119e-294
5:   3.72097434e-294 3.72097434e-294 3.72097132e-294 3.13381565e-294
5:   3.72097434e-294 3.72097434e-294 3.52295223e-294 4.18837432e-309
5:   3.72097434e-294 3.72097433e-294 3.71868076e-294 3.72097434e-294
5:   3.72097434e-294]
5:  [3.71867176e-294 1.63608372e-311 3.72097434e-294 3.72097431e-294
5:   4.77830978e-299 4.18837432e-309 3.72097434e-294 3.72020082e-294
5:   6.39095203e-314 3.72097434e-294 3.72097433e-294 3.52295220e-294
5:   1.63608372e-311 3.72097434e-294 3.72097132e-294 4.77830978e-299
5:   4.18837432e-309]
5:  [3.72097434e-294 3.72020082e-294 6.39095203e-314 3.72097434e-294
5:   3.72250940e-294 4.11394853e-294 1.76626814e-284 3.72097434e-294
5:   3.72097434e-294 3.72097434e-294 4.18837432e-309 3.72097434e-294
5:   3.72097434e-294 3.71868076e-294 3.72097434e-294 3.72097434e-294
5:   3.72097431e-294]
5:  [3.13381565e-294 3.72097434e-294 3.72097434e-294 3.72020086e-294
5:   4.18837432e-309 3.72097434e-294 3.72097433e-294 3.52296119e-294
5:   3.72097434e-294 3.72097434e-294 3.72096535e-294 3.13381565e-294
5:   3.72097434e-294 3.72097434e-294 3.52295223e-294 4.18837432e-309
5:   3.72097434e-294]
5:  [3.72097434e-294 3.71868076e-294 3.72097434e-294 3.72097434e-294
5:   3.71867176e-294 1.63608372e-311 3.72097434e-294 3.72097433e-294
5:   3.13151310e-294 4.18837432e-309 3.72097434e-294 3.72020082e-294
5:   6.39095203e-314 3.72097434e-294 3.72097433e-294 3.52295220e-294
5:   1.63608372e-311]]
5:  eager grad out tensor:
5: [[4.22903750e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:   6.93786453e-310 7.90999099e-321 4.22904462e-316 4.22903750e-316
5:   0.00000000e+000 0.00000000e+000 1.46243431e-320 7.67283948e-321
5:   4.22904462e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:   2.13375552e-081]
5:  [7.43568797e-321 4.22904462e-316 6.93786520e-310 0.00000000e+000
5:   0.00000000e+000 1.25231822e-071 7.19853646e-321 4.22904462e-316
5:   6.93786520e-310 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:   6.96138495e-321 4.22358381e-316 4.22904462e-316 0.00000000e+000
5:   0.00000000e+000]
5:  [2.37151510e-322 6.72423344e-321 4.22905568e-316 6.93786520e-310
5:   0.00000000e+000 0.00000000e+000 1.13259849e-095 6.48708193e-321
5:   4.22358381e-316 4.22905568e-316 0.00000000e+000 0.00000000e+000
5:   4.22899244e-316 6.24993042e-321 4.22358381e-316 6.93786520e-310
5:   0.00000000e+000]
5:  [0.00000000e+000 2.37151510e-322 6.01277891e-321 4.22541541e-316
5:   4.22358381e-316 0.00000000e+000 0.00000000e+000 7.50017730e+247
5:   5.77562740e-321 4.22362096e-316 6.93786520e-310 0.00000000e+000
5:   0.00000000e+000 4.22894106e-316 5.53847589e-321 4.22541541e-316
5:   4.22362096e-316]
5:  [0.00000000e+000 0.00000000e+000 2.37151510e-322 5.30132438e-321
5:   4.22541541e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:   2.05531309e-321 5.06417287e-321 4.22541541e-316 6.93786520e-310
5:   0.00000000e+000 0.00000000e+000 1.02617937e+200 4.82702136e-321
5:   4.22541541e-316]
5:  [6.93786520e-310 4.79243676e-322 0.00000000e+000 2.37151510e-322
5:   4.58986985e-321 4.22544308e-316 4.22541541e-316 1.48219694e-322
5:   0.00000000e+000 4.14504728e-316 4.35271834e-321 4.22544308e-316
5:   6.93786520e-310 5.08887615e-322 0.00000000e+000 6.74775095e-067
5:   4.11556683e-321]
5:  [4.22544308e-316 6.93786520e-310 3.40905296e-322 0.00000000e+000
5:   2.37151510e-322 3.87841532e-321 4.22544308e-316 6.93786520e-310
5:   5.78056806e-322 0.00000000e+000 1.02617937e+200 3.64126381e-321
5:   4.22544308e-316 6.93786520e-310 2.42092166e-322 0.00000000e+000
5:   4.22964382e-316]
5:  [3.40411230e-321 4.22544308e-316 6.93786520e-310 5.82997462e-322
5:   0.00000000e+000 2.37151510e-322 3.16696079e-321 4.22545177e-316
5:   4.22544308e-316 1.67982320e-322 0.00000000e+000 5.03304002e-038
5:   2.92980928e-321 4.22545177e-316 6.93786520e-310 5.53353523e-322
5:   0.00000000e+000]
5:  [4.42682819e-321 2.69265777e-321 4.22546047e-316 4.22545177e-316
5:   1.48219694e-322 0.00000000e+000 2.37151510e-322 2.45550626e-321
5:   4.22546047e-316 6.93786520e-310 5.13828272e-322 0.00000000e+000
5:   3.43102888e-315 2.21835475e-321 4.22547154e-316 4.22546047e-316
5:   1.48219694e-322]
5:  [0.00000000e+000 4.22903829e-316 1.98120324e-321 4.22547154e-316
5:   6.93786520e-310 5.08887615e-322 0.00000000e+000 7.58799416e-096
5:   1.74405173e-321 4.22547154e-316 6.93786520e-310 2.12448228e-322
5:   0.00000000e+000 2.63518306e+267 1.50690022e-321 4.22496245e-316
5:   4.22904462e-316]
5:  [4.09781745e-080 3.48191142e+227 2.61645970e+180 3.68167801e+180
5:   2.64887982e+180 3.94648579e+180 1.99108518e+209 1.53675404e+171
5:   1.98854893e-081 4.49576165e+251 7.50017970e+247 1.14323763e+243
5:   6.72775353e+199 2.41784537e+198 2.88067937e+214 7.89171056e-320
5:   7.90505033e-322]
5:  [7.11454530e-322 2.62612206e-144 1.96502797e+214 4.09781745e-080
5:   3.48191142e+227 2.61645970e+180 3.68167801e+180 2.64887982e+180
5:   3.94648579e+180 1.99108518e+209 1.53675404e+171 1.98854893e-081
5:   4.49576165e+251 7.50017970e+247 1.14323763e+243 6.72775353e+199
5:   2.41784992e+198]
5:  [2.22922419e-320 3.95252517e-322 4.22960271e-316 5.30482726e+180
5:   6.99588595e+098 3.94612158e+180 7.67614547e+218 4.89561428e-109
5:   7.50014226e+247 8.70031696e-313 0.00000000e+000 4.43176884e-321
5:   4.22904462e-316 6.93786520e-310 5.91870742e-061 5.82663669e+252
5:   1.24034881e+224]]
5: 
5: Mismatched elements: 37 / 221 (16.7%)
5: Max absolute difference among violations: 2.63518306e+267
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[1.636084e-311, 3.720974e-294, 3.720971e-294, 4.778310e-299,
5:         4.188374e-309, 3.720974e-294, 3.720201e-294, 6.390952e-314,
5:         3.720974e-294, 3.720974e-294, 3.722509e-294, 1.766268e-284,...
5:  DESIRED: array([[4.229038e-316, 6.937865e-310, 0.000000e+000, 0.000000e+000,
5:         6.937865e-310, 7.909991e-321, 4.229045e-316, 4.229038e-316,
5:         0.000000e+000, 0.000000e+000, 1.462434e-320, 7.672839e-321,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.ElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[6.93785681e-310 6.93785681e-310 6.93767800e-310 4.27586267e-316
5:   3.18299369e-313 1.76692485e-284 3.13535070e-294 1.76692744e-284
5:   1.76692744e-284 1.76626430e-284 6.98062387e-309 1.76692744e-284
5:   1.76692743e-284 1.51310288e-284 1.76692744e-284 1.76692744e-284
5:   1.76692357e-284]
5:  [4.78416550e-299 1.76692744e-284 1.76692744e-284 1.76593594e-284
5:   6.98062387e-309 1.76692744e-284 1.76692743e-284 3.52296719e-294
5:   1.76692744e-284 1.76692744e-284 1.76692357e-284 4.78416550e-299
5:   1.76692744e-284 1.76692744e-284 1.76593594e-284 6.98062387e-309
5:   1.76692744e-284]
5:  [1.76692743e-284 3.52296719e-294 1.76692744e-284 3.72097437e-294
5:   3.72098034e-294 1.76626814e-284 1.76692744e-284 1.76692744e-284
5:   1.76692488e-284 1.76692744e-284 1.76692744e-284 1.76692744e-284
5:   1.76627071e-284 1.76692744e-284 1.76692744e-284 1.76692743e-284
5:   4.27588125e-316]
5:  [3.18299369e-313 5.61769430e-314 8.48798317e-314 6.93786383e-310
5:   1.76692744e-284 1.76692744e-284 1.76528563e-284 1.76692744e-284
5:   1.76692744e-284 1.76692742e-284 1.34662287e-284 1.76692744e-284
5:   1.76692744e-284 1.76692103e-284 6.98062387e-309 1.76692744e-284
5:   1.76692744e-284]
5:  [1.76528563e-284 1.76692744e-284 1.76692744e-284 1.76692742e-284
5:   2.05478344e-289 1.76692744e-284 4.27589074e-316 3.18299369e-313
5:   6.98062387e-309 1.76692744e-284 1.76692744e-284 1.34498106e-284
5:   1.76692744e-284 1.76692744e-284 1.76692613e-284 7.30005722e-304
5:   1.76692744e-284]
5:  [1.76692744e-284 1.76659266e-284 6.98062387e-309 1.76692744e-284
5:   1.76692742e-284 3.13152805e-294 1.76692744e-284 1.76692744e-284
5:   3.72098034e-294 1.76626815e-284 1.76692744e-284 1.76692744e-284
5:   4.27589983e-316 3.18299369e-313 3.39519327e-313 2.75859453e-313
5:   1.59782050e-284]
5:  [1.76692744e-284 1.76692744e-284 1.76692485e-284 3.13535070e-294
5:   1.76692744e-284 1.76692742e-284 1.76626430e-284 6.98062387e-309
5:   1.76692744e-284 1.76692743e-284 1.59716380e-284 1.76692744e-284
5:   1.76692744e-284 1.76692485e-284 4.78416550e-299 1.76692744e-284
5:   1.76692744e-284]
5:  [1.76593594e-284 6.98062387e-309 1.76692744e-284 1.76692742e-284
5:   1.34498106e-284 1.76692744e-284 1.76692744e-284 1.76692100e-284
5:   2.72680620e-311 1.76692744e-284 1.76692744e-284 1.76527921e-284
5:   6.98062387e-309 1.76692744e-284 1.76692742e-284 1.34498106e-284
5:   1.76692744e-284]
5:  [1.76692744e-284 1.76659264e-284 2.72680620e-311 1.76692744e-284
5:   1.76692744e-284 1.68121830e-284 6.98062387e-309 1.76692744e-284
5:   1.76692613e-284 2.05227823e-289 1.76692744e-284 3.72097434e-294
5:   3.72097434e-294 1.76626814e-284 1.76692744e-284 1.76692744e-284
5:   1.76692488e-284]
5:  [1.76692744e-284 1.76692744e-284 1.76692744e-284 1.76627071e-284
5:   1.76692744e-284 1.76692744e-284 1.76692743e-284 1.34662287e-284
5:   1.76692744e-284 1.76692744e-284 1.76692359e-284 6.27738729e-299
5:   1.76692744e-284 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:   0.00000000e+000]
5:  [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:   1.76692103e-284 6.98062387e-309 1.76692744e-284 1.58830490e-316
5:   6.93786520e-310 6.93786520e-310 4.27593026e-316 4.27593026e-316
5:   1.34662287e-284 1.76692744e-284 1.76692744e-284 1.76692103e-284
5:   6.98062387e-309]
5:  [1.76692744e-284 1.76692744e-284 1.51310288e-284 1.76692744e-284
5:   1.76692744e-284 1.76692742e-284 2.05478344e-289 1.76692744e-284
5:   1.76692744e-284 1.76692103e-284 6.98062387e-309 1.76692744e-284
5:   1.76692744e-284 3.52296719e-294 1.76692744e-284 3.72097434e-294
5:   3.72097437e-294]
5:  [1.76626814e-284 1.76692744e-284 1.76692744e-284 1.76692744e-284
5:   1.76692744e-284 1.76692744e-284 1.76692744e-284 1.76659908e-284
5:   1.76692744e-284 1.76692744e-284 1.76692742e-284 1.43068379e-284
5:   1.76692744e-284 1.76692744e-284 1.76692616e-284 9.57853285e-304
5:   1.76692744e-284]]
5:  eager grad out tensor:
5: [[6.01347002e-154 7.16395186e-322 4.22377590e-316 6.93786520e-310
5:   3.77081128e-317 0.00000000e+000 1.72051116e+243 4.79243676e-322
5:   4.22377590e-316 6.93786520e-310 4.71061266e+257 5.73116149e-322
5:   7.20362275e+159 2.42092166e-322 4.22362333e-316 4.22377590e-316
5:   2.25824004e-306]
5:  [0.00000000e+000 3.32012114e-321 2.37151510e-322 4.22366602e-316
5:   4.94065646e-324 4.34777768e-322 0.00000000e+000 3.55727265e-321
5:   2.37151510e-322 4.22378223e-316 5.43472210e-323 3.50638655e+151
5:   3.70354276e-317 8.71399938e-315 7.16395186e-322 4.22378855e-316
5:   6.93786520e-310]
5:  [2.15294903e-312 0.00000000e+000 2.45066354e+198 4.79243676e-322
5:   4.22379487e-316 4.22378855e-316 8.41470667e+276 2.15211916e-312
5:   7.26610610e+223 2.42092166e-322 4.22362333e-316 4.22379487e-316
5:   1.48219694e-322 0.00000000e+000 4.50587869e-321 2.37151510e-322
5:   4.22368025e-316]
5:  [0.00000000e+000 0.00000000e+000 0.00000000e+000 4.74303020e-321
5:   2.37151510e-322 4.22369448e-316 4.65782209e+151 3.70783145e-310
5:   0.00000000e+000 4.95270029e+223 9.53546696e-322 4.22380752e-316
5:   4.22380120e-316 2.15253881e-312 0.00000000e+000 6.01347002e-154
5:   7.16395186e-322]
5:  [4.22380752e-316 6.93786520e-310 9.93669318e+247 8.90145784e-315
5:   2.35624955e+251 4.79243676e-322 4.22362333e-316 4.22380752e-316
5:   0.00000000e+000 0.00000000e+000 1.46923558e+195 2.42092166e-322
5:   4.22362333e-316 6.93786520e-310 7.49088705e+247 8.40834198e-315
5:   5.92878775e-321]
5:  [2.37151510e-322 4.22383045e-316 3.45845952e-323 9.53380758e-307
5:   0.00000000e+000 1.14157059e+243 1.19069821e-321 4.22382096e-316
5:   6.93786520e-310 3.50638655e+151 3.70354276e-317 4.37338152e+257
5:   9.53546696e-322 4.22382096e-316 6.93786520e-310 3.01467462e+161
5:   1.64254070e-306]
5:  [6.01347002e-154 7.16395186e-322 4.22382096e-316 6.93786520e-310
5:   2.63518306e+267 1.26645419e-306 1.75304730e-152 4.79243676e-322
5:   4.22382096e-316 6.93786520e-310 2.37151510e-322 0.00000000e+000
5:   6.98860380e+228 2.42092166e-322 4.22362333e-316 4.22382096e-316
5:   4.34777768e-322]
5:  [0.00000000e+000 7.35169681e-321 2.37151510e-322 4.22374665e-316
5:   5.49206313e+247 3.67142682e+228 3.41160272e-315 3.54662652e+160
5:   9.53546696e-322 4.22374428e-316 4.22391424e-316 8.58118109e-313
5:   0.00000000e+000 2.44014100e-154 7.16395186e-322 4.22362333e-316
5:   4.22388736e-316]
5:  [9.93669318e+247 8.90145784e-315 5.35721112e+199 4.79243676e-322
5:   4.22383519e-316 6.93786520e-310 7.68755682e+170 4.99006302e-322
5:   6.01347002e-154 2.42092166e-322 4.22382096e-316 4.22383519e-316
5:   7.49088705e+247 8.40834198e-315 8.53745436e-321 2.37151510e-322
5:   4.22393400e-316]
5:  [7.07091625e+194 7.00113194e+194 8.73368664e-313 8.77460587e-321
5:   2.37151510e-322 4.22943513e-316 5.43519716e-109 0.00000000e+000
5:   0.00000000e+000 1.81270012e-152 1.42784972e-321 4.22386444e-316
5:   4.22391977e-316 2.04725594e+190 9.26741558e+175 1.67494802e+243
5:   1.19069821e-321]
5:  [4.22382096e-316 1.31125734e-316 0.00000000e+000 0.00000000e+000
5:   2.30908182e+251 9.53546696e-322 4.22391977e-316 4.22836162e-316
5:   2.04725594e+190 9.26741558e+175 1.24033555e+224 7.16395186e-322
5:   4.22391977e-316 4.22836636e-316 6.21452776e+175 4.88065533e+252
5:   6.89906703e-310]
5:  [4.79243676e-322 4.22362333e-316 4.22391977e-316 3.34812076e-315
5:   0.00000000e+000 0.00000000e+000 2.42092166e-322 4.22362333e-316
5:   4.22392689e-316 1.49005498e+195 6.78537599e-308 1.04346664e-320
5:   2.37151510e-322 4.22943038e-316 3.60705816e-085 2.04725594e+190
5:   9.26741558e+175]
5:  [8.56486049e+183 1.19069821e-321 4.22388736e-316 4.22391424e-316
5:   5.18768928e-322 0.00000000e+000 1.45516433e-152 9.53546696e-322
5:   4.22382096e-316 6.93786520e-310 3.50786609e-322 0.00000000e+000
5:   2.14715295e+243 7.16395186e-322 4.22382096e-316 6.93786520e-310
5:   5.13828272e-322]]
5: 
5: Mismatched elements: 42 / 221 (19%)
5: Max absolute difference among violations: 8.41470667e+276
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[6.937857e-310, 6.937857e-310, 6.937678e-310, 4.275863e-316,
5:         3.182994e-313, 1.766925e-284, 3.135351e-294, 1.766927e-284,
5:         1.766927e-284, 1.766264e-284, 6.980624e-309, 1.766927e-284,...
5:  DESIRED: array([[6.013470e-154, 7.163952e-322, 4.223776e-316, 6.937865e-310,
5:         3.770811e-317, 0.000000e+000, 1.720511e+243, 4.792437e-322,
5:         4.223776e-316, 6.937865e-310, 4.710613e+257, 5.731161e-322,...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.ElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[ 0.000000e+000,  2.241705e-145,  0.000000e+000,  0.000000e+000,
5:          4.172944e-063,  0.000000e+000,  0.000000e+000,  4.303079e-065,
5:          0.000000e+000,  0.000000e+000,  0.000000e+000,  0.000000e+000,...
5:  DESIRED: array([[0.045559, 0.067158, 0.425577, 0.070198, 0.020496, 0.521741,
5:         0.229545, 0.216193, 0.683392, 0.047969, 0.345317, 0.044743,
5:         0.369469, 0.730301, 0.202321, 0.070117, 0.206777],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.ElementwiseMulOp_broadcast)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369 7.01229369
5:   7.01229369 7.01229369 7.01229369 7.01229369 7.01229369]
5:  [6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923]
5:  [6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923]
5:  [6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923]
5:  [6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923 6.98486923
5:   6.98486923 6.98486923 6.98486923 6.98486923 6.98486923]]
5:  eager grad out tensor:
5: [[1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277]
5:  [1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277 1.12857358e+277 1.12857358e+277 1.12857358e+277
5:   1.12857358e+277]
5:  [9.82220279e+252 9.82220279e+252 9.82220279e+252 9.82220279e+252
5:   9.82220279e+252 9.82220279e+252 9.82220279e+252 9.82220279e+252
5:   9.82220279e+252 9.82220279e+252 9.82220279e+252 9.82220279e+252
5:   9.82220279e+252 9.82220279e+252 9.82220279e+252 9.82220279e+252
5:   9.82220279e+252]
5:  [9.61987860e+228 9.61987860e+228 9.61987860e+228 9.61987860e+228
5:   9.61987860e+228 9.61987860e+228 9.61987860e+228 9.61987860e+228
5:   9.61987860e+228 9.61987860e+228 9.61987860e+228 9.61987860e+228
5:   9.61987860e+228 9.61987860e+228 9.61987860e+228 9.61987860e+228
5:   9.61987860e+228]
5:  [7.21113890e+252 7.21113890e+252 7.21113890e+252 7.21113890e+252
5:   7.21113890e+252 7.21113890e+252 7.21113890e+252 7.21113890e+252
5:   7.21113890e+252 7.21113890e+252 7.21113890e+252 7.21113890e+252
5:   7.21113890e+252 7.21113890e+252 7.21113890e+252 7.21113890e+252
5:   7.21113890e+252]
5:  [1.12855330e+277 1.12855330e+277 1.12855330e+277 1.12855330e+277
5:   1.12855330e+277 1.12855330e+277 1.12855330e+277 1.12855330e+277
5:   1.12855330e+277 1.12855330e+277 1.12855330e+277 1.12855330e+277
5:   1.12855330e+277 1.12855330e+277 1.12855330e+277 1.12855330e+277
5:   1.12855330e+277]
5:  [1.34277722e+253 1.34277722e+253 1.34277722e+253 1.34277722e+253
5:   1.34277722e+253 1.34277722e+253 1.34277722e+253 1.34277722e+253
5:   1.34277722e+253 1.34277722e+253 1.34277722e+253 1.34277722e+253
5:   1.34277722e+253 1.34277722e+253 1.34277722e+253 1.34277722e+253
5:   1.34277722e+253]
5:  [1.01820900e+277 1.01820900e+277 1.01820900e+277 1.01820900e+277
5:   1.01820900e+277 1.01820900e+277 1.01820900e+277 1.01820900e+277
5:   1.01820900e+277 1.01820900e+277 1.01820900e+277 1.01820900e+277
5:   1.01820900e+277 1.01820900e+277 1.01820900e+277 1.01820900e+277
5:   1.01820900e+277]
5:  [3.02433134e+184 3.02433134e+184 3.02433134e+184 3.02433134e+184
5:   3.02433134e+184 3.02433134e+184 3.02433134e+184 3.02433134e+184
5:   3.02433134e+184 3.02433134e+184 3.02433134e+184 3.02433134e+184
5:   3.02433134e+184 3.02433134e+184 3.02433134e+184 3.02433134e+184
5:   3.02433134e+184]
5:  [9.82161512e+242 9.82161512e+242 9.82161512e+242 9.82161512e+242
5:   9.82161512e+242 9.82161512e+242 9.82161512e+242 9.82161512e+242
5:   9.82161512e+242 9.82161512e+242 9.82161512e+242 9.82161512e+242
5:   9.82161512e+242 9.82161512e+242 9.82161512e+242 9.82161512e+242
5:   9.82161512e+242]
5:  [4.82588762e+276 4.82588762e+276 4.82588762e+276 4.82588762e+276
5:   4.82588762e+276 4.82588762e+276 4.82588762e+276 4.82588762e+276
5:   4.82588762e+276 4.82588762e+276 4.82588762e+276 4.82588762e+276
5:   4.82588762e+276 4.82588762e+276 4.82588762e+276 4.82588762e+276
5:   4.82588762e+276]
5:  [7.48668256e+295 7.48668256e+295 7.48668256e+295 7.48668256e+295
5:   7.48668256e+295 7.48668256e+295 7.48668256e+295 7.48668256e+295
5:   7.48668256e+295 7.48668256e+295 7.48668256e+295 7.48668256e+295
5:   7.48668256e+295 7.48668256e+295 7.48668256e+295 7.48668256e+295
5:   7.48668256e+295]
5:  [4.32845148e+223 4.32845148e+223 4.32845148e+223 4.32845148e+223
5:   4.32845148e+223 4.32845148e+223 4.32845148e+223 4.32845148e+223
5:   4.32845148e+223 4.32845148e+223 4.32845148e+223 4.32845148e+223
5:   4.32845148e+223 4.32845148e+223 4.32845148e+223 4.32845148e+223
5:   4.32845148e+223]
5:  [3.98450019e+252 3.98450019e+252 3.98450019e+252 3.98450019e+252
5:   3.98450019e+252 3.98450019e+252 3.98450019e+252 3.98450019e+252
5:   3.98450019e+252 3.98450019e+252 3.98450019e+252 3.98450019e+252
5:   3.98450019e+252 3.98450019e+252 3.98450019e+252 3.98450019e+252
5:   3.98450019e+252]
5:  [4.79137541e+276 4.79137541e+276 4.79137541e+276 4.79137541e+276
5:   4.79137541e+276 4.79137541e+276 4.79137541e+276 4.79137541e+276
5:   4.79137541e+276 4.79137541e+276 4.79137541e+276 4.79137541e+276
5:   4.79137541e+276 4.79137541e+276 4.79137541e+276 4.79137541e+276
5:   4.79137541e+276]
5:  [9.82220280e+252 9.82220280e+252 9.82220280e+252 9.82220280e+252
5:   9.82220280e+252 9.82220280e+252 9.82220280e+252 9.82220280e+252
5:   9.82220280e+252 9.82220280e+252 9.82220280e+252 9.82220280e+252
5:   9.82220280e+252 9.82220280e+252 9.82220280e+252 9.82220280e+252
5:   9.82220280e+252]
5:  [9.30684868e+242 9.30684868e+242 9.30684868e+242 9.30684868e+242
5:   9.30684868e+242 9.30684868e+242 9.30684868e+242 9.30684868e+242
5:   9.30684868e+242 9.30684868e+242 9.30684868e+242 9.30684868e+242
5:   9.30684868e+242 9.30684868e+242 9.30684868e+242 9.30684868e+242
5:   9.30684868e+242]]
5: 
5: Mismatched elements: 289 / 289 (100%)
5: Max absolute difference among violations: 7.48668256e+295
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[7.012294, 7.012294, 7.012294, 7.012294, 7.012294, 7.012294,
5:         7.012294, 7.012294, 7.012294, 7.012294, 7.012294, 7.012294,
5:         7.012294, 7.012294, 7.012294, 7.012294, 7.012294],...
5:  DESIRED: array([[1.128574e+277, 1.128574e+277, 1.128574e+277, 1.128574e+277,
5:         1.128574e+277, 1.128574e+277, 1.128574e+277, 1.128574e+277,
5:         1.128574e+277, 1.128574e+277, 1.128574e+277, 1.128574e+277,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.ElementwiseMulOp_broadcast)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]
5: 
5:  [[3.02433134e+184]
5:   [2.73971565e+277]
5:   [1.61116235e+277]
5:   [4.79137541e+276]
5:   [4.79137541e+276]
5:   [1.01820900e+277]
5:   [4.79137541e+276]
5:   [5.13913477e+025]
5:   [1.22151557e+161]
5:   [1.08359175e+142]
5:   [4.72585285e+257]
5:   [1.01820900e+277]
5:   [9.82220279e+252]
5:   [4.79137541e+276]
5:   [1.38067030e+253]
5:   [1.34277722e+253]
5:   [1.38067030e+253]]]
5:  eager grad out tensor:
5: [[[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]
5: 
5:  [[8.94213159e+130]
5:   [4.08277747e+179]
5:   [4.08277747e+179]
5:   [2.78795578e+179]
5:   [2.78795578e+179]
5:   [3.43505865e+179]
5:   [3.43505865e+179]
5:   [3.92576715e+179]
5:   [3.92576715e+179]
5:   [3.11530995e+179]
5:   [3.11530995e+179]
5:   [2.78922463e+179]
5:   [2.78922463e+179]
5:   [8.94213159e+130]
5:   [2.62397078e+179]
5:   [2.62397078e+179]
5:   [3.43885022e+179]]]
5: 
5: Mismatched elements: 221 / 221 (100%)
5: Max absolute difference among violations: 2.73971565e+277
5: Max relative difference among violations: 5.35820275e+145
5:  ACTUAL: array([[[3.024331e+184],
5:         [2.739716e+277],
5:         [1.611162e+277],...
5:  DESIRED: array([[[8.942132e+130],
5:         [4.082777e+179],
5:         [4.082777e+179],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.ElementwiseMulOp_broadcast)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]
5: 
5:  [[3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.20334544e+132]
5:   [4.08793336e+141]
5:   [2.79117083e+141]
5:   [2.11458125e+141]
5:   [1.33396781e-037]
5:   [3.30672792e-033]
5:   [3.07827721e+141]
5:   [3.99266516e+141]
5:   [1.10566864e+141]
5:   [4.27787206e-033]
5:   [1.48500415e+161]
5:   [6.20393707e+223]
5:   [2.98245152e+141]
5:   [3.02896920e+141]]]
5:  eager grad out tensor:
5: [[[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]
5: 
5:  [[1.24033555e+224]
5:   [5.82669747e+252]
5:   [7.50014226e+247]
5:   [5.44356270e+257]
5:   [2.14570104e+253]
5:   [5.44345563e+257]
5:   [1.38182478e+267]
5:   [5.82663669e+252]
5:   [1.26715068e-315]
5:   [3.67142682e+228]
5:   [6.05754487e+252]
5:   [1.27219222e-315]
5:   [7.26607066e+223]
5:   [7.00113194e+194]
5:   [1.38757474e-309]
5:   [2.57976858e+267]
5:   [1.67793437e+243]]]
5: 
5: Mismatched elements: 208 / 221 (94.1%)
5: Max absolute difference among violations: 2.57976858e+267
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[3.992665e+141],
5:         [1.105669e+141],
5:         [4.277872e-033],...
5:  DESIRED: array([[[1.240336e+224],
5:         [5.826697e+252],
5:         [7.500142e+247],...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.ElementwiseMulOp_broadcast)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[0.000000e+000, 0.000000e+000, 0.000000e+000, ...,
5:          0.000000e+000, 0.000000e+000, 0.000000e+000],
5:         [0.000000e+000, 0.000000e+000, 0.000000e+000, ...,...
5:  DESIRED: array([[[0.311502, 0.183594, 0.289076, ..., 0.03437 , 0.115469,
5:          0.097138],
5:         [0.151829, 0.524337, 0.365979, ..., 0.318883, 0.229111,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 544, in test_check_grad_ingore_x
5:     self.check_grad(['Y'], 'Out', no_grad_set=set("X"), check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable Y (shape: (2, 3, 4, 5), dtype: complex128) max gradient diff nan over limit 5.000000e-03, the first error element is 0, expected nan, but got 3.554787e-318.
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 547, in test_check_grad_ingore_y
5:     self.check_grad(['X'], 'Out', no_grad_set=set('Y'), check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 3, 4, 5), dtype: complex128) max gradient diff nan over limit 5.000000e-03, the first error element is 0, expected nan, but got 1.621507e+225.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 541, in test_check_grad_normal
5:     self.check_grad(['X', 'Y'], 'Out', check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 3, 4, 5), dtype: complex128) max gradient diff nan over limit 5.000000e-03, the first error element is 0, expected nan, but got 3.774355e-318.
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 538, in test_check_output
5:     self.check_output(check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=1e-05
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 120 / 120 (100%)
5: Max absolute difference among violations: inf
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[[ 0.000000e+000+0.000000e+000j,  0.000000e+000+0.000000e+000j,
5:            0.000000e+000+0.000000e+000j,  9.011778e-159-9.011766e-159j,
5:            0.000000e+000+0.000000e+000j],...
5:  DESIRED: array([[[[ 0.374096+0.643907j,  0.032569+0.578421j,
5:            0.090165+0.221111j, -0.029816+0.418771j,
5:           -0.228952+0.476953j],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestComplexElementwiseMulOpWithCheckGrad)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 138, in test_check_grad_ingore_x
5:     self.check_grad(['Y'], 'Out', no_grad_set=set("X"), check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(4.653424341062855e+242) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable Y (shape: (2,), dtype: complex128) max gradient diff 4.653424e+242 over limit 5.000000e-03, the first error element is 0, expected 0.000000e+00, but got 6.300076e+231.
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestComplexElementwiseMulOpWithCheckGrad)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 141, in test_check_grad_ingore_y
5:     self.check_grad(['X'], 'Out', no_grad_set=set('Y'), check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(1.0) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2,), dtype: complex128) max gradient diff 1.000000e+00 over limit 5.000000e-03, the first error element is 0, expected 1.802772e+252, but got 2.123417e-316.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestComplexElementwiseMulOpWithCheckGrad)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 135, in test_check_grad_normal
5:     self.check_grad(['X', 'Y'], 'Out', check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(131211.97722093857) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2,), dtype: complex128) max gradient diff 1.312120e+05 over limit 5.000000e-03, the first error element is 0, expected 0.000000e+00, but got 2.619572e-316.
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestComplexElementwiseMulOpWithCheckGrad)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=1e-05
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 2 / 2 (100%)
5: Max absolute difference among violations: 25.
5: Max relative difference among violations: 1.
5:  ACTUAL: array([ 0.000000e+00+0.000000e+00j, -1.363309e-95+1.363309e-95j])
5:  DESIRED: array([-7.+24.j, -7.+16.j])
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_Vector)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [4.64421707e-322 2.64933471e+180 5.25802605e-316 4.69362364e-322
5:  4.69362364e-322 4.60348836e+169 5.25803158e-316 4.49599738e-322
5:  4.49599738e-322 2.41784487e+198 2.13436359e-321 7.11454530e-322
5:  1.12855330e+277 3.45043490e+175 1.99909154e+161 9.89843640e+169
5:  3.02433134e+184 2.04091783e-153 4.82588762e+276 2.64933471e+180
5:  1.24041768e+223 5.44452876e-109 2.51312310e+180 4.60348836e+169
5:  6.72780911e+199 1.12857358e+277 1.36175072e+069 3.68167800e+180
5:  2.84581812e-321 7.11454530e-322 1.12855330e+277 3.45043490e+175
5:  1.99909154e+161 9.89843640e+169 7.04157014e+174 9.67594028e-023
5:  1.99446555e+161 6.00190289e+175 7.99282716e+005 9.82220279e+252
5:  5.26047668e+170 4.34256169e-114 6.83055591e+212 3.60556945e+252
5:  7.71521092e+044 4.05918702e-317 4.23146989e-316 9.53546696e-322
5:  4.22330871e-316 5.25295812e-316 3.95252517e-323 2.06683447e+243
5:  4.22325733e-316 4.22330950e-316 4.22331741e-316 4.22331741e-316
5:  3.95252517e-322 5.53353523e-322 4.52520021e-316 3.99461109e+252
5:  2.85678800e+151 1.53730015e+103 4.24048879e+175 9.61987860e+228
5:  2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:  4.10698629e+059 2.58125593e-306 4.50587869e-321 3.95252517e-322
5:  4.22327630e-316 5.22109365e-316 1.23516411e-322 1.23516411e-322
5:  2.27551609e+161 4.22327709e-316 4.22328026e-316 4.22328026e-316
5:  4.90113121e-321 3.95252517e-322 4.22321939e-316 4.59481051e-322
5:  4.59481051e-322 0.00000000e+000 4.22328105e-316 4.64421707e-322
5:  4.64421707e-322 0.00000000e+000 7.43074731e-321 5.53353523e-322
5:  5.22110077e-316 1.24282155e+214 2.86530675e+161 3.63041963e+132
5:  3.94355938e+180 4.79137541e+276 1.26001517e+232 1.99034017e+161]
5:  eager grad out tensor:
5: [7.99282716e+005 9.82220279e+252 5.26047668e+170 4.34256169e-114
5:  6.83055591e+212 3.60556945e+252 7.71521092e+044 4.05918702e-317
5:  0.00000000e+000 2.37645576e-321 5.26283864e-316 6.93786520e-310
5:  3.98450019e+252 9.82220280e+252 7.34514290e+223 5.00404667e+160
5:  2.04089393e-153 5.50656807e+213 5.70540353e-109 2.58468518e+161
5:  1.90221584e+132 2.61352510e-306 0.00000000e+000 1.82310223e-321
5:  4.25109180e-316 4.53486967e-316 5.43472210e-323 2.99489167e+262
5:  3.24427959e-317 5.22094662e-316 5.22094820e-316 5.22094820e-316
5:  3.95252517e-322 3.95252517e-322 5.22094188e-316 5.22093990e-316
5:  6.42285340e-323 1.35889693e+248 2.13007321e-312 5.22094267e-316
5:  5.22094583e-316 5.22094583e-316 7.90505033e-322 3.95252517e-322
5:  5.26286394e-316 4.59481051e-322 4.59481051e-322 0.00000000e+000
5:  5.26282283e-316 4.64421707e-322 4.64421707e-322 0.00000000e+000
5:  0.00000000e+000 6.37344683e-322 4.57405026e-316 4.26661732e-316
5:  4.59481051e-322 0.00000000e+000 6.93786453e-310 4.00193173e-322
5:  5.23439864e-316 4.58248021e-316 2.47032823e-323 2.38073041e-312
5:  5.22095136e-316 5.22097587e-316 5.22098535e-316 5.22098535e-316
5:  4.90113121e-321 6.32404027e-322 4.57386845e-316 3.45043490e+175
5:  1.99909154e+161 2.78616216e+184 9.66533815e-023 1.99446555e+161
5:  6.00190289e+175 7.99282716e+005 9.82220279e+252 5.26047668e+170
5:  4.34256169e-114 6.83055591e+212 3.60556945e+252 7.71521092e+044
5:  4.05918702e-317 1.92932635e-320 4.22093878e-316 5.24628230e-316
5:  0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:  2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:  4.10698629e+059 4.94719943e+173 2.58740967e+161 5.58294180e-322]
5: 
5: Mismatched elements: 66 / 100 (66%)
5: Max absolute difference among violations: 1.12857358e+277
5: Max relative difference among violations: inf
5:  ACTUAL: array([4.644217e-322, 2.649335e+180, 5.258026e-316, 4.693624e-322,
5:        4.693624e-322, 4.603488e+169, 5.258032e-316, 4.495997e-322,
5:        4.495997e-322, 2.417845e+198, 2.134364e-321, 7.114545e-322,...
5:  DESIRED: array([7.992827e+005, 9.822203e+252, 5.260477e+170, 4.342562e-114,
5:        6.830556e+212, 3.605569e+252, 7.715211e+044, 4.059187e-317,
5:        0.000000e+000, 2.376456e-321, 5.262839e-316, 6.937865e-310,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_Vector)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [ 7.33953285e+223  3.33825400e+151  5.23494883e-316  6.62047965e-322
5:   6.62047965e-322  1.67183294e-303  4.94065646e-324  2.91498731e-322
5:   1.02137327e+173  1.12209350e+200  3.24107064e-321  7.11454530e-322
5:   5.23490061e-316  6.93786520e-310  5.23489508e-316  4.44659081e-323
5:   4.26137353e+257  2.41762435e-168  5.23489666e-316  0.00000000e+000
5:   0.00000000e+000  3.27909849e-260  5.23496781e-316  1.67982320e-322
5:   1.48219694e-323  1.14573407e+243 -9.98929336e+204  3.16202013e-322
5:   3.95252517e-321  7.11454530e-322  5.23490773e-316  6.93786520e-310
5:   5.23490219e-316  4.44659081e-323  7.68755682e+170  4.99006302e-322
5:   5.23490378e-316  0.00000000e+000  5.23497729e-316  1.67982320e-322
5:   1.67982320e-322  1.31120721e+214  1.48219694e-323  3.16202013e-322
5:   3.39740367e+223  5.91847393e-085  4.66397970e-321  7.11454530e-322
5:   5.23491484e-316  6.93786520e-310  5.23490931e-316  4.44659081e-323
5:   2.40611003e+233  4.09781745e-080  5.23498994e-316  1.67982320e-322
5:   1.67982320e-322  2.64887982e+180  5.23499389e-316  3.11261357e-322
5:   3.45845952e-323  1.98854893e-081 -6.45706555e-080  1.77863633e-322
5:   5.37543423e-321  7.11454530e-322  5.23492196e-316  6.93786520e-310
5:   5.23491642e-316  3.45845952e-323  9.53380758e-307  5.92878775e-323
5:   1.48890282e+195  4.89519911e-109  5.23500496e-316  7.70742408e-322
5:   7.70742408e-322  8.96584634e+276  4.94065646e-324  2.66795449e-322
5:  -2.23314317e-302  6.72775353e+199  6.08688876e-321  7.11454530e-322
5:   5.23492907e-316  6.93786520e-310  5.23492354e-316  2.47032823e-323
5:   2.15253881e-312  1.22521275e+224  1.18575755e-322  6.91691904e-322
5:   6.91691904e-322  2.54724494e+151  5.23503421e-316  1.97626258e-322
5:   4.94065646e-324  2.04725594e+190 -2.02395279e-176  6.52166653e-322]
5:  eager grad out tensor:
5: [-3.71982886e-235  6.93680724e-310  6.93680722e-310  5.59213030e+169
5:   6.93680724e-310  6.93680722e-310  5.60510695e+203  6.93680724e-310
5:   6.93680722e-310 -5.73743282e-196  6.93680724e-310  6.93680722e-310
5:  -1.26255690e-223  6.93680724e-310  6.93680722e-310  1.31287820e+059
5:   6.93680724e-310  6.93680722e-310 -9.43166091e-033  6.93680724e-310
5:   6.93680722e-310  1.43254177e-049  6.93681398e-310  6.93680722e-310
5:   2.68027506e-114  6.93680724e-310  6.93680728e-310 -1.51529522e-097
5:   6.93680724e-310  6.93680722e-310  2.27790043e-085  6.93680724e-310
5:   6.93680722e-310  1.73158043e+044  6.93681398e-310  6.93680722e-310
5:   3.35054152e+117  6.93681398e-310  6.93680722e-310  4.32621322e-278
5:   6.93681398e-310  6.93680722e-310  9.57369307e-017  6.93681398e-310
5:   6.93680722e-310  9.41045175e+111  6.93680724e-310  6.93680722e-310
5:   3.34912549e+273  6.93680724e-310  6.93680722e-310 -1.97322667e+042
5:   6.93681398e-310  6.93680722e-310 -5.35929806e+032  6.93680724e-310
5:   6.93680722e-310  1.17291056e-004  6.93680728e-310  6.93680722e-310
5:  -2.44507625e-037  6.93680728e-310  6.93680722e-310 -2.16553744e-072
5:   6.93781712e-310  6.93680722e-310  1.83229562e+142  6.93680728e-310
5:   6.93680722e-310 -7.17248897e+221  6.93781712e-310  6.93680722e-310
5:  -3.45897376e+059  6.93781623e-310  6.93680722e-310  2.37141251e+077
5:   6.93781623e-310  6.93680722e-310  2.84622747e-158  6.93767807e-310
5:   6.93680722e-310  1.84660926e-222  6.93680733e-310  6.93680722e-310
5:  -4.13545879e-109  6.93680733e-310  6.93680722e-310  7.22179969e+201
5:   6.93680728e-310  6.93680722e-310 -1.06435402e-222  6.93680733e-310
5:   6.93680722e-310  2.45151350e-123  6.93680728e-310  6.93680722e-310
5:   2.10155289e-068  6.93680733e-310  6.93680722e-310 -9.25653718e-181]
5: 
5: Mismatched elements: 31 / 100 (31%)
5: Max absolute difference among violations: 8.96584634e+276
5: Max relative difference among violations: inf
5:  ACTUAL: array([ 7.339533e+223,  3.338254e+151,  5.234949e-316,  6.620480e-322,
5:         6.620480e-322,  1.671833e-303,  4.940656e-324,  2.914987e-322,
5:         1.021373e+173,  1.122094e+200,  3.241071e-321,  7.114545e-322,...
5:  DESIRED: array([-3.719829e-235,  6.936807e-310,  6.936807e-310,  5.592130e+169,
5:         6.936807e-310,  6.936807e-310,  5.605107e+203,  6.936807e-310,
5:         6.936807e-310, -5.737433e-196,  6.936807e-310,  6.936807e-310,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_Vector)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [ 5.25199133e-316  4.65782209e+151  3.70783145e-310  7.16395186e-322
5:   5.25170596e-316  4.22045499e-316  5.25168857e-316  5.92878775e-323
5:   9.93669318e+247  3.68148022e+180  0.00000000e+000  0.00000000e+000
5:   0.00000000e+000  4.31192064e-080  3.60554323e+252  3.60153652e+252
5:   4.32454994e-096  3.49104610e-033 -1.35530893e-042  2.58740964e+161
5:   1.73911107e-321  2.37151510e-322  5.25166881e-316  8.39911598e-323
5:   8.39911598e-323  6.78537599e-308  6.93786227e-310  2.77170827e-321
5:   5.25179687e-316  6.93786520e-310  4.09781745e-080  3.48191142e+227
5:   2.61645970e+180  3.68167801e+180  2.64887982e+180  3.94648579e+180
5:   4.03250054e+175  4.31192064e-080  3.60554323e+252  3.60153652e+252
5:   4.32455059e-096  1.65180090e-076  3.10055385e+169  2.58740964e+161
5:   1.95046116e+233  8.40411503e-315  7.90505033e-322  2.37151510e-322
5:   5.25169331e-316  6.42285340e-323  8.41470667e+276  2.15211916e-312
5:   8.29652574e-114  1.74405173e-321  5.25179687e-316  6.93786520e-310
5:   5.25170833e-316  7.41098469e-323  3.01467462e+161  1.64254070e-306
5:   2.64887982e+180  3.94648579e+180  4.03250054e+175  4.31192064e-080
5:   3.60554323e+252  1.12285550e+219  8.88782093e+252  6.72775353e+199
5:  -6.11685172e+100  1.99270017e+209  7.11454530e-322  2.37151510e-322
5:   5.25170359e-316  3.60705816e-085  2.04725594e+190  9.26741558e+175
5:   5.24253057e-320  7.95445690e-322  4.22045499e-316  5.25169568e-316
5:   4.09781745e-080  3.48191142e+227  2.61645970e+180  3.68167801e+180
5:   2.64887982e+180  3.94648579e+180  1.99108518e+209  1.53675404e+171
5:   1.98854893e-081  4.49576165e+251  7.50017970e+247  1.14323763e+243
5:   6.72775353e+199  2.41784537e+198  2.88067937e+214  2.38567043e-312
5:   4.74303020e-321  3.16202013e-322  5.25176130e-316  4.06900159e+233]
5:  eager grad out tensor:
5: [8.60243211e-067 1.26931635e-076 6.12104373e-062 5.20842942e-090
5:  2.90925466e-033 2.94942292e+179 5.40156029e-067 8.24021635e-067
5:  6.04708275e-154 7.27310988e-043 1.20766849e-153 2.09096487e-076
5:  1.48541334e-076 2.21460355e-052 7.25416364e-043 6.12104373e-062
5:  5.20267283e-090 2.90925466e-033 2.94942292e+179 3.44353328e-086
5:  8.23736941e-067 5.51684844e-048 8.60103991e-067 2.44151121e-154
5:  2.09096487e-076 1.48541334e-076 2.90191158e-057 6.73944700e-067
5:  8.62005337e-043 3.57280045e-062 6.12104373e-062 5.20267658e-090
5:  3.85589668e-086 1.58741262e-047 5.51688500e-048 8.23156822e-067
5:  6.03687449e-154 7.27310988e-043 1.20766849e-153 1.14284077e-071
5:  1.03979315e-042 8.62005337e-043 3.57280231e-062 6.12104373e-062
5:  5.20267658e-090 4.43687444e-038 3.76114153e+179 4.44891572e-086
5:  7.27311680e-043 5.98151388e-154 7.27310988e-043 1.20766849e-153
5:  1.91824281e-076 6.74927343e-067 6.52402046e-038 1.08539815e-071
5:  1.31531576e-047 3.57280046e-062 8.54737633e-072 4.08722444e+179
5:  4.44891573e-086 4.67104863e-062 5.51683794e-048 8.60103991e-067
5:  6.03687449e-154 4.85634684e-033 1.39738411e-259 6.52402046e-038
5:  1.08539817e-071 1.31531576e-047 3.57280046e-062 1.10711049e-047
5:  5.20266904e-090 2.90925466e-033 3.11214654e+179 5.51663955e-048
5:  8.60103991e-067 6.03687449e-154 4.18298011e-062 1.43170778e-153
5:  2.09096487e-076 1.48541334e-076 6.52402046e-038 1.08539815e-071
5:  1.48373282e-076 5.20941063e-090 2.90925466e-033 3.11214654e+179
5:  4.44891562e-086 6.75653396e-067 2.80470629e-033 1.31307520e-071
5:  9.49317552e-259 1.23064847e+171 9.45312995e+218 1.08248712e-071
5:  2.05423999e-115 7.31199202e+271 9.81676357e+252 1.14156408e+243]
5: 
5: Mismatched elements: 50 / 100 (50%)
5: Max absolute difference among violations: 8.41470667e+276
5: Max relative difference among violations: inf
5:  ACTUAL: array([ 5.251991e-316,  4.657822e+151,  3.707831e-310,  7.163952e-322,
5:         5.251706e-316,  4.220455e-316,  5.251689e-316,  5.928788e-323,
5:         9.936693e+247,  3.681480e+180,  0.000000e+000,  0.000000e+000,...
5:  DESIRED: array([8.602432e-067, 1.269316e-076, 6.121044e-062, 5.208429e-090,
5:        2.909255e-033, 2.949423e+179, 5.401560e-067, 8.240216e-067,
5:        6.047083e-154, 7.273110e-043, 1.207668e-153, 2.090965e-076,...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_Vector)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 100 / 100 (100%)
5: Max absolute difference among violations: 1.89140337e+23
5: Max relative difference among violations: 7.18736062e+23
5:  ACTUAL: array([ 0.000000e+000,  0.000000e+000,  0.000000e+000,  0.000000e+000,
5:         0.000000e+000,  0.000000e+000,  0.000000e+000,  0.000000e+000,
5:         1.363433e-154,  0.000000e+000,  0.000000e+000,  0.000000e+000,...
5:  DESIRED: array([6.629510e-01, 2.052597e-01, 6.979168e-01, 1.316473e-02,
5:        4.114911e-01, 4.001955e-02, 3.352950e-01, 2.217446e-01,
5:        1.887867e-01, 2.174650e-02, 3.963461e-01, 1.198556e-01,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 888, in check
5:     self.check_eager_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1038, in check_eager_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check eager comp grad out failed. Mismatch between eager comp and eager on Place(cpu), when enable_rev_comp is True,the eager comp grad out tensor's index is : 0 
5: eager comp grad out tensor:
5: 9.936693181797595e+247
5:  eager grad out tensor:
5: 5.2508016e-316
5: 
5: Mismatched elements: 1 / 1 (100%)
5: Max absolute difference among violations: 9.93669318e+247
5: Max relative difference among violations: inf
5:  ACTUAL: array(9.936693e+247)
5:  DESIRED: array(5.250802e-316)
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 1 / 1 (100%)
5: Max absolute difference among violations: 0.70394521
5: Max relative difference among violations: 1.
5:  ACTUAL: array(0.)
5:  DESIRED: array(0.703945)
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 888, in check
5:     self.check_eager_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1038, in check_eager_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check eager comp grad out failed. Mismatch between eager comp and eager on Place(cpu), when enable_rev_comp is True,the eager comp grad out tensor's index is : 0 
5: eager comp grad out tensor:
5: 2.516046264327181e-38
5:  eager grad out tensor:
5: 6.21701777808814e+277
5: 
5: Mismatched elements: 1 / 1 (100%)
5: Max absolute difference among violations: 6.21701778e+277
5: Max relative difference among violations: 1.
5:  ACTUAL: array(2.516046e-38)
5:  DESIRED: array(6.217018e+277)
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(4.783494628365118e+141) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (13, 17), dtype: float64) max gradient diff 4.783495e+141 over limit 1.000000e-07, the first error element is 0, expected 2.470328e-318, but got 4.783495e+138.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 776, in assert_array_compare
5:     flagged = func_assert_same_pos(x, y, func=isnan, hasval='nan')
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 1 
5: static comp grad out tensor:
5: nan
5:  eager grad out tensor:
5: 9.033787664087086e+271
5: 
5: nan location mismatch:
5:  ACTUAL: array(nan)
5:  DESIRED: array(9.033788e+271)
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 221 / 221 (100%)
5: Max absolute difference among violations: 5.80929323e+168
5: Max relative difference among violations: 7.0675376e+169
5:  ACTUAL: array([[0.000000e+000, 2.460125e-188, 2.057108e+144, 2.054822e+144,
5:         2.467330e-204, 1.991783e-141, 1.938356e+115, 1.476222e+053,
5:         0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000,...
5:  DESIRED: array([[0.298994, 0.270695, 0.244975, 0.340605, 0.084461, 0.044027,
5:         0.413657, 0.237152, 0.17091 , 0.186357, 0.355845, 0.191925,
5:         0.342749, 0.214519, 0.371493, 0.072851, 0.230335],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]
5:  [2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:   2.88067937e+214]]
5:  eager grad out tensor:
5: [[4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]
5:  [4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316 4.8309628e-316 4.8309628e-316 4.8309628e-316
5:   4.8309628e-316]]
5: 
5: Mismatched elements: 221 / 221 (100%)
5: Max absolute difference among violations: 2.88067937e+214
5: Max relative difference among violations: inf
5:  ACTUAL: array([[2.880679e+214, 2.880679e+214, 2.880679e+214, 2.880679e+214,
5:         2.880679e+214, 2.880679e+214, 2.880679e+214, 2.880679e+214,
5:         2.880679e+214, 2.880679e+214, 2.880679e+214, 2.880679e+214,...
5:  DESIRED: array([[4.830963e-316, 4.830963e-316, 4.830963e-316, 4.830963e-316,
5:         4.830963e-316, 4.830963e-316, 4.830963e-316, 4.830963e-316,
5:         4.830963e-316, 4.830963e-316, 4.830963e-316, 4.830963e-316,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3043, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 901, in check
5:     self.check_jit_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1276, in check_jit_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check jit comp grad out failed. Mismatch between jit comp and eager on Place(cpu), when enable_fw_comp is True, enable_rev_comp is True,the grad out tensor's index is : 0 
5: jit comp grad out tensor:
5: 4.790954202734058e+276
5:  eager grad out out tensor:
5: inf
5: 
5: +inf location mismatch:
5:  ACTUAL: array(4.790954e+276)
5:  DESIRED: array(inf)
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: 4.2996385706631105e+253
5:  eager grad out tensor:
5: 5.44469518054897e+257
5: 
5: Mismatched elements: 1 / 1 (100%)
5: Max absolute difference among violations: 5.44426522e+257
5: Max relative difference among violations: 0.99992103
5:  ACTUAL: array(4.299639e+253)
5:  DESIRED: array(5.444695e+257)
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_ZeroDim3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[5.292635e-063,           inf, 6.899062e+192,           inf,
5:                   inf, 1.222974e+100, 1.103391e+158,           inf,
5:         9.214929e-062, 2.401507e-068, 5.292675e-063, 8.087029e-057,...
5:  DESIRED: array([[0.433416, 0.392235, 0.54535 , 0.135232, 0.070493, 0.662315,
5:         0.37971 , 0.273648, 0.298381, 0.569752, 0.307296, 0.548783,
5:         0.34347 , 0.594806, 0.116643, 0.368794, 0.359902],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_0)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable Y (shape: (100,), dtype: float64) max gradient diff nan over limit 1.000000e-07, the first error element is 0, expected 0.000000e+00, but got 2.237962e+250.
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_0)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (100, 2, 3), dtype: float64) max gradient diff nan over limit 1.000000e-07, the first error element is 0, expected nan, but got 4.080142e-157.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_0)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (100, 2, 3), dtype: float64) max gradient diff nan over limit 1.000000e-07, the first error element is 0, expected nan, but got inf.
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_0)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[ 0.000000e+000,  0.000000e+000,  0.000000e+000],
5:         [ 0.000000e+000,  0.000000e+000,  0.000000e+000]],
5: ...
5:  DESIRED: array([[[6.007969e-02, 5.788356e-04, 5.204523e-02],
5:         [1.651137e-02, 7.555150e-02, 3.894109e-02]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(1.3076392855465597e+180) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable Y (shape: (100,), dtype: float64) max gradient diff 1.307639e+180 over limit 1.000000e-07, the first error element is 1, expected 0.000000e+00, but got 1.200317e+177.
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(2.4045378353856596e+267) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 100, 3), dtype: float64) max gradient diff 2.404538e+267 over limit 1.000000e-07, the first error element is 0, expected 1.520651e-03, but got 0.000000e+00.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 100, 3), dtype: float64) max gradient diff nan over limit 1.000000e-07, the first error element is 0, expected nan, but got inf.
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[0.000000e+000, 0.000000e+000, 0.000000e+000],
5:         [0.000000e+000, 0.000000e+000, 0.000000e+000],
5:         [0.000000e+000, 0.000000e+000, 0.000000e+000],...
5:  DESIRED: array([[[6.007969e-02, 5.788356e-04, 5.204523e-02],
5:         [1.346062e-01, 6.159213e-01, 3.174608e-01],
5:         [9.749661e-02, 6.746394e-02, 5.095833e-02],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 776, in assert_array_compare
5:     flagged = func_assert_same_pos(x, y, func=isnan, hasval='nan')
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [8.96381756e+276 9.93681786e+247 7.26581816e+223 8.15327662e+241
5:  5.37625007e+241 1.09909653e+248 3.99461109e+252 1.47104513e+248
5:  5.82663669e+252 1.20140535e+272 4.14066619e+233 9.01582969e+223
5:  1.06278608e+277             nan 2.10505582e+262 5.45984172e+257
5:  1.37299545e+214 7.50009795e+247 6.52022889e+252 3.06836125e+257
5:  4.63461863e+228 2.55019006e+224 2.92682894e+256 8.91634375e+252
5:  6.21472446e+228 5.44345563e+257 7.49501587e+247 9.66471474e+232
5:  9.79412959e+252 8.88792145e+252 5.51738542e+257 1.75182681e+267
5:  8.96584634e+276 1.48117632e+248 8.96584634e+276 2.44139418e+232
5:  7.50054757e+247 2.35999798e+232 3.60554323e+252 7.26593040e+223
5:  1.47144600e+253 9.01432502e+271 8.88774970e+252 3.49284725e+251
5:  1.04922556e+242 1.15282921e+253 4.71065384e+257 1.16564259e+253
5:  1.49545045e+248 8.88782412e+252 1.79275257e+277 9.15634626e+242
5:  6.82237101e+226 8.88789811e+252 1.52391109e+256 4.90908846e+252
5:  3.12765833e+265 5.34157777e+246 2.73873467e+257 5.45967888e+257
5:  1.24679773e+243 8.88782412e+252 2.35954087e+232 4.63461863e+228
5:  3.16152051e+233 1.04870816e+242 8.96381756e+276 8.41470667e+276
5:  8.88782223e+252 6.96411093e+252 3.85608588e+233 2.92256445e+256
5:  2.43624408e+232 5.21974948e+180 4.49576165e+251 9.35316437e+252
5:  5.83289446e+252 6.80552218e+212 7.35161510e+223 8.88782181e+252
5:  1.99270017e+209 1.20140535e+272 2.44140606e+232 5.83433429e+252
5:  2.10495972e+262 6.87281382e+246 2.63520411e+267 1.52788857e+224
5:  4.41031205e+257 5.76137972e+257 6.12501441e+257 9.76409756e+252
5:  1.20140535e+272 2.30908182e+251 5.82471389e+257 8.88782905e+252
5:  1.16565133e+253 8.96584634e+276 8.88782411e+252 7.50054757e+247]
5:  eager grad out tensor:
5: [0.00000000e+000 2.50625987e-108 1.35286953e-136 0.00000000e+000
5:  0.00000000e+000 0.00000000e+000 0.00000000e+000 6.86806750e-147
5:  0.00000000e+000 0.00000000e+000 1.57717157e-154 1.58804690e-154
5:  2.95506317e-159 0.00000000e+000 0.00000000e+000 3.22763841e-092
5:              inf             inf 1.60035498e+167 3.77173950e-151
5:  1.63016877e-160 4.35368719e-074 6.38606653e-049 0.00000000e+000
5:  0.00000000e+000 8.77864029e-166 0.00000000e+000 0.00000000e+000
5:  1.51620286e-101 1.39050143e-101 4.56372152e-316 3.18299369e-313
5:  2.93340183e-068 4.21628906e-062 6.93786383e-310 0.00000000e+000
5:  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:  0.00000000e+000 3.16689192e+144 3.60853494e+182 1.84527299e-068
5:  5.19436826e+176 3.20207404e-068 3.20200532e-068 0.00000000e+000
5:  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:  1.25986740e-321 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:  1.38098724e-071 1.38098724e-071 2.81076108e-065 6.05240686e+158
5:              inf             inf 1.87046973e-179 0.00000000e+000
5:  0.00000000e+000 0.00000000e+000 1.25986740e-321 4.94065646e-324
5:  4.94065646e-323 4.56374010e-316 6.94104409e-310 0.00000000e+000
5:  6.93785670e-310 6.93785670e-310 6.93786109e-310 4.56370295e-316
5:  3.18299369e-313 6.10472201e-136 2.86500787e-140 2.25006820e-063
5:  1.68432711e-164 4.24399159e-314 6.93786383e-310 0.00000000e+000
5:  0.00000000e+000 3.54277906e-087 0.00000000e+000 0.00000000e+000
5:  1.21354005e-181 0.00000000e+000 1.20459559e-044 0.00000000e+000]
5: 
5: nan location mismatch:
5:  ACTUAL: array([8.963818e+276, 9.936818e+247, 7.265818e+223, 8.153277e+241,
5:        5.376250e+241, 1.099097e+248, 3.994611e+252, 1.471045e+248,
5:        5.826637e+252, 1.201405e+272, 4.140666e+233, 9.015830e+223,...
5:  DESIRED: array([0.000000e+000, 2.506260e-108, 1.352870e-136, 0.000000e+000,
5:        0.000000e+000, 0.000000e+000, 0.000000e+000, 6.868067e-147,
5:        0.000000e+000, 0.000000e+000, 1.577172e-154, 1.588047e-154,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0.]
5:   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0.]
5:   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0.]]
5: 
5:  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0.]
5:   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0.]
5:   [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
5:    0. 0. 0. 0. 0. 0. 0. 0.]]]
5:  eager grad out tensor:
5: [[[6.02760088e-322 9.89843640e+169 5.53063250e-316 4.59481051e-322
5:    4.59481051e-322 6.00190289e+175 5.53068546e-316 6.07700744e-322
5:    6.07700744e-322 4.34256169e-114 5.53069258e-316 4.64421707e-322
5:    4.64421707e-322 6.01346931e-154 2.52961611e-321 5.53353523e-322
5:    5.22636395e-316 1.08359175e+142 1.05642658e+214 3.08803380e+223
5:    4.14115279e-114 4.72585285e+257 1.99906365e+161 2.51312285e+180
5:    2.04089393e-153 8.32973142e+151 3.71115879e+006 2.15182208e-053
5:    1.26931635e-076 1.22577687e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:    2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:    4.10698629e+059 4.44317707e+064 2.61352513e-306 1.17044151e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    8.51512316e-096 1.11510616e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 1.31643272e+294 2.90253149e-057 1.05977081e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    1.15298046e-259 1.00443546e-320 4.61070598e-316 4.80334890e-316
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 6.81018598e-310 6.01347002e-154 9.49100106e-321]
5:   [6.02760088e-322 9.89843640e+169 5.53063250e-316 4.59481051e-322
5:    4.59481051e-322 6.00190289e+175 5.53068546e-316 6.07700744e-322
5:    6.07700744e-322 4.34256169e-114 5.53069258e-316 4.64421707e-322
5:    4.64421707e-322 6.01346931e-154 2.52961611e-321 5.53353523e-322
5:    5.22636395e-316 1.08359175e+142 1.05642658e+214 3.08803380e+223
5:    4.14115279e-114 4.72585285e+257 1.99906365e+161 2.51312285e+180
5:    2.04089393e-153 8.32973142e+151 3.71115879e+006 2.15182208e-053
5:    1.26931635e-076 1.22577687e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:    2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:    4.10698629e+059 4.44317707e+064 2.61352513e-306 1.17044151e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    8.51512316e-096 1.11510616e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 1.31643272e+294 2.90253149e-057 1.05977081e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    1.15298046e-259 1.00443546e-320 4.61070598e-316 4.80334890e-316
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 6.81018598e-310 6.01347002e-154 9.49100106e-321]
5:   [6.02760088e-322 9.89843640e+169 5.53063250e-316 4.59481051e-322
5:    4.59481051e-322 6.00190289e+175 5.53068546e-316 6.07700744e-322
5:    6.07700744e-322 4.34256169e-114 5.53069258e-316 4.64421707e-322
5:    4.64421707e-322 6.01346931e-154 2.52961611e-321 5.53353523e-322
5:    5.22636395e-316 1.08359175e+142 1.05642658e+214 3.08803380e+223
5:    4.14115279e-114 4.72585285e+257 1.99906365e+161 2.51312285e+180
5:    2.04089393e-153 8.32973142e+151 3.71115879e+006 2.15182208e-053
5:    1.26931635e-076 1.22577687e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:    2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:    4.10698629e+059 4.44317707e+064 2.61352513e-306 1.17044151e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    8.51512316e-096 1.11510616e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 1.31643272e+294 2.90253149e-057 1.05977081e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    1.15298046e-259 1.00443546e-320 4.61070598e-316 4.80334890e-316
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 6.81018598e-310 6.01347002e-154 9.49100106e-321]]
5: 
5:  [[6.02760088e-322 9.89843640e+169 5.53063250e-316 4.59481051e-322
5:    4.59481051e-322 6.00190289e+175 5.53068546e-316 6.07700744e-322
5:    6.07700744e-322 4.34256169e-114 5.53069258e-316 4.64421707e-322
5:    4.64421707e-322 6.01346931e-154 2.52961611e-321 5.53353523e-322
5:    5.22636395e-316 1.08359175e+142 1.05642658e+214 3.08803380e+223
5:    4.14115279e-114 4.72585285e+257 1.99906365e+161 2.51312285e+180
5:    2.04089393e-153 8.32973142e+151 3.71115879e+006 2.15182208e-053
5:    1.26931635e-076 1.22577687e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:    2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:    4.10698629e+059 4.44317707e+064 2.61352513e-306 1.17044151e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    8.51512316e-096 1.11510616e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 1.31643272e+294 2.90253149e-057 1.05977081e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    1.15298046e-259 1.00443546e-320 4.61070598e-316 4.80334890e-316
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 6.81018598e-310 6.01347002e-154 9.49100106e-321]
5:   [6.02760088e-322 9.89843640e+169 5.53063250e-316 4.59481051e-322
5:    4.59481051e-322 6.00190289e+175 5.53068546e-316 6.07700744e-322
5:    6.07700744e-322 4.34256169e-114 5.53069258e-316 4.64421707e-322
5:    4.64421707e-322 6.01346931e-154 2.52961611e-321 5.53353523e-322
5:    5.22636395e-316 1.08359175e+142 1.05642658e+214 3.08803380e+223
5:    4.14115279e-114 4.72585285e+257 1.99906365e+161 2.51312285e+180
5:    2.04089393e-153 8.32973142e+151 3.71115879e+006 2.15182208e-053
5:    1.26931635e-076 1.22577687e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:    2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:    4.10698629e+059 4.44317707e+064 2.61352513e-306 1.17044151e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    8.51512316e-096 1.11510616e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 1.31643272e+294 2.90253149e-057 1.05977081e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    1.15298046e-259 1.00443546e-320 4.61070598e-316 4.80334890e-316
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 6.81018598e-310 6.01347002e-154 9.49100106e-321]
5:   [6.02760088e-322 9.89843640e+169 5.53063250e-316 4.59481051e-322
5:    4.59481051e-322 6.00190289e+175 5.53068546e-316 6.07700744e-322
5:    6.07700744e-322 4.34256169e-114 5.53069258e-316 4.64421707e-322
5:    4.64421707e-322 6.01346931e-154 2.52961611e-321 5.53353523e-322
5:    5.22636395e-316 1.08359175e+142 1.05642658e+214 3.08803380e+223
5:    4.14115279e-114 4.72585285e+257 1.99906365e+161 2.51312285e+180
5:    2.04089393e-153 8.32973142e+151 3.71115879e+006 2.15182208e-053
5:    1.26931635e-076 1.22577687e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 4.24048879e+175 9.61987860e+228
5:    2.17150608e+214 5.26033033e+170 9.64887928e-023 1.01820900e+277
5:    4.10698629e+059 4.44317707e+064 2.61352513e-306 1.17044151e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    8.51512316e-096 1.11510616e-320 4.80339000e-316 6.93786520e-310
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 1.31643272e+294 2.90253149e-057 1.05977081e-320
5:    4.80339000e-316 6.93786520e-310 0.00000000e+000 0.00000000e+000
5:    7.34514290e+223 5.00404667e+160 2.04089393e-153 5.50656807e+213
5:    5.70540353e-109 2.58468518e+161 1.90221584e+132 2.61352510e-306
5:    1.15298046e-259 1.00443546e-320 4.61070598e-316 4.80334890e-316
5:    0.00000000e+000 0.00000000e+000 3.94355938e+180 4.79137541e+276
5:    1.26001517e+232 1.99034017e+161 6.06352263e+140 9.30684868e+242
5:    1.26890749e+079 6.81018598e-310 6.01347002e-154 9.49100106e-321]]]
5: 
5: Mismatched elements: 252 / 600 (42%)
5: Max absolute difference among violations: 1.31643272e+294
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
5:          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
5:          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,...
5:  DESIRED: array([[[6.027601e-322, 9.898436e+169, 5.530632e-316, 4.594811e-322,
5:          4.594811e-322, 6.001903e+175, 5.530685e-316, 6.077007e-322,
5:          6.077007e-322, 4.342562e-114, 5.530693e-316, 4.644217e-322,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[4.08784766e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.43822948e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 4.08658136e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.11149856e+164 1.00000000e-015 1.00000000e-015 3.27803846e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015]
5:   [4.08784766e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.43822948e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 4.08658136e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.11149856e+164 1.00000000e-015 1.00000000e-015 3.27803846e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015]
5:   [4.08784766e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.43822948e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 4.08658136e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.11149856e+164 1.00000000e-015 1.00000000e-015 3.27803846e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015]]
5: 
5:  [[4.08784766e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.43822948e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 4.08658136e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.11149856e+164 1.00000000e-015 1.00000000e-015 3.27803846e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015]
5:   [4.08784766e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.43822948e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 4.08658136e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.11149856e+164 1.00000000e-015 1.00000000e-015 3.27803846e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015]
5:   [4.08784766e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.43822948e+164 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 4.08658136e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    3.11149856e+164 1.00000000e-015 1.00000000e-015 3.27803846e+164
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015
5:    1.00000000e-015 1.00000000e-015 1.00000000e-015 1.00000000e-015]]]
5:  eager grad out tensor:
5: [[[1.08539816e-071 1.03979508e-042 6.75072234e-067 6.01347002e-154
5:    1.03979508e-042 6.75072234e-067 1.90057646e-052 9.45331423e-096
5:    4.27330755e-033 1.05206192e-153 7.71100199e-043 2.44005592e-154
5:    6.01347002e-154 7.71100199e-043 5.53288499e-048 4.08658136e+179
5:    3.64464661e-086 5.20650805e-090 1.57690623e-052 3.57280603e-062
5:    6.01346953e-154 5.60229622e-067 3.57280420e-062 4.75500310e-038
5:    2.73526121e-057 1.44975581e-047 2.73947681e-057 2.09163556e-076
5:    8.98698246e-096 6.01347002e-154 9.73704843e-072 9.45331423e-096
5:    2.52741198e-052 1.05220986e-153 3.53481178e-057 2.99938018e-067
5:    3.11149856e+179 5.98149110e-154 2.99063837e-067 3.27803846e+179
5:    4.85110541e-086 1.21112017e-099 6.77100908e-043 1.18295664e-076
5:    1.79372292e-052 1.68899158e-052 6.01347002e-154 1.24395599e-047
5:    7.25416369e-043 1.89975285e-052 1.57177503e-076 4.66443126e-062
5:    1.03896638e-095 1.65690032e-047 1.14583445e-259 6.01347002e-154
5:    1.39804329e-076 1.05118732e-153 1.39804329e-076 1.28822273e-057
5:    4.08784766e+179 5.05218190e-086 1.21290958e-099 6.01334510e-154
5:    3.64460956e-086 1.21223374e-099 5.60229635e-067 7.75258574e-072
5:    3.53356694e-057 6.00050364e-067 5.05115942e-038 1.04014348e-042
5:    6.01347002e-154 1.10790921e-047 4.08069160e-033 1.08690342e-071
5:    9.46245093e-096 7.86350579e-067 1.05206135e-153 7.49402844e-067
5:    2.44005308e-154 6.01347002e-154 2.17631446e-076 5.53290585e-048
5:    3.43822948e+179 3.24249363e-086 1.21089429e-099 1.30354289e-076
5:    1.18295070e-076 6.01346953e-154 3.67247041e-062 5.08074309e-067
5:    1.39737509e-076 1.17478576e-047 1.74348740e-076 5.15151388e-062
5:    2.31656781e-052 8.98515368e-096 6.01347002e-154 3.37549762e-057]
5:   [1.08539816e-071 1.03979508e-042 6.75072234e-067 6.01347002e-154
5:    1.03979508e-042 6.75072234e-067 1.90057646e-052 9.45331423e-096
5:    4.27330755e-033 1.05206192e-153 7.71100199e-043 2.44005592e-154
5:    6.01347002e-154 7.71100199e-043 5.53288499e-048 4.08658136e+179
5:    3.64464661e-086 5.20650805e-090 1.57690623e-052 3.57280603e-062
5:    6.01346953e-154 5.60229622e-067 3.57280420e-062 4.75500310e-038
5:    2.73526121e-057 1.44975581e-047 2.73947681e-057 2.09163556e-076
5:    8.98698246e-096 6.01347002e-154 9.73704843e-072 9.45331423e-096
5:    2.52741198e-052 1.05220986e-153 3.53481178e-057 2.99938018e-067
5:    3.11149856e+179 5.98149110e-154 2.99063837e-067 3.27803846e+179
5:    4.85110541e-086 1.21112017e-099 6.77100908e-043 1.18295664e-076
5:    1.79372292e-052 1.68899158e-052 6.01347002e-154 1.24395599e-047
5:    7.25416369e-043 1.89975285e-052 1.57177503e-076 4.66443126e-062
5:    1.03896638e-095 1.65690032e-047 1.14583445e-259 6.01347002e-154
5:    1.39804329e-076 1.05118732e-153 1.39804329e-076 1.28822273e-057
5:    4.08784766e+179 5.05218190e-086 1.21290958e-099 6.01334510e-154
5:    3.64460956e-086 1.21223374e-099 5.60229635e-067 7.75258574e-072
5:    3.53356694e-057 6.00050364e-067 5.05115942e-038 1.04014348e-042
5:    6.01347002e-154 1.10790921e-047 4.08069160e-033 1.08690342e-071
5:    9.46245093e-096 7.86350579e-067 1.05206135e-153 7.49402844e-067
5:    2.44005308e-154 6.01347002e-154 2.17631446e-076 5.53290585e-048
5:    3.43822948e+179 3.24249363e-086 1.21089429e-099 1.30354289e-076
5:    1.18295070e-076 6.01346953e-154 3.67247041e-062 5.08074309e-067
5:    1.39737509e-076 1.17478576e-047 1.74348740e-076 5.15151388e-062
5:    2.31656781e-052 8.98515368e-096 6.01347002e-154 3.37549762e-057]
5:   [1.08539816e-071 1.03979508e-042 6.75072234e-067 6.01347002e-154
5:    1.03979508e-042 6.75072234e-067 1.90057646e-052 9.45331423e-096
5:    4.27330755e-033 1.05206192e-153 7.71100199e-043 2.44005592e-154
5:    6.01347002e-154 7.71100199e-043 5.53288499e-048 4.08658136e+179
5:    3.64464661e-086 5.20650805e-090 1.57690623e-052 3.57280603e-062
5:    6.01346953e-154 5.60229622e-067 3.57280420e-062 4.75500310e-038
5:    2.73526121e-057 1.44975581e-047 2.73947681e-057 2.09163556e-076
5:    8.98698246e-096 6.01347002e-154 9.73704843e-072 9.45331423e-096
5:    2.52741198e-052 1.05220986e-153 3.53481178e-057 2.99938018e-067
5:    3.11149856e+179 5.98149110e-154 2.99063837e-067 3.27803846e+179
5:    4.85110541e-086 1.21112017e-099 6.77100908e-043 1.18295664e-076
5:    1.79372292e-052 1.68899158e-052 6.01347002e-154 1.24395599e-047
5:    7.25416369e-043 1.89975285e-052 1.57177503e-076 4.66443126e-062
5:    1.03896638e-095 1.65690032e-047 1.14583445e-259 6.01347002e-154
5:    1.39804329e-076 1.05118732e-153 1.39804329e-076 1.28822273e-057
5:    4.08784766e+179 5.05218190e-086 1.21290958e-099 6.01334510e-154
5:    3.64460956e-086 1.21223374e-099 5.60229635e-067 7.75258574e-072
5:    3.53356694e-057 6.00050364e-067 5.05115942e-038 1.04014348e-042
5:    6.01347002e-154 1.10790921e-047 4.08069160e-033 1.08690342e-071
5:    9.46245093e-096 7.86350579e-067 1.05206135e-153 7.49402844e-067
5:    2.44005308e-154 6.01347002e-154 2.17631446e-076 5.53290585e-048
5:    3.43822948e+179 3.24249363e-086 1.21089429e-099 1.30354289e-076
5:    1.18295070e-076 6.01346953e-154 3.67247041e-062 5.08074309e-067
5:    1.39737509e-076 1.17478576e-047 1.74348740e-076 5.15151388e-062
5:    2.31656781e-052 8.98515368e-096 6.01347002e-154 3.37549762e-057]]
5: 
5:  [[1.08539816e-071 1.03979508e-042 6.75072234e-067 6.01347002e-154
5:    1.03979508e-042 6.75072234e-067 1.90057646e-052 9.45331423e-096
5:    4.27330755e-033 1.05206192e-153 7.71100199e-043 2.44005592e-154
5:    6.01347002e-154 7.71100199e-043 5.53288499e-048 4.08658136e+179
5:    3.64464661e-086 5.20650805e-090 1.57690623e-052 3.57280603e-062
5:    6.01346953e-154 5.60229622e-067 3.57280420e-062 4.75500310e-038
5:    2.73526121e-057 1.44975581e-047 2.73947681e-057 2.09163556e-076
5:    8.98698246e-096 6.01347002e-154 9.73704843e-072 9.45331423e-096
5:    2.52741198e-052 1.05220986e-153 3.53481178e-057 2.99938018e-067
5:    3.11149856e+179 5.98149110e-154 2.99063837e-067 3.27803846e+179
5:    4.85110541e-086 1.21112017e-099 6.77100908e-043 1.18295664e-076
5:    1.79372292e-052 1.68899158e-052 6.01347002e-154 1.24395599e-047
5:    7.25416369e-043 1.89975285e-052 1.57177503e-076 4.66443126e-062
5:    1.03896638e-095 1.65690032e-047 1.14583445e-259 6.01347002e-154
5:    1.39804329e-076 1.05118732e-153 1.39804329e-076 1.28822273e-057
5:    4.08784766e+179 5.05218190e-086 1.21290958e-099 6.01334510e-154
5:    3.64460956e-086 1.21223374e-099 5.60229635e-067 7.75258574e-072
5:    3.53356694e-057 6.00050364e-067 5.05115942e-038 1.04014348e-042
5:    6.01347002e-154 1.10790921e-047 4.08069160e-033 1.08690342e-071
5:    9.46245093e-096 7.86350579e-067 1.05206135e-153 7.49402844e-067
5:    2.44005308e-154 6.01347002e-154 2.17631446e-076 5.53290585e-048
5:    3.43822948e+179 3.24249363e-086 1.21089429e-099 1.30354289e-076
5:    1.18295070e-076 6.01346953e-154 3.67247041e-062 5.08074309e-067
5:    1.39737509e-076 1.17478576e-047 1.74348740e-076 5.15151388e-062
5:    2.31656781e-052 8.98515368e-096 6.01347002e-154 3.37549762e-057]
5:   [1.08539816e-071 1.03979508e-042 6.75072234e-067 6.01347002e-154
5:    1.03979508e-042 6.75072234e-067 1.90057646e-052 9.45331423e-096
5:    4.27330755e-033 1.05206192e-153 7.71100199e-043 2.44005592e-154
5:    6.01347002e-154 7.71100199e-043 5.53288499e-048 4.08658136e+179
5:    3.64464661e-086 5.20650805e-090 1.57690623e-052 3.57280603e-062
5:    6.01346953e-154 5.60229622e-067 3.57280420e-062 4.75500310e-038
5:    2.73526121e-057 1.44975581e-047 2.73947681e-057 2.09163556e-076
5:    8.98698246e-096 6.01347002e-154 9.73704843e-072 9.45331423e-096
5:    2.52741198e-052 1.05220986e-153 3.53481178e-057 2.99938018e-067
5:    3.11149856e+179 5.98149110e-154 2.99063837e-067 3.27803846e+179
5:    4.85110541e-086 1.21112017e-099 6.77100908e-043 1.18295664e-076
5:    1.79372292e-052 1.68899158e-052 6.01347002e-154 1.24395599e-047
5:    7.25416369e-043 1.89975285e-052 1.57177503e-076 4.66443126e-062
5:    1.03896638e-095 1.65690032e-047 1.14583445e-259 6.01347002e-154
5:    1.39804329e-076 1.05118732e-153 1.39804329e-076 1.28822273e-057
5:    4.08784766e+179 5.05218190e-086 1.21290958e-099 6.01334510e-154
5:    3.64460956e-086 1.21223374e-099 5.60229635e-067 7.75258574e-072
5:    3.53356694e-057 6.00050364e-067 5.05115942e-038 1.04014348e-042
5:    6.01347002e-154 1.10790921e-047 4.08069160e-033 1.08690342e-071
5:    9.46245093e-096 7.86350579e-067 1.05206135e-153 7.49402844e-067
5:    2.44005308e-154 6.01347002e-154 2.17631446e-076 5.53290585e-048
5:    3.43822948e+179 3.24249363e-086 1.21089429e-099 1.30354289e-076
5:    1.18295070e-076 6.01346953e-154 3.67247041e-062 5.08074309e-067
5:    1.39737509e-076 1.17478576e-047 1.74348740e-076 5.15151388e-062
5:    2.31656781e-052 8.98515368e-096 6.01347002e-154 3.37549762e-057]
5:   [1.08539816e-071 1.03979508e-042 6.75072234e-067 6.01347002e-154
5:    1.03979508e-042 6.75072234e-067 1.90057646e-052 9.45331423e-096
5:    4.27330755e-033 1.05206192e-153 7.71100199e-043 2.44005592e-154
5:    6.01347002e-154 7.71100199e-043 5.53288499e-048 4.08658136e+179
5:    3.64464661e-086 5.20650805e-090 1.57690623e-052 3.57280603e-062
5:    6.01346953e-154 5.60229622e-067 3.57280420e-062 4.75500310e-038
5:    2.73526121e-057 1.44975581e-047 2.73947681e-057 2.09163556e-076
5:    8.98698246e-096 6.01347002e-154 9.73704843e-072 9.45331423e-096
5:    2.52741198e-052 1.05220986e-153 3.53481178e-057 2.99938018e-067
5:    3.11149856e+179 5.98149110e-154 2.99063837e-067 3.27803846e+179
5:    4.85110541e-086 1.21112017e-099 6.77100908e-043 1.18295664e-076
5:    1.79372292e-052 1.68899158e-052 6.01347002e-154 1.24395599e-047
5:    7.25416369e-043 1.89975285e-052 1.57177503e-076 4.66443126e-062
5:    1.03896638e-095 1.65690032e-047 1.14583445e-259 6.01347002e-154
5:    1.39804329e-076 1.05118732e-153 1.39804329e-076 1.28822273e-057
5:    4.08784766e+179 5.05218190e-086 1.21290958e-099 6.01334510e-154
5:    3.64460956e-086 1.21223374e-099 5.60229635e-067 7.75258574e-072
5:    3.53356694e-057 6.00050364e-067 5.05115942e-038 1.04014348e-042
5:    6.01347002e-154 1.10790921e-047 4.08069160e-033 1.08690342e-071
5:    9.46245093e-096 7.86350579e-067 1.05206135e-153 7.49402844e-067
5:    2.44005308e-154 6.01347002e-154 2.17631446e-076 5.53290585e-048
5:    3.43822948e+179 3.24249363e-086 1.21089429e-099 1.30354289e-076
5:    1.18295070e-076 6.01346953e-154 3.67247041e-062 5.08074309e-067
5:    1.39737509e-076 1.17478576e-047 1.74348740e-076 5.15151388e-062
5:    2.31656781e-052 8.98515368e-096 6.01347002e-154 3.37549762e-057]]]
5: 
5: Mismatched elements: 60 / 600 (10%)
5: Max absolute difference among violations: 4.08784766e+179
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[4.087848e+164, 1.000000e-015, 1.000000e-015, 1.000000e-015,
5:          1.000000e-015, 1.000000e-015, 1.000000e-015, 1.000000e-015,
5:          1.000000e-015, 1.000000e-015, 1.000000e-015, 1.000000e-015,...
5:  DESIRED: array([[[1.085398e-071, 1.039795e-042, 6.750722e-067, 6.013470e-154,
5:          1.039795e-042, 6.750722e-067, 1.900576e-052, 9.453314e-096,
5:          4.273308e-033, 1.052062e-153, 7.711002e-043, 2.440056e-154,...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[ 3.869555e+069,  0.000000e+000,  0.000000e+000,  0.000000e+000,
5:           0.000000e+000,  0.000000e+000,  7.490887e+232,  0.000000e+000,
5:           0.000000e+000,  0.000000e+000,  0.000000e+000,  1.171191e+151,...
5:  DESIRED: array([[[6.007969e-02, 4.718862e-03, 1.223461e-01, 4.248121e-02,
5:          1.044858e-01, 2.472815e-01, 2.256950e-01, 8.556311e-02,
5:          1.510760e-01, 1.244100e-01, 3.993570e-01, 2.573475e-01,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable Y (shape: (10, 12), dtype: float64) max gradient diff nan over limit 1.000000e-07, the first error element is 0, expected nan, but got 4.775298e+176.
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(3.1223881014440617e+214) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 10, 12, 3), dtype: float64) max gradient diff 3.122388e+214 over limit 1.000000e-07, the first error element is 0, expected 7.756914e-04, but got 7.518098e-319.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(nan) not less than or equal to 1e-07 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 10, 12, 3), dtype: float64) max gradient diff nan over limit 1.000000e-07, the first error element is 0, expected nan, but got 2.401555e-66.
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_3)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 720 / 720 (100%)
5: Max absolute difference among violations: 8.17861658e+206
5: Max relative difference among violations: 4.71098801e+207
5:  ACTUAL: array([[[[ 0.000000e+000,  2.384695e-134,  9.254321e-125],
5:          [ 0.000000e+000,  0.000000e+000,  0.000000e+000],
5:          [ 0.000000e+000,  0.000000e+000,  1.861653e-164],...
5:  DESIRED: array([[[[8.743665e-01, 2.833723e-01, 4.187881e-01],
5:          [3.601631e-02, 8.987739e-02, 9.156039e-02],
5:          [4.129678e-02, 9.470335e-02, 4.947683e-02],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_4)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 894, in check
5:     self.check_jit_comp_with_cinn()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1407, in check_jit_comp_with_cinn
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check jit comp with cinn grad out failed. Mismatch between jit comp with cinn and eager on Place(cpu), when enable_fw_comp is True, enable_rev_comp is True, enable_cinn is False,the grad out tensor's index is : 0 ,jit comp with cinn grad out tensor:
5: [[[1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]
5: 
5:  [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]]
5:  eager grad out out tensor:
5: [[[0.98987267 0.92110943 0.67564105 1.00500465 0.86835566 0.84234017
5:    0.45994821 1.23161122 0.80859098 0.48400449 0.37085665]]
5: 
5:  [[0.12626742 0.37622699 0.51897137 0.3451522  1.16725216 0.14848227
5:    0.56475316 0.17061589 1.01829702 0.18204508 0.59247628]]
5: 
5:  [[0.40162154 0.7661147  0.32158186 0.68169567 0.97753299 0.84522975
5:    0.85574024 0.82044694 0.32216671 0.31780483 0.82674498]]
5: 
5:  [[0.38658717 0.53093219 0.57238829 0.07997591 0.11159569 0.11860719
5:    0.66321736 1.12749804 0.68908129 0.14034411 0.20754036]]
5: 
5:  [[0.58071742 0.67750711 0.03103443 0.28264704 0.52356725 0.29278662
5:    0.74125149 0.31769571 0.22373644 0.65383579 0.98129134]]
5: 
5:  [[0.44512471 0.204454   0.58708965 0.26289437 1.09284538 0.26945559
5:    0.99909645 0.08817807 0.68796363 0.57284704 0.45744446]]
5: 
5:  [[0.11848463 0.38274642 0.61306675 0.40280811 0.14379484 0.30411105
5:    1.36409891 0.75186283 1.14451119 0.61575169 0.34039704]]
5: 
5:  [[0.71676629 1.07745718 0.77019204 0.96204244 0.44903451 0.06364437
5:    0.22605168 0.54457999 0.98877109 1.19644343 0.43096447]]
5: 
5:  [[0.4885937  0.83552658 0.46086478 0.54638559 1.12778978 0.58532166
5:    0.87844792 0.50603872 0.94928928 0.74377997 0.42768596]]
5: 
5:  [[0.12266277 0.4867895  0.46843976 0.68775715 0.94299607 0.7241036
5:    0.30704921 0.546169   0.59676299 0.42670991 0.40146098]]]
5: 
5: Mismatched elements: 110 / 110 (100%)
5: Max absolute difference among violations: 1.96896557
5: Max relative difference among violations: 63.44454971
5:  ACTUAL: array([[[1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2.]],
5: 
5:        [[2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]],...
5:  DESIRED: array([[[0.989873, 0.921109, 0.675641, 1.005005, 0.868356, 0.84234 ,
5:          0.459948, 1.231611, 0.808591, 0.484004, 0.370857]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_4)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 888, in check
5:     self.check_eager_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1038, in check_eager_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check eager comp grad out failed. Mismatch between eager comp and eager on Place(cpu), when enable_rev_comp is True,the eager comp grad out tensor's index is : 0 
5: eager comp grad out tensor:
5: [[[4.61120479e-316 4.56819815e-316 1.48219694e-323 2.37151510e-322
5:    0.00000000e+000 4.61121190e-316 4.61121190e-316 4.79959004e-316
5:    9.88131292e-324 4.61112258e-316 6.42285340e-323]
5:   [4.61120479e-316 4.56819815e-316 1.48219694e-323 2.37151510e-322
5:    0.00000000e+000 4.61121190e-316 4.61121190e-316 4.79959004e-316
5:    9.88131292e-324 4.61112258e-316 6.42285340e-323]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 5.26354425e-315 1.25986740e-321
5:    0.00000000e+000 5.57776952e-316 6.42285340e-323 0.00000000e+000
5:    0.00000000e+000 5.26354425e-315 6.42285340e-323]
5:   [0.00000000e+000 0.00000000e+000 5.26354425e-315 1.25986740e-321
5:    0.00000000e+000 5.57776952e-316 6.42285340e-323 0.00000000e+000
5:    0.00000000e+000 5.26354425e-315 6.42285340e-323]]
5: 
5:  [[0.00000000e+000 3.49104610e-033 1.30828583e-320 1.30729770e-320
5:    6.95644429e-321 2.37151510e-322 3.14203538e-316 9.88131292e-323
5:    9.88131292e-323 6.78537599e-308 7.19359580e-321]
5:   [0.00000000e+000 3.49104610e-033 1.30828583e-320 1.30729770e-320
5:    6.95644429e-321 2.37151510e-322 3.14203538e-316 9.88131292e-323
5:    9.88131292e-323 6.78537599e-308 7.19359580e-321]]
5: 
5:  [[2.37151510e-322 4.61120163e-316 4.94065646e-324 4.09781745e-080
5:    3.48191142e+227 7.43074731e-321 2.37151510e-322 4.61121586e-316
5:    4.94065646e-324 4.03250054e+175 4.31192064e-080]
5:   [2.37151510e-322 4.61120163e-316 4.94065646e-324 4.09781745e-080
5:    3.48191142e+227 7.43074731e-321 2.37151510e-322 4.61121586e-316
5:    4.94065646e-324 4.03250054e+175 4.31192064e-080]]
5: 
5:  [[7.66789882e-321 4.74303020e-322 4.56818551e-316 4.56818630e-316
5:    0.00000000e+000 0.00000000e+000 4.61120716e-316 3.95252517e-323
5:    6.39046855e+020 0.00000000e+000 4.61120953e-316]
5:   [7.66789882e-321 4.74303020e-322 4.56818551e-316 4.56818630e-316
5:    0.00000000e+000 0.00000000e+000 4.61120716e-316 3.95252517e-323
5:    6.39046855e+020 0.00000000e+000 4.61120953e-316]]
5: 
5:  [[4.61121111e-316 4.61121111e-316 2.42092166e-322 3.14201482e-316
5:    4.80239476e-316 6.39046855e+020 3.94648579e+180 8.37935335e-321
5:    4.74303020e-322 4.79958925e-316 4.61118819e-316]
5:   [4.61121111e-316 4.61121111e-316 2.42092166e-322 3.14201482e-316
5:    4.80239476e-316 6.39046855e+020 3.94648579e+180 8.37935335e-321
5:    4.74303020e-322 4.79958925e-316 4.61118819e-316]]
5: 
5:  [[0.00000000e+000 4.79959004e-316 4.61121428e-316 2.96439388e-323
5:    3.70783145e-310 2.37151510e-322 4.61121665e-316 4.61121823e-316
5:    4.61121823e-316 1.45502333e-320 5.23045244e-316]
5:   [0.00000000e+000 4.79959004e-316 4.61121428e-316 2.96439388e-323
5:    3.70783145e-310 2.37151510e-322 4.61121665e-316 4.61121823e-316
5:    4.61121823e-316 1.45502333e-320 5.23045244e-316]]
5: 
5:  [[5.57221702e-316 0.00000000e+000 0.00000000e+000 2.37151510e-322
5:    3.16202013e-322 4.52832113e-316 3.60705816e-085 2.04725594e+190
5:    9.26741558e+175 9.53245539e-315 0.00000000e+000]
5:   [5.57221702e-316 0.00000000e+000 0.00000000e+000 2.37151510e-322
5:    3.16202013e-322 4.52832113e-316 3.60705816e-085 2.04725594e+190
5:    9.26741558e+175 9.53245539e-315 0.00000000e+000]]
5: 
5:  [[3.16202013e-322 6.40803143e-321 4.61133127e-316 4.61132495e-316
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [3.16202013e-322 6.40803143e-321 4.61133127e-316 4.61132495e-316
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]]
5:  eager grad out tensor:
5: [[[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 2.71198838e-098 7.52017058e-136 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 2.71198838e-098 7.52017058e-136 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 2.71198838e-098 7.52017058e-136 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 2.71198838e-098 7.52017058e-136 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 3.65447516e-098 1.23807526e-135 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 3.65447516e-098 1.23807526e-135 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 3.65447516e-098 1.23807526e-135 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 3.65447516e-098 1.23807526e-135 0.00000000e+000
5:    0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 8.40042067e-107 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 7.46379355e-146 8.52198262e-155]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 8.40042067e-107 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 7.46379355e-146 8.52198262e-155]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 8.40042067e-107 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 7.46379355e-146 8.52198262e-155]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 8.40042067e-107 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 7.46379355e-146 8.52198262e-155]]]
5: 
5: Mismatched elements: 14 / 220 (6.36%)
5: Max absolute difference among violations: 3.48191142e+227
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[4.611205e-316, 4.568198e-316, 1.482197e-323, 2.371515e-322,
5:          0.000000e+000, 4.611212e-316, 4.611212e-316, 4.799590e-316,
5:          9.881313e-324, 4.611123e-316, 6.422853e-323],...
5:  DESIRED: array([[[0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000,
5:          0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000,
5:          0.000000e+000, 0.000000e+000, 0.000000e+000],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_4)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[6.9378652e-310 6.9378652e-310 4.8024248e-316 4.8024248e-316
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [6.9378652e-310 6.9378652e-310 4.8024248e-316 4.8024248e-316
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]
5: 
5:  [[1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]
5:   [1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000 1.0000000e+000
5:    1.0000000e+000 1.0000000e+000 1.0000000e+000]]]
5:  eager grad out tensor:
5: [[[0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 4.61121190e-316 4.61121190e-316 4.79959004e-316
5:    9.88131292e-324 4.61112258e-316 6.42285340e-323]
5:   [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:    0.00000000e+000 4.61121190e-316 4.61121190e-316 4.79959004e-316
5:    9.88131292e-324 4.61112258e-316 6.42285340e-323]]
5: 
5:  [[4.61120479e-316 4.56819815e-316 1.48219694e-323 2.37151510e-322
5:    0.00000000e+000 4.61121190e-316 4.61121190e-316 4.79959004e-316
5:    9.88131292e-324 4.61112258e-316 6.42285340e-323]
5:   [4.61120479e-316 4.56819815e-316 1.48219694e-323 2.37151510e-322
5:    0.00000000e+000 4.61121190e-316 4.61121190e-316 4.79959004e-316
5:    9.88131292e-324 4.61112258e-316 6.42285340e-323]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 5.26354425e-315 1.25986740e-321
5:    0.00000000e+000 5.57776952e-316 6.42285340e-323 0.00000000e+000
5:    0.00000000e+000 5.26354425e-315 6.42285340e-323]
5:   [0.00000000e+000 0.00000000e+000 5.26354425e-315 1.25986740e-321
5:    0.00000000e+000 5.57776952e-316 6.42285340e-323 0.00000000e+000
5:    0.00000000e+000 5.26354425e-315 6.42285340e-323]]
5: 
5:  [[0.00000000e+000 0.00000000e+000 5.26354425e-315 1.25986740e-321
5:    0.00000000e+000 5.57776952e-316 6.42285340e-323 0.00000000e+000
5:    0.00000000e+000 5.26354425e-315 6.42285340e-323]
5:   [0.00000000e+000 0.00000000e+000 5.26354425e-315 1.25986740e-321
5:    0.00000000e+000 5.57776952e-316 6.42285340e-323 0.00000000e+000
5:    0.00000000e+000 5.26354425e-315 6.42285340e-323]]
5: 
5:  [[0.00000000e+000 3.49104610e-033 1.30828583e-320 1.30729770e-320
5:    6.95644429e-321 2.37151510e-322 3.14203538e-316 9.88131292e-323
5:    9.88131292e-323 6.78537599e-308 7.19359580e-321]
5:   [0.00000000e+000 3.49104610e-033 1.30828583e-320 1.30729770e-320
5:    6.95644429e-321 2.37151510e-322 3.14203538e-316 9.88131292e-323
5:    9.88131292e-323 6.78537599e-308 7.19359580e-321]]
5: 
5:  [[0.00000000e+000 3.49104610e-033 1.30828583e-320 1.30729770e-320
5:    6.95644429e-321 2.37151510e-322 3.14203538e-316 9.88131292e-323
5:    9.88131292e-323 6.78537599e-308 7.19359580e-321]
5:   [0.00000000e+000 3.49104610e-033 1.30828583e-320 1.30729770e-320
5:    6.95644429e-321 2.37151510e-322 3.14203538e-316 9.88131292e-323
5:    9.88131292e-323 6.78537599e-308 7.19359580e-321]]
5: 
5:  [[2.37151510e-322 4.61120163e-316 4.94065646e-324 4.09781745e-080
5:    3.48191142e+227 7.43074731e-321 2.37151510e-322 4.61121586e-316
5:    4.94065646e-324 4.03250054e+175 4.31192064e-080]
5:   [2.37151510e-322 4.61120163e-316 4.94065646e-324 4.09781745e-080
5:    3.48191142e+227 7.43074731e-321 2.37151510e-322 4.61121586e-316
5:    4.94065646e-324 4.03250054e+175 4.31192064e-080]]
5: 
5:  [[2.37151510e-322 4.61120163e-316 4.94065646e-324 4.09781745e-080
5:    3.48191142e+227 7.43074731e-321 2.37151510e-322 4.61121586e-316
5:    4.94065646e-324 4.03250054e+175 4.31192064e-080]
5:   [2.37151510e-322 4.61120163e-316 4.94065646e-324 4.09781745e-080
5:    3.48191142e+227 7.43074731e-321 2.37151510e-322 4.61121586e-316
5:    4.94065646e-324 4.03250054e+175 4.31192064e-080]]
5: 
5:  [[7.66789882e-321 4.74303020e-322 4.56818551e-316 4.56818630e-316
5:    0.00000000e+000 0.00000000e+000 4.61120716e-316 3.95252517e-323
5:    6.39046855e+020 0.00000000e+000 4.61120953e-316]
5:   [7.66789882e-321 4.74303020e-322 4.56818551e-316 4.56818630e-316
5:    0.00000000e+000 0.00000000e+000 4.61120716e-316 3.95252517e-323
5:    6.39046855e+020 0.00000000e+000 4.61120953e-316]]
5: 
5:  [[7.66789882e-321 4.74303020e-322 4.56818551e-316 4.56818630e-316
5:    0.00000000e+000 0.00000000e+000 4.61120716e-316 3.95252517e-323
5:    6.39046855e+020 0.00000000e+000 4.61120953e-316]
5:   [7.66789882e-321 4.74303020e-322 4.56818551e-316 4.56818630e-316
5:    0.00000000e+000 0.00000000e+000 4.61120716e-316 3.95252517e-323
5:    6.39046855e+020 0.00000000e+000 4.61120953e-316]]]
5: 
5: Mismatched elements: 212 / 220 (96.4%)
5: Max absolute difference among violations: 3.48191142e+227
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[6.937865e-310, 6.937865e-310, 4.802425e-316, 4.802425e-316,
5:          1.000000e+000, 1.000000e+000, 1.000000e+000, 1.000000e+000,
5:          1.000000e+000, 1.000000e+000, 1.000000e+000],...
5:  DESIRED: array([[[0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000,
5:          0.000000e+000, 4.611212e-316, 4.611212e-316, 4.799590e-316,
5:          9.881313e-324, 4.611123e-316, 6.422853e-323],...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_4)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 220 / 220 (100%)
5: Max absolute difference among violations: 0.93456274
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[[ 0.000000e+000,  0.000000e+000,  0.000000e+000,  0.000000e+000,
5:           0.000000e+000,  0.000000e+000,  9.222424e-316,  9.599180e-316,
5:           1.976263e-323,  9.222245e-316,  1.284571e-322],...
5:  DESIRED: array([[[3.610938e-03, 5.573807e-02, 1.930876e-01, 7.131154e-02,
5:          5.859486e-01, 2.051612e-01, 1.680462e-01, 3.362203e-02,
5:          3.506642e-03, 2.203041e-01, 7.029323e-02],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_5)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 290, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[[ 1.38744602e-309  1.38746710e-309 -3.13061020e+031]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -1.21625330e+052]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -4.91944469e+005]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  8.39689377e+036]]]
5: 
5: 
5:  [[[ 6.93999017e-310  6.93723157e-310  6.95293699e-310]]
5: 
5:   [[ 1.38746710e-309  1.38736144e-309 -2.77996599e+186]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  1.52957767e+233]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -2.12998662e-179]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309  5.51627603e+119]]
5: 
5:   [[ 1.38746724e-309  3.15460915e-320  6.93786520e-310]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -4.97169578e+048]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  2.04924940e+101]]]
5: 
5: 
5:  [[[ 1.38736114e-309  6.93681275e-310 -7.47107827e+249]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  1.17198234e+011]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  8.18734660e-172]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  8.36785482e-213]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309 -5.21459005e+033]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -7.81040461e+119]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  3.29505023e+223]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -7.50072343e+039]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309 -2.59546307e+118]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  5.60853088e-158]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -1.10054096e-083]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -9.86918468e+209]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309 -2.43334216e-197]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  4.11773404e+274]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  8.12607611e+198]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  3.11237043e+215]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309 -8.14593556e+101]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -2.93989001e+277]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  1.25073936e+286]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -4.41988966e+283]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309 -1.38746070e+110]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  3.40319777e-137]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309 -1.66098380e+228]]
5: 
5:   [[ 1.38736144e-309  1.38736144e-309  2.53358353e-049]]]
5: 
5: 
5:  [[[ 1.38736144e-309  1.38736144e-309  3.15936206e+155]]
5: 
5:   [[ 1.38736211e-309  1.38736212e-309 -2.91905101e-020]]
5: 
5:   [[ 1.38736145e-309  1.38736145e-309  2.42158018e-075]]
5: 
5:   [[ 1.38736145e-309  1.38736145e-309  2.16920385e+266]]]]
5:  eager grad out tensor:
5: [[[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[1.84446690e-315 9.59918137e-316 1.10592937e-315]]
5: 
5:   [[1.89290778e-092 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[1.28457068e-322 0.00000000e+000 4.58193103e-068]]]
5: 
5: 
5:  [[[9.22242381e-316 9.22242381e-316 9.59919269e-316]]
5: 
5:   [[0.00000000e+000 9.22224516e-316 1.28457068e-322]]
5: 
5:   [[0.00000000e+000 3.52267303e-092 1.97626258e-323]]
5: 
5:   [[9.13639631e-316 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 9.22242381e-316 9.22224516e-316]]
5: 
5:   [[2.96439388e-323 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 9.22242381e-316 1.28457068e-322]]
5: 
5:   [[4.74303020e-322 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 9.59918009e-316 9.22240958e-316]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 1.97626258e-323 0.00000000e+000]]
5: 
5:   [[1.11555390e-315 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 1.05270885e-314 1.05270885e-314]]
5: 
5:   [[1.28457068e-322 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 7.83690659e-120 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 0.00000000e+000]]]
5: 
5: 
5:  [[[0.00000000e+000 4.80871750e+151 4.82702136e-321]]
5: 
5:   [[1.96502797e+214 1.17075772e+214 1.16563652e+253]]
5: 
5:   [[6.07721446e+247 9.80058441e+252 2.27551609e+161]]
5: 
5:   [[6.32697045e+180 4.90363071e+252 7.69847862e+218]]]]
5: 
5: Mismatched elements: 37 / 120 (30.8%)
5: Max absolute difference among violations: 1.25073936e+286
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[[ 1.387446e-309,  1.387467e-309, -3.130610e+031]],
5: 
5:         [[ 1.387361e-309,  1.387361e-309, -1.216253e+052]],...
5:  DESIRED: array([[[[0.000000e+000, 0.000000e+000, 0.000000e+000]],
5: 
5:         [[1.844467e-315, 9.599181e-316, 1.105929e-315]],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_5)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 300, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 888, in check
5:     self.check_eager_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1038, in check_eager_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check eager comp grad out failed. Mismatch between eager comp and eager on Place(cpu), when enable_rev_comp is True,the eager comp grad out tensor's index is : 0 
5: eager comp grad out tensor:
5: [[[[ 0.00000000e+000  1.09706368e-107  5.64385100e-102]
5:    [ 0.00000000e+000  1.09706368e-107  5.64385100e-102]]
5: 
5:   [[ 0.00000000e+000  8.54350475e-108  0.00000000e+000]
5:    [ 0.00000000e+000  8.54350475e-108  0.00000000e+000]]
5: 
5:   [[ 5.75900960e-071  0.00000000e+000 -1.17572458e-131]
5:    [ 5.75900960e-071  0.00000000e+000 -1.17572458e-131]]
5: 
5:   [[ 6.49802506e-063  0.00000000e+000  3.74572751e-151]
5:    [ 6.49802506e-063  0.00000000e+000  3.74572751e-151]]]
5: 
5: 
5:  [[[ 0.00000000e+000  4.21628947e-062  1.83550023e-154]
5:    [ 0.00000000e+000  4.21628947e-062  1.83550023e-154]]
5: 
5:   [[ 4.49701731e-162  1.59579035e+104              inf]
5:    [ 4.49701731e-162  1.59579035e+104              inf]]
5: 
5:   [[ 4.29938952e-233  6.67436210e+151              inf]
5:    [ 4.29938952e-233  6.67436210e+151              inf]]
5: 
5:   [[ 3.86277831e+027  5.07755108e+104              inf]
5:    [ 3.86277831e+027  5.07755108e+104              inf]]]
5: 
5: 
5:  [[[             inf  1.52074528e+154  3.00812372e-081]
5:    [             inf  1.52074528e+154  3.00812372e-081]]
5: 
5:   [[             inf  1.36611242e-087  0.00000000e+000]
5:    [             inf  1.36611242e-087  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]
5: 
5: 
5:  [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  9.06117279e-102]
5:    [ 0.00000000e+000  0.00000000e+000  9.06117279e-102]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]
5: 
5: 
5:  [[[ 9.25489868e-109  1.76052924e-067  0.00000000e+000]
5:    [ 9.25489868e-109  1.76052924e-067  0.00000000e+000]]
5: 
5:   [[ 4.04901817e-109  1.18016493e-054  0.00000000e+000]
5:    [ 4.04901817e-109  1.18016493e-054  0.00000000e+000]]
5: 
5:   [[ 2.60119966e-153              inf              inf]
5:    [ 2.60119966e-153              inf              inf]]
5: 
5:   [[ 2.93178841e-145  1.84681268e-136  3.38413871e-068]
5:    [ 2.93178841e-145  1.84681268e-136  3.38413871e-068]]]
5: 
5: 
5:  [[[ 2.34251395e-160  0.00000000e+000  0.00000000e+000]
5:    [ 2.34251395e-160  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[             inf  0.00000000e+000  0.00000000e+000]
5:    [             inf  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 6.10477316e-155  7.19958160e-155  1.65829545e-087]
5:    [ 6.10477316e-155  7.19958160e-155  1.65829545e-087]]
5: 
5:   [[ 3.78367012e-162  2.80159890e-145  0.00000000e+000]
5:    [ 3.78367012e-162  2.80159890e-145  0.00000000e+000]]]
5: 
5: 
5:  [[[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 0.00000000e+000  0.00000000e+000  0.00000000e+000]
5:    [ 0.00000000e+000  0.00000000e+000  0.00000000e+000]]]
5: 
5: 
5:  [[[ 4.03160766e-150  1.81060532e-129  0.00000000e+000]
5:    [ 4.03160766e-150  1.81060532e-129  0.00000000e+000]]
5: 
5:   [[ 3.57306457e-157              inf  1.09103446e-146]
5:    [ 3.57306457e-157              inf  1.09103446e-146]]
5: 
5:   [[ 1.26423354e-160  8.42049701e-151              inf]
5:    [ 1.26423354e-160  8.42049701e-151              inf]]
5: 
5:   [[ 6.68088420e-108  1.72160337e-092  8.13748106e+100]
5:    [ 6.68088420e-108  1.72160337e-092  8.13748106e+100]]]
5: 
5: 
5:  [[[             inf              inf  0.00000000e+000]
5:    [             inf              inf  0.00000000e+000]]
5: 
5:   [[ 2.38939232e+061  1.65859477e-068  0.00000000e+000]
5:    [ 2.38939232e+061  1.65859477e-068  0.00000000e+000]]
5: 
5:   [[ 2.93502389e-145  1.57872239e-148  3.38787003e-068]
5:    [ 2.93502389e-145  1.57872239e-148  3.38787003e-068]]
5: 
5:   [[ 3.38155955e-152              inf  1.47523577e-064]
5:    [ 3.38155955e-152              inf  1.47523577e-064]]]
5: 
5: 
5:  [[[ 1.83552314e-154  0.00000000e+000  0.00000000e+000]
5:    [ 1.83552314e-154  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[             inf  0.00000000e+000  0.00000000e+000]
5:    [             inf  0.00000000e+000  0.00000000e+000]]
5: 
5:   [[ 2.51295484e-099  1.84480543e+067 -3.42290863e+083]
5:    [ 2.51295484e-099  1.84480543e+067 -3.42290863e+083]]
5: 
5:   [[ 3.37453935e-098  9.06719539e-141  9.85371692e-103]
5:    [ 3.37453935e-098  9.06719539e-141  9.85371692e-103]]]]
5:  eager grad out tensor:
5: [[[[5.22638213e-316 1.96502797e+214 1.17075772e+214]
5:    [5.22638213e-316 1.96502797e+214 1.17075772e+214]]
5: 
5:   [[1.16563652e+253 2.13375552e-081 5.26488672e+170]
5:    [1.16563652e+253 2.13375552e-081 5.26488672e+170]]
5: 
5:   [[2.27551609e+161 6.07721446e+247 3.29257015e+161]
5:    [2.27551609e+161 6.07721446e+247 3.29257015e+161]]
5: 
5:   [[1.04918961e-153 1.91686729e-076 1.34452040e+161]
5:    [1.04918961e-153 1.91686729e-076 1.34452040e+161]]]
5: 
5: 
5:  [[[4.87511892e+199 3.52683968e+233 3.05999322e-057]
5:    [4.87511892e+199 3.52683968e+233 3.05999322e-057]]
5: 
5:   [[1.15463141e-320 5.57521066e-316 6.93786520e-310]
5:    [1.15463141e-320 5.57521066e-316 6.93786520e-310]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 2.37151510e-322]
5:    [0.00000000e+000 0.00000000e+000 2.37151510e-322]]
5: 
5:   [[6.32404027e-322 5.57507074e-316 1.96502797e+214]
5:    [6.32404027e-322 5.57507074e-316 1.96502797e+214]]]
5: 
5: 
5:  [[[1.17075772e+214 1.16563652e+253 2.13375552e-081]
5:    [1.17075772e+214 1.16563652e+253 2.13375552e-081]]
5: 
5:   [[5.26488672e+170 2.27551609e+161 6.07721446e+247]
5:    [5.26488672e+170 2.27551609e+161 6.07721446e+247]]
5: 
5:   [[3.29257015e+161 1.04918961e-153 9.05436137e-043]
5:    [3.29257015e+161 1.04918961e-153 9.05436137e-043]]
5: 
5:   [[1.09403337e+161 1.29151346e+161 2.97468401e+228]
5:    [1.09403337e+161 1.29151346e+161 2.97468401e+228]]]
5: 
5: 
5:  [[[3.57281122e-062 1.06767586e-320 5.57521066e-316]
5:    [3.57281122e-062 1.06767586e-320 5.57521066e-316]]
5: 
5:   [[6.93786520e-310 0.00000000e+000 0.00000000e+000]
5:    [6.93786520e-310 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[7.23196320e+165 2.60974417e+180 3.16202013e-322]
5:    [7.23196320e+165 2.60974417e+180 3.16202013e-322]]
5: 
5:   [[6.32404027e-322 5.57507943e-316 1.96502797e+214]
5:    [6.32404027e-322 5.57507943e-316 1.96502797e+214]]]
5: 
5: 
5:  [[[1.17075772e+214 1.16563652e+253 2.13375552e-081]
5:    [1.17075772e+214 1.16563652e+253 2.13375552e-081]]
5: 
5:   [[5.26488672e+170 2.27551609e+161 6.07721446e+247]
5:    [5.26488672e+170 2.27551609e+161 6.07721446e+247]]
5: 
5:   [[3.29257015e+161 1.04918961e-153 8.97327464e-067]
5:    [3.29257015e+161 1.04918961e-153 8.97327464e-067]]
5: 
5:   [[7.26611030e+223 4.24819625e+180 1.38501007e+219]
5:    [7.26611030e+223 4.24819625e+180 1.38501007e+219]]]
5: 
5: 
5:  [[[3.26510312e-058 9.72815257e-321 5.57521066e-316]
5:    [3.26510312e-058 9.72815257e-321 5.57521066e-316]]
5: 
5:   [[6.93786520e-310 0.00000000e+000 0.00000000e+000]
5:    [6.93786520e-310 0.00000000e+000 0.00000000e+000]]
5: 
5:   [[2.37151510e-322 6.32404027e-322 5.57508892e-316]
5:    [2.37151510e-322 6.32404027e-322 5.57508892e-316]]
5: 
5:   [[1.96502797e+214 1.17075772e+214 1.16563652e+253]
5:    [1.96502797e+214 1.17075772e+214 1.16563652e+253]]]
5: 
5: 
5:  [[[2.13375552e-081 5.26488672e+170 2.27551609e+161]
5:    [2.13375552e-081 5.26488672e+170 2.27551609e+161]]
5: 
5:   [[6.07721446e+247 1.38182478e+267 4.32455059e-096]
5:    [6.07721446e+247 1.38182478e+267 4.32455059e-096]]
5: 
5:   [[3.20634404e-057 7.21903949e+159 3.89652558e+233]
5:    [3.20634404e-057 7.21903949e+159 3.89652558e+233]]
5: 
5:   [[1.39803697e-076 3.37736213e-057 8.85859703e-321]
5:    [1.39803697e-076 3.37736213e-057 8.85859703e-321]]]
5: 
5: 
5:  [[[5.57489683e-316 5.57486837e-316 0.00000000e+000]
5:    [5.57489683e-316 5.57486837e-316 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 5.57505730e-316 1.96502797e+214]
5:    [0.00000000e+000 5.57505730e-316 1.96502797e+214]]
5: 
5:   [[1.17075772e+214 1.16563652e+253 2.13375552e-081]
5:    [1.17075772e+214 1.16563652e+253 2.13375552e-081]]
5: 
5:   [[5.26488672e+170 2.27551609e+161 6.07721446e+247]
5:    [5.26488672e+170 2.27551609e+161 6.07721446e+247]]]
5: 
5: 
5:  [[[1.38182478e+267 4.32455059e-096 1.65180090e-076]
5:    [1.38182478e+267 4.32455059e-096 1.65180090e-076]]
5: 
5:   [[1.68081361e+150 6.98069530e-307 3.16696079e-321]
5:    [1.68081361e+150 6.98069530e-307 3.16696079e-321]]
5: 
5:   [[5.57544386e-316 5.57508576e-316 1.29315989e+161]
5:    [5.57544386e-316 5.57508576e-316 1.29315989e+161]]
5: 
5:   [[2.59460261e+161 1.99111381e+209 6.75217468e-067]
5:    [2.59460261e+161 1.99111381e+209 6.75217468e-067]]]
5: 
5: 
5:  [[[3.57279676e-062 2.85075878e-321 5.57652764e-316]
5:    [3.57279676e-062 2.85075878e-321 5.57652764e-316]]
5: 
5:   [[5.57525730e-316 5.57511659e-316 5.43472210e-323]
5:    [5.57525730e-316 5.57511659e-316 5.43472210e-323]]
5: 
5:   [[3.50638655e+151 1.79575392e-052 0.00000000e+000]
5:    [3.50638655e+151 1.79575392e-052 0.00000000e+000]]
5: 
5:   [[0.00000000e+000 0.00000000e+000 1.21089429e-099]
5:    [0.00000000e+000 0.00000000e+000 1.21089429e-099]]]]
5: 
5: +inf location mismatch:
5:  ACTUAL: array([[[[ 0.000000e+000,  1.097064e-107,  5.643851e-102],
5:          [ 0.000000e+000,  1.097064e-107,  5.643851e-102]],
5: ...
5:  DESIRED: array([[[[5.226382e-316, 1.965028e+214, 1.170758e+214],
5:          [5.226382e-316, 1.965028e+214, 1.170758e+214]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_5)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 281, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 776, in assert_array_compare
5:     flagged = func_assert_same_pos(x, y, func=isnan, hasval='nan')
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[[ 3.89387714e-308              nan              nan]
5:    [ 3.89387714e-308              nan              nan]]
5: 
5:   [[ 1.67281945e-298  6.11779034e-296              nan]
5:    [ 1.67281945e-298  6.11779034e-296              nan]]
5: 
5:   [[ 9.70593598e-300              nan  3.82286478e-296]
5:    [ 9.70593598e-300              nan  3.82286478e-296]]
5: 
5:   [[             nan  1.44678098e-307              nan]
5:    [             nan  1.44678098e-307              nan]]]
5: 
5: 
5:  [[[ 2.36420464e-308  4.89298684e-296  3.96866311e-301]
5:    [ 2.36420464e-308  4.89298684e-296  3.96866311e-301]]
5: 
5:   [[ 1.05690924e-307  2.23983127e-300              nan]
5:    [ 1.05690924e-307  2.23983127e-300              nan]]
5: 
5:   [[ 2.05321271e-300  2.90633458e-296  4.30055169e-298]
5:    [ 2.05321271e-300  2.90633458e-296  4.30055169e-298]]
5: 
5:   [[ 4.58717501e-296  2.33315762e-300  1.63332437e-301]
5:    [ 4.58717501e-296  2.33315762e-300  1.63332437e-301]]]
5: 
5: 
5:  [[[ 5.46855012e-303  8.96154657e-300  1.24622810e-306]
5:    [ 5.46855012e-303  8.96154657e-300  1.24622810e-306]]
5: 
5:   [[             nan              nan  3.43454685e-299]
5:    [             nan              nan  3.43454685e-299]]
5: 
5:   [[ 1.19661952e-304  7.87724653e-302              nan]
5:    [ 1.19661952e-304  7.87724653e-302              nan]]
5: 
5:   [[ 3.64556101e-304              nan  2.31407546e-306]
5:    [ 3.64556101e-304              nan  2.31407546e-306]]]
5: 
5: 
5:  [[[ 4.01149112e-303              nan              nan]
5:    [ 4.01149112e-303              nan              nan]]
5: 
5:   [[ 2.78134402e-307              nan  2.62816697e-298]
5:    [ 2.78134402e-307              nan  2.62816697e-298]]
5: 
5:   [[ 2.50861261e-298              nan              nan]
5:    [ 2.50861261e-298              nan              nan]]
5: 
5:   [[             nan  4.37563889e-302              nan]
5:    [             nan  4.37563889e-302              nan]]]
5: 
5: 
5:  [[[             nan  1.04848818e-303  1.13044600e-302]
5:    [             nan  1.04848818e-303  1.13044600e-302]]
5: 
5:   [[ 7.40638008e-298  6.67597701e-308  8.60267821e-298]
5:    [ 7.40638008e-298  6.67597701e-308  8.60267821e-298]]
5: 
5:   [[ 1.94693752e-308  9.55661945e-299              nan]
5:    [ 1.94693752e-308  9.55661945e-299              nan]]
5: 
5:   [[ 9.11389820e-305  4.28145884e-296  1.15703841e-306]
5:    [ 9.11389820e-305  4.28145884e-296  1.15703841e-306]]]
5: 
5: 
5:  [[[             nan  2.10260356e-297  6.00769604e-307]
5:    [             nan  2.10260356e-297  6.00769604e-307]]
5: 
5:   [[ 1.07530561e-298              nan  3.46518231e-298]
5:    [ 1.07530561e-298              nan  3.46518231e-298]]
5: 
5:   [[ 9.56959076e-304              nan  1.27606804e-303]
5:    [ 9.56959076e-304              nan  1.27606804e-303]]
5: 
5:   [[ 4.33056649e-299  1.25353535e-304  1.66880455e-307]
5:    [ 4.33056649e-299  1.25353535e-304  1.66880455e-307]]]
5: 
5: 
5:  [[[ 2.73495301e-304  5.10378541e-303  1.40008234e-301]
5:    [ 2.73495301e-304  5.10378541e-303  1.40008234e-301]]
5: 
5:   [[ 5.60137033e-300              nan  1.69179764e-306]
5:    [ 5.60137033e-300              nan  1.69179764e-306]]
5: 
5:   [[ 2.31497587e-306  2.55189271e-303              nan]
5:    [ 2.31497587e-306  2.55189271e-303              nan]]
5: 
5:   [[             nan              nan              nan]
5:    [             nan              nan              nan]]]
5: 
5: 
5:  [[[ 1.72443224e-307              nan  8.62339198e-308]
5:    [ 1.72443224e-307              nan  8.62339198e-308]]
5: 
5:   [[ 3.28540466e-299  1.66911310e-308  9.61480707e-306]
5:    [ 3.28540466e-299  1.66911310e-308  9.61480707e-306]]
5: 
5:   [[ 5.73568509e-298  1.38610208e-297  4.95795854e-302]
5:    [ 5.73568509e-298  1.38610208e-297  4.95795854e-302]]
5: 
5:   [[ 1.59534319e-304  4.80615683e-306  1.34195896e-301]
5:    [ 1.59534319e-304  4.80615683e-306  1.34195896e-301]]]
5: 
5: 
5:  [[[ 3.38210958e-306  1.51670113e-301  5.83289407e-301]
5:    [ 3.38210958e-306  1.51670113e-301  5.83289407e-301]]
5: 
5:   [[ 6.83767633e-304              nan              nan]
5:    [ 6.83767633e-304              nan              nan]]
5: 
5:   [[ 5.16254077e-306              nan  3.89431638e-308]
5:    [ 5.16254077e-306              nan  3.89431638e-308]]
5: 
5:   [[             nan              nan -9.59926587e-063]
5:    [             nan              nan -9.59926587e-063]]]
5: 
5: 
5:  [[[ 6.93681657e-310  6.93680718e-310  1.16283724e-190]
5:    [ 6.93681657e-310  6.93680718e-310  1.16283724e-190]]
5: 
5:   [[ 6.93680725e-310  6.93680718e-310 -3.78742315e-097]
5:    [ 6.93680725e-310  6.93680718e-310 -3.78742315e-097]]
5: 
5:   [[ 6.93681398e-310  6.93680718e-310  5.24905285e+246]
5:    [ 6.93681398e-310  6.93680718e-310  5.24905285e+246]]
5: 
5:   [[ 6.93680724e-310  6.93680718e-310 -1.76781128e-074]
5:    [ 6.93680724e-310  6.93680718e-310 -1.76781128e-074]]]]
5:  eager grad out tensor:
5: [[[[4.56340413e-072 2.62397079e+179 2.44011711e-154]
5:    [4.56340413e-072 2.62397079e+179 2.44011711e-154]]
5: 
5:   [[6.01347002e-154 1.39804328e-076 7.57880771e-096]
5:    [6.01347002e-154 1.39804328e-076 7.57880771e-096]]
5: 
5:   [[8.23153407e-067 8.52611571e-096 2.21339121e-052]
5:    [8.23153407e-067 8.52611571e-096 2.21339121e-052]]
5: 
5:   [[9.62579545e+140 6.01346953e-154 1.30289901e-057]
5:    [9.62579545e+140 6.01346953e-154 1.30289901e-057]]]
5: 
5: 
5:  [[[3.75922477e+179 4.56340413e-072 2.62967423e+179]
5:    [3.75922477e+179 4.56340413e-072 2.62967423e+179]]
5: 
5:   [[6.01353875e-154 6.01347002e-154 2.44011738e-154]
5:    [6.01353875e-154 6.01347002e-154 2.44011738e-154]]
5: 
5:   [[6.01347002e-154 2.00561650e-076 8.52612286e-096]
5:    [6.01347002e-154 2.00561650e-076 8.52612286e-096]]
5: 
5:   [[9.51693207e-043 9.45148539e-096 6.01347002e-154]
5:    [9.51693207e-043 9.45148539e-096 6.01347002e-154]]]
5: 
5: 
5:  [[[8.54795557e+141 6.01346953e-154 1.30289901e-057]
5:    [8.54795557e+141 6.01346953e-154 1.30289901e-057]]
5: 
5:   [[2.62966437e+179 5.51681710e-048 2.95194573e+179]
5:    [2.62966437e+179 5.51681710e-048 2.95194573e+179]]
5: 
5:   [[6.01353868e-154 6.01347002e-154 2.44011738e-154]
5:    [6.01353868e-154 6.01347002e-154 2.44011738e-154]]
5: 
5:   [[6.01347002e-154 6.13147422e-062 8.98881130e-096]
5:    [6.01347002e-154 6.13147422e-062 8.98881130e-096]]]
5: 
5: 
5:  [[[4.91415728e-062 8.05612638e-096 6.01347002e-154]
5:    [4.91415728e-062 8.05612638e-096 6.01347002e-154]]
5: 
5:   [[8.54795557e+141 6.01346953e-154 1.98806611e-062]
5:    [8.54795557e+141 6.01346953e-154 1.98806611e-062]]
5: 
5:   [[3.92513148e+179 8.41799496e-053 2.78922709e+179]
5:    [3.92513148e+179 8.41799496e-053 2.78922709e+179]]
5: 
5:   [[6.01353875e-154 6.01347002e-154 2.44011738e-154]
5:    [6.01353875e-154 6.01347002e-154 2.44011738e-154]]]
5: 
5: 
5:  [[[6.01347002e-154 8.16645120e-043 1.08596387e-095]
5:    [6.01347002e-154 8.16645120e-043 1.08596387e-095]]
5: 
5:   [[2.42381953e-052 9.45148539e-096 6.01347002e-154]
5:    [2.42381953e-052 9.45148539e-096 6.01347002e-154]]
5: 
5:   [[8.54795557e+141 5.98129679e-154 1.21358461e+132]
5:    [8.54795557e+141 5.98129679e-154 1.21358461e+132]]
5: 
5:   [[6.01347002e-154 1.05908894e-153 3.69410392e-057]
5:    [6.01347002e-154 1.05908894e-153 3.69410392e-057]]]
5: 
5: 
5:  [[[1.05177285e-153 6.38706208e-067 5.46674921e-095]
5:    [1.05177285e-153 6.38706208e-067 5.46674921e-095]]
5: 
5:   [[6.01347002e-154 6.01347002e-154 7.26576487e+223]
5:    [6.01347002e-154 6.01347002e-154 7.26576487e+223]]
5: 
5:   [[7.85626075e-067 1.79258080e-052 1.39737246e-076]
5:    [7.85626075e-067 1.79258080e-052 1.39737246e-076]]
5: 
5:   [[4.75408868e-038 5.98129755e-154 1.20336039e+132]
5:    [4.75408868e-038 5.98129755e-154 1.20336039e+132]]]
5: 
5: 
5:  [[[6.01347002e-154 1.05908894e-153 9.74151350e-072]
5:    [6.01347002e-154 1.05908894e-153 9.74151350e-072]]
5: 
5:   [[1.05221156e-153 1.39804329e-076 5.46599484e-095]
5:    [1.05221156e-153 1.39804329e-076 5.46599484e-095]]
5: 
5:   [[6.01347002e-154 6.01347002e-154 7.26576487e+223]
5:    [6.01347002e-154 6.01347002e-154 7.26576487e+223]]
5: 
5:   [[1.31237876e-047 4.75408865e-038 1.39736850e-076]
5:    [1.31237876e-047 4.75408865e-038 1.39736850e-076]]]
5: 
5: 
5:  [[[1.39642638e-076 5.98129755e-154 1.20336039e+132]
5:    [1.39642638e-076 5.98129755e-154 1.20336039e+132]]
5: 
5:   [[1.39804329e-076 1.05118732e-153 1.39804329e-076]
5:    [1.39804329e-076 1.05118732e-153 1.39804329e-076]]
5: 
5:   [[1.05118732e-153 1.39804329e-076 5.46599484e-095]
5:    [1.05118732e-153 1.39804329e-076 5.46599484e-095]]
5: 
5:   [[6.01347002e-154 1.39736850e-076 1.39642638e-076]
5:    [6.01347002e-154 1.39736850e-076 1.39642638e-076]]]
5: 
5: 
5:  [[[1.39736850e-076 1.39642638e-076 1.39736850e-076]
5:    [1.39736850e-076 1.39642638e-076 1.39736850e-076]]
5: 
5:   [[1.39642638e-076 5.98129755e-154 1.20336039e+132]
5:    [1.39642638e-076 5.98129755e-154 1.20336039e+132]]
5: 
5:   [[1.39804329e-076 1.05118732e-153 1.39804329e-076]
5:    [1.39804329e-076 1.05118732e-153 1.39804329e-076]]
5: 
5:   [[1.05118732e-153 1.39804329e-076 5.46599484e-095]
5:    [1.05118732e-153 1.39804329e-076 5.46599484e-095]]]
5: 
5: 
5:  [[[6.01347002e-154 1.39736850e-076 1.39642638e-076]
5:    [6.01347002e-154 1.39736850e-076 1.39642638e-076]]
5: 
5:   [[1.39736850e-076 1.39642638e-076 1.39736850e-076]
5:    [1.39736850e-076 1.39642638e-076 1.39736850e-076]]
5: 
5:   [[1.39642638e-076 2.64624709e-260 8.94213159e+130]
5:    [1.39642638e-076 2.64624709e-260 8.94213159e+130]]
5: 
5:   [[1.39804065e-076 1.39803697e-076 1.39804065e-076]
5:    [1.39804065e-076 1.39803697e-076 1.39804065e-076]]]]
5: 
5: nan location mismatch:
5:  ACTUAL: array([[[[ 3.893877e-308,            nan,            nan],
5:          [ 3.893877e-308,            nan,            nan]],
5: ...
5:  DESIRED: array([[[[4.563404e-072, 2.623971e+179, 2.440117e-154],
5:          [4.563404e-072, 2.623971e+179, 2.440117e-154]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_broadcast_5)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 275, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 240 / 240 (100%)
5: Max absolute difference among violations: 7.06293789e+292
5: Max relative difference among violations: 1.99383643e+294
5:  ACTUAL: array([[[[ 8.003961e+080,  8.220736e+042,  1.396261e+076],
5:          [ 7.062938e+292,  9.596237e-050,  2.329390e+066]],
5: ...
5:  DESIRED: array([[[[7.466591e-02, 1.152415e-01, 1.245357e-02],
5:          [3.542386e-02, 3.634837e-01, 1.219437e-03]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 888, in check
5:     self.check_eager_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1038, in check_eager_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check eager comp grad out failed. Mismatch between eager comp and eager on Place(cpu), when enable_rev_comp is True,the eager comp grad out tensor's index is : 0 
5: eager comp grad out tensor:
5: [[[ 0.00000000e+000  5.71052950e-108  5.18548858e-045  0.00000000e+000
5:     2.23830728e-135  2.22521875e-045  5.85178395e-090  4.99240712e+031
5:     5.67588620e-160  5.14997751e-079  2.40682537e-140              inf
5:                 inf  1.90972065e+303              inf              inf
5:                 inf              inf              inf              inf
5:     2.75544033e+293  7.08687650e-008  7.16203779e-060  5.39785620e-044
5:     2.21837807e-063  5.41856787e-039  0.00000000e+000  0.00000000e+000
5:     4.09592602e-092              inf              inf              inf
5:                 inf              inf              inf              inf
5:     3.84459613e+230              inf  3.01664012e+283  1.99155347e-016
5:     1.25523246e-132  3.81127892e-045  7.15162702e-084  5.57498375e-063
5:     5.45794370e-152  4.14456023e-070  1.75261435e-076  2.71952335e-039
5:     7.15168878e-084  5.46012299e-087  8.58293242e-108  1.89380319e-045
5:     2.26156233e-063  4.67847741e-045  3.37495398e-098  0.00000000e+000
5:     3.87693879e-103  1.67450472e-069  9.26273095e-161  2.26729154e-063
5:     4.29078005e-170  5.43515052e-069  2.26153902e-063              inf
5:                 inf              inf              inf              inf
5:                 inf              inf              inf              inf
5:     8.66734357e+291  5.17586430e-057  2.73591451e-129  1.91747573e-045
5:     6.24399093e-084  5.44888041e-087  7.62977691e-046  2.32935022e-069
5:     6.48148578e-093  4.67847741e-045  3.13851101e-109  0.00000000e+000
5:     1.64869376e-121  6.24500170e-045 -3.51026750e-017  1.55547848e+240
5:     5.22467596e-145  1.60161569e-150              inf              inf
5:     3.67153763e+283              inf  1.92045520e+139              inf
5:     3.82230999e+302  3.94074263e+254 -3.52215869e-278  0.00000000e+000]]]
5:  eager grad out tensor:
5: [[[-4.28453567e-109  1.24282155e+214  1.12855330e+277  3.45043490e+175
5:     3.94355938e+180  4.79137541e+276  1.26001517e+232  3.99461109e+252
5:     1.99446555e+161  9.30684868e+242  4.24048879e+175  9.82220279e+252
5:     2.17150608e+214  5.26033033e+170  6.83055591e+212  1.01820900e+277
5:     2.85678800e+151  3.67268283e+257  4.24048879e+175  9.61987860e+228
5:     2.17150608e+214  1.24282155e+214  5.73061349e+161  1.01820900e+277
5:     3.98450019e+252  9.58275082e+276  2.52003034e+232  4.48108501e+161
5:     7.34514290e+223  1.86136974e+243  1.05642658e+214  3.08803380e+223
5:     6.44409916e+257  4.72585285e+257  1.99906365e+161  2.51312285e+180
5:     3.98450019e+252  9.82220280e+252  7.34514290e+223  7.48639891e+295
5:     3.94355938e+180  4.79137541e+276  1.26001517e+232  9.82220279e+252
5:     5.26047668e+170  3.99461109e+252  6.83055591e+212  4.79137541e+276
5:     1.26001517e+232  9.61987860e+228  2.17150608e+214  4.79137541e+276
5:     3.98450019e+252  1.01820900e+277  7.34514290e+223  9.30684868e+242
5:     6.83055591e+212  3.60556945e+252  1.99446555e+161  3.99461109e+252
5:     2.85678800e+151  9.82220279e+252  3.98450019e+252  9.82220280e+252
5:     7.34514290e+223  3.60556945e+252  3.94355938e+180  1.49734654e+277
5:     1.26001517e+232  9.82220279e+252  5.26047668e+170  9.30684868e+242
5:     6.83055591e+212  7.60018054e+252  3.94355938e+180  4.79137541e+276
5:     1.26001517e+232  9.61987860e+228  4.82588762e+276  9.82220279e+252
5:     1.24041768e+223  1.01820900e+277  6.83055591e+212  3.60556945e+252
5:     6.72780911e+199  1.12857358e+277 -6.20792987e+298  4.50806501e+064
5:     2.61352513e-306  1.06040471e-315  4.24048879e+175  9.61987860e+228
5:     2.17150608e+214  8.76349401e+189  9.64887928e-023  1.01820900e+277
5:     4.10698629e+059  8.74153904e+189 -1.34766590e+028  1.12964908e-315]]]
5: 
5: +inf location mismatch:
5:  ACTUAL: array([[[ 0.000000e+000,  5.710530e-108,  5.185489e-045,  0.000000e+000,
5:           2.238307e-135,  2.225219e-045,  5.851784e-090,  4.992407e+031,
5:           5.675886e-160,  5.149978e-079,  2.406825e-140,            inf,...
5:  DESIRED: array([[[-4.284536e-109,  1.242822e+214,  1.128553e+277,  3.450435e+175,
5:           3.943559e+180,  4.791375e+276,  1.260015e+232,  3.994611e+252,
5:           1.994466e+161,  9.306849e+242,  4.240489e+175,  9.822203e+252,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[1.27466862e-15 1.02044973e-15 1.58343228e-15 1.88738875e-15
5:    1.60640126e-15 1.08028582e-15 1.69322354e-15 1.67594061e-15
5:    1.16674421e-15 1.82898429e-15 1.91239036e-15 1.39169909e-15
5:    1.30876412e-15 1.57467375e-15 1.97251719e-15 1.72496312e-15
5:    1.79006255e-15 1.76858347e-15 1.95699396e-15 1.81619220e-15
5:    1.26831424e-15 1.23130105e-15 1.24209064e-15 1.21293491e-15
5:    1.20826341e-15 1.93450238e-15 1.95166244e-15 1.08985440e-15
5:    1.06407435e-15 1.85703032e-15 1.56374186e-15 1.21156632e-15
5:    1.46181669e-15 1.07374630e-15 1.98221668e-15 1.13995143e-15
5:    1.80655334e-15 1.95817982e-15 1.71101111e-15 1.08855097e-15
5:    1.58998498e-15 1.12921650e-15 1.86695003e-15 1.67282636e-15
5:    1.94205615e-15 1.14588586e-15 1.45368436e-15 1.54356312e-15
5:    1.10833803e-15 1.67690569e-15 1.56643561e-15 1.34804206e-15
5:    1.41291738e-15 1.79622892e-15 1.35641322e-15 1.97263315e-15
5:    1.91818463e-15 1.07808612e-15 1.85241683e-15 1.67377893e-15
5:    1.05531950e-15 1.95536614e-15 1.83348698e-15 1.83268396e-15
5:    1.18005249e-15 1.63537484e-15 1.74220923e-15 1.18045347e-15
5:    1.22194477e-15 1.69980925e-15 1.79450432e-15 1.95335483e-15
5:    1.30864990e-15 1.54863358e-15 1.14681276e-15 1.43689516e-15
5:    1.26112068e-15 1.34880018e-15 1.80336560e-15 1.80322972e-15
5:    1.70450568e-15 1.12363931e-15 1.28402508e-15 1.93929582e-15
5:    1.45252615e-15 1.62184475e-15 1.01830906e-15 1.51802005e-15
5:    1.68914287e-15 1.05888178e-15 1.11528453e-15 1.36836930e-15
5:    1.94445876e-15 1.24879800e-15 1.02160267e-15 1.80927847e-15
5:    1.84924838e-15 1.53669550e-15 1.89304722e-15 1.17903407e-15]
5:   [1.27466862e-15 1.02044973e-15 1.58343228e-15 1.88738875e-15
5:    1.60640126e-15 1.08028582e-15 1.69322354e-15 1.67594061e-15
5:    1.16674421e-15 1.82898429e-15 1.91239036e-15 1.39169909e-15
5:    1.30876412e-15 1.57467375e-15 1.97251719e-15 1.72496312e-15
5:    1.79006255e-15 1.76858347e-15 1.95699396e-15 1.81619220e-15
5:    1.26831424e-15 1.23130105e-15 1.24209064e-15 1.21293491e-15
5:    1.20826341e-15 1.93450238e-15 1.95166244e-15 1.08985440e-15
5:    1.06407435e-15 1.85703032e-15 1.56374186e-15 1.21156632e-15
5:    1.46181669e-15 1.07374630e-15 1.98221668e-15 1.13995143e-15
5:    1.80655334e-15 1.95817982e-15 1.71101111e-15 1.08855097e-15
5:    1.58998498e-15 1.12921650e-15 1.86695003e-15 1.67282636e-15
5:    1.94205615e-15 1.14588586e-15 1.45368436e-15 1.54356312e-15
5:    1.10833803e-15 1.67690569e-15 1.56643561e-15 1.34804206e-15
5:    1.41291738e-15 1.79622892e-15 1.35641322e-15 1.97263315e-15
5:    1.91818463e-15 1.07808612e-15 1.85241683e-15 1.67377893e-15
5:    1.05531950e-15 1.95536614e-15 1.83348698e-15 1.83268396e-15
5:    1.18005249e-15 1.63537484e-15 1.74220923e-15 1.18045347e-15
5:    1.22194477e-15 1.69980925e-15 1.79450432e-15 1.95335483e-15
5:    1.30864990e-15 1.54863358e-15 1.14681276e-15 1.43689516e-15
5:    1.26112068e-15 1.34880018e-15 1.80336560e-15 1.80322972e-15
5:    1.70450568e-15 1.12363931e-15 1.28402508e-15 1.93929582e-15
5:    1.45252615e-15 1.62184475e-15 1.01830906e-15 1.51802005e-15
5:    1.68914287e-15 1.05888178e-15 1.11528453e-15 1.36836930e-15
5:    1.94445876e-15 1.24879800e-15 1.02160267e-15 1.80927847e-15
5:    1.84924838e-15 1.53669550e-15 1.89304722e-15 1.17903407e-15]
5:   [1.27466862e-15 1.02044973e-15 1.58343228e-15 1.88738875e-15
5:    1.60640126e-15 1.08028582e-15 1.69322354e-15 1.67594061e-15
5:    1.16674421e-15 1.82898429e-15 1.91239036e-15 1.39169909e-15
5:    1.30876412e-15 1.57467375e-15 1.97251719e-15 1.72496312e-15
5:    1.79006255e-15 1.76858347e-15 1.95699396e-15 1.81619220e-15
5:    1.26831424e-15 1.23130105e-15 1.24209064e-15 1.21293491e-15
5:    1.20826341e-15 1.93450238e-15 1.95166244e-15 1.08985440e-15
5:    1.06407435e-15 1.85703032e-15 1.56374186e-15 1.21156632e-15
5:    1.46181669e-15 1.07374630e-15 1.98221668e-15 1.13995143e-15
5:    1.80655334e-15 1.95817982e-15 1.71101111e-15 1.08855097e-15
5:    1.58998498e-15 1.12921650e-15 1.86695003e-15 1.67282636e-15
5:    1.94205615e-15 1.14588586e-15 1.45368436e-15 1.54356312e-15
5:    1.10833803e-15 1.67690569e-15 1.56643561e-15 1.34804206e-15
5:    1.41291738e-15 1.79622892e-15 1.35641322e-15 1.97263315e-15
5:    1.91818463e-15 1.07808612e-15 1.85241683e-15 1.67377893e-15
5:    1.05531950e-15 1.95536614e-15 1.83348698e-15 1.83268396e-15
5:    1.18005249e-15 1.63537484e-15 1.74220923e-15 1.18045347e-15
5:    1.22194477e-15 1.69980925e-15 1.79450432e-15 1.95335483e-15
5:    1.30864990e-15 1.54863358e-15 1.14681276e-15 1.43689516e-15
5:    1.26112068e-15 1.34880018e-15 1.80336560e-15 1.80322972e-15
5:    1.70450568e-15 1.12363931e-15 1.28402508e-15 1.93929582e-15
5:    1.45252615e-15 1.62184475e-15 1.01830906e-15 1.51802005e-15
5:    1.68914287e-15 1.05888178e-15 1.11528453e-15 1.36836930e-15
5:    1.94445876e-15 1.24879800e-15 1.02160267e-15 1.80927847e-15
5:    1.84924838e-15 1.53669550e-15 1.89304722e-15 1.17903407e-15]]
5: 
5:  [[1.27466862e-15 1.02044973e-15 1.58343228e-15 1.88738875e-15
5:    1.60640126e-15 1.08028582e-15 1.69322354e-15 1.67594061e-15
5:    1.16674421e-15 1.82898429e-15 1.91239036e-15 1.39169909e-15
5:    1.30876412e-15 1.57467375e-15 1.97251719e-15 1.72496312e-15
5:    1.79006255e-15 1.76858347e-15 1.95699396e-15 1.81619220e-15
5:    1.26831424e-15 1.23130105e-15 1.24209064e-15 1.21293491e-15
5:    1.20826341e-15 1.93450238e-15 1.95166244e-15 1.08985440e-15
5:    1.06407435e-15 1.85703032e-15 1.56374186e-15 1.21156632e-15
5:    1.46181669e-15 1.07374630e-15 1.98221668e-15 1.13995143e-15
5:    1.80655334e-15 1.95817982e-15 1.71101111e-15 1.08855097e-15
5:    1.58998498e-15 1.12921650e-15 1.86695003e-15 1.67282636e-15
5:    1.94205615e-15 1.14588586e-15 1.45368436e-15 1.54356312e-15
5:    1.10833803e-15 1.67690569e-15 1.56643561e-15 1.34804206e-15
5:    1.41291738e-15 1.79622892e-15 1.35641322e-15 1.97263315e-15
5:    1.91818463e-15 1.07808612e-15 1.85241683e-15 1.67377893e-15
5:    1.05531950e-15 1.95536614e-15 1.83348698e-15 1.83268396e-15
5:    1.18005249e-15 1.63537484e-15 1.74220923e-15 1.18045347e-15
5:    1.22194477e-15 1.69980925e-15 1.79450432e-15 1.95335483e-15
5:    1.30864990e-15 1.54863358e-15 1.14681276e-15 1.43689516e-15
5:    1.26112068e-15 1.34880018e-15 1.80336560e-15 1.80322972e-15
5:    1.70450568e-15 1.12363931e-15 1.28402508e-15 1.93929582e-15
5:    1.45252615e-15 1.62184475e-15 1.01830906e-15 1.51802005e-15
5:    1.68914287e-15 1.05888178e-15 1.11528453e-15 1.36836930e-15
5:    1.94445876e-15 1.24879800e-15 1.02160267e-15 1.80927847e-15
5:    1.84924838e-15 1.53669550e-15 1.89304722e-15 1.17903407e-15]
5:   [1.27466862e-15 1.02044973e-15 1.58343228e-15 1.88738875e-15
5:    1.60640126e-15 1.08028582e-15 1.69322354e-15 1.67594061e-15
5:    1.16674421e-15 1.82898429e-15 1.91239036e-15 1.39169909e-15
5:    1.30876412e-15 1.57467375e-15 1.97251719e-15 1.72496312e-15
5:    1.79006255e-15 1.76858347e-15 1.95699396e-15 1.81619220e-15
5:    1.26831424e-15 1.23130105e-15 1.24209064e-15 1.21293491e-15
5:    1.20826341e-15 1.93450238e-15 1.95166244e-15 1.08985440e-15
5:    1.06407435e-15 1.85703032e-15 1.56374186e-15 1.21156632e-15
5:    1.46181669e-15 1.07374630e-15 1.98221668e-15 1.13995143e-15
5:    1.80655334e-15 1.95817982e-15 1.71101111e-15 1.08855097e-15
5:    1.58998498e-15 1.12921650e-15 1.86695003e-15 1.67282636e-15
5:    1.94205615e-15 1.14588586e-15 1.45368436e-15 1.54356312e-15
5:    1.10833803e-15 1.67690569e-15 1.56643561e-15 1.34804206e-15
5:    1.41291738e-15 1.79622892e-15 1.35641322e-15 1.97263315e-15
5:    1.91818463e-15 1.07808612e-15 1.85241683e-15 1.67377893e-15
5:    1.05531950e-15 1.95536614e-15 1.83348698e-15 1.83268396e-15
5:    1.18005249e-15 1.63537484e-15 1.74220923e-15 1.18045347e-15
5:    1.22194477e-15 1.69980925e-15 1.79450432e-15 1.95335483e-15
5:    1.30864990e-15 1.54863358e-15 1.14681276e-15 1.43689516e-15
5:    1.26112068e-15 1.34880018e-15 1.80336560e-15 1.80322972e-15
5:    1.70450568e-15 1.12363931e-15 1.28402508e-15 1.93929582e-15
5:    1.45252615e-15 1.62184475e-15 1.01830906e-15 1.51802005e-15
5:    1.68914287e-15 1.05888178e-15 1.11528453e-15 1.36836930e-15
5:    1.94445876e-15 1.24879800e-15 1.02160267e-15 1.80927847e-15
5:    1.84924838e-15 1.53669550e-15 1.89304722e-15 1.17903407e-15]
5:   [1.27466862e-15 1.02044973e-15 1.58343228e-15 1.88738875e-15
5:    1.60640126e-15 1.08028582e-15 1.69322354e-15 1.67594061e-15
5:    1.16674421e-15 1.82898429e-15 1.91239036e-15 1.39169909e-15
5:    1.30876412e-15 1.57467375e-15 1.97251719e-15 1.72496312e-15
5:    1.79006255e-15 1.76858347e-15 1.95699396e-15 1.81619220e-15
5:    1.26831424e-15 1.23130105e-15 1.24209064e-15 1.21293491e-15
5:    1.20826341e-15 1.93450238e-15 1.95166244e-15 1.08985440e-15
5:    1.06407435e-15 1.85703032e-15 1.56374186e-15 1.21156632e-15
5:    1.46181669e-15 1.07374630e-15 1.98221668e-15 1.13995143e-15
5:    1.80655334e-15 1.95817982e-15 1.71101111e-15 1.08855097e-15
5:    1.58998498e-15 1.12921650e-15 1.86695003e-15 1.67282636e-15
5:    1.94205615e-15 1.14588586e-15 1.45368436e-15 1.54356312e-15
5:    1.10833803e-15 1.67690569e-15 1.56643561e-15 1.34804206e-15
5:    1.41291738e-15 1.79622892e-15 1.35641322e-15 1.97263315e-15
5:    1.91818463e-15 1.07808612e-15 1.85241683e-15 1.67377893e-15
5:    1.05531950e-15 1.95536614e-15 1.83348698e-15 1.83268396e-15
5:    1.18005249e-15 1.63537484e-15 1.74220923e-15 1.18045347e-15
5:    1.22194477e-15 1.69980925e-15 1.79450432e-15 1.95335483e-15
5:    1.30864990e-15 1.54863358e-15 1.14681276e-15 1.43689516e-15
5:    1.26112068e-15 1.34880018e-15 1.80336560e-15 1.80322972e-15
5:    1.70450568e-15 1.12363931e-15 1.28402508e-15 1.93929582e-15
5:    1.45252615e-15 1.62184475e-15 1.01830906e-15 1.51802005e-15
5:    1.68914287e-15 1.05888178e-15 1.11528453e-15 1.36836930e-15
5:    1.94445876e-15 1.24879800e-15 1.02160267e-15 1.80927847e-15
5:    1.84924838e-15 1.53669550e-15 1.89304722e-15 1.17903407e-15]]]
5:  eager grad out tensor:
5: [[[0.27466862 0.02044973 0.58343228 0.88738875 0.60640126 0.08028582
5:    0.69322354 0.67594061 0.16674421 0.82898429 0.91239036 0.39169909
5:    0.30876412 0.57467375 0.97251719 0.72496312 0.79006255 0.76858347
5:    0.95699396 0.8161922  0.26831424 0.23130105 0.24209064 0.21293491
5:    0.20826341 0.93450238 0.95166244 0.0898544  0.06407435 0.85703032
5:    0.56374186 0.21156632 0.46181669 0.0737463  0.98221668 0.13995143
5:    0.80655334 0.95817982 0.71101111 0.08855097 0.58998498 0.1292165
5:    0.86695003 0.67282636 0.94205615 0.14588586 0.45368436 0.54356312
5:    0.10833803 0.67690569 0.56643561 0.34804206 0.41291738 0.79622892
5:    0.35641322 0.97263315 0.91818463 0.07808612 0.85241683 0.67377893
5:    0.0553195  0.95536614 0.83348698 0.83268396 0.18005249 0.63537484
5:    0.74220923 0.18045347 0.22194477 0.69980925 0.79450432 0.95335483
5:    0.3086499  0.54863358 0.14681276 0.43689516 0.26112068 0.34880018
5:    0.8033656  0.80322972 0.70450568 0.12363931 0.28402508 0.93929582
5:    0.45252615 0.62184475 0.01830906 0.51802005 0.68914287 0.05888178
5:    0.11528453 0.3683693  0.94445876 0.248798   0.02160267 0.80927847
5:    0.84924838 0.5366955  0.89304722 0.17903407]
5:   [0.27466862 0.02044973 0.58343228 0.88738875 0.60640126 0.08028582
5:    0.69322354 0.67594061 0.16674421 0.82898429 0.91239036 0.39169909
5:    0.30876412 0.57467375 0.97251719 0.72496312 0.79006255 0.76858347
5:    0.95699396 0.8161922  0.26831424 0.23130105 0.24209064 0.21293491
5:    0.20826341 0.93450238 0.95166244 0.0898544  0.06407435 0.85703032
5:    0.56374186 0.21156632 0.46181669 0.0737463  0.98221668 0.13995143
5:    0.80655334 0.95817982 0.71101111 0.08855097 0.58998498 0.1292165
5:    0.86695003 0.67282636 0.94205615 0.14588586 0.45368436 0.54356312
5:    0.10833803 0.67690569 0.56643561 0.34804206 0.41291738 0.79622892
5:    0.35641322 0.97263315 0.91818463 0.07808612 0.85241683 0.67377893
5:    0.0553195  0.95536614 0.83348698 0.83268396 0.18005249 0.63537484
5:    0.74220923 0.18045347 0.22194477 0.69980925 0.79450432 0.95335483
5:    0.3086499  0.54863358 0.14681276 0.43689516 0.26112068 0.34880018
5:    0.8033656  0.80322972 0.70450568 0.12363931 0.28402508 0.93929582
5:    0.45252615 0.62184475 0.01830906 0.51802005 0.68914287 0.05888178
5:    0.11528453 0.3683693  0.94445876 0.248798   0.02160267 0.80927847
5:    0.84924838 0.5366955  0.89304722 0.17903407]
5:   [0.27466862 0.02044973 0.58343228 0.88738875 0.60640126 0.08028582
5:    0.69322354 0.67594061 0.16674421 0.82898429 0.91239036 0.39169909
5:    0.30876412 0.57467375 0.97251719 0.72496312 0.79006255 0.76858347
5:    0.95699396 0.8161922  0.26831424 0.23130105 0.24209064 0.21293491
5:    0.20826341 0.93450238 0.95166244 0.0898544  0.06407435 0.85703032
5:    0.56374186 0.21156632 0.46181669 0.0737463  0.98221668 0.13995143
5:    0.80655334 0.95817982 0.71101111 0.08855097 0.58998498 0.1292165
5:    0.86695003 0.67282636 0.94205615 0.14588586 0.45368436 0.54356312
5:    0.10833803 0.67690569 0.56643561 0.34804206 0.41291738 0.79622892
5:    0.35641322 0.97263315 0.91818463 0.07808612 0.85241683 0.67377893
5:    0.0553195  0.95536614 0.83348698 0.83268396 0.18005249 0.63537484
5:    0.74220923 0.18045347 0.22194477 0.69980925 0.79450432 0.95335483
5:    0.3086499  0.54863358 0.14681276 0.43689516 0.26112068 0.34880018
5:    0.8033656  0.80322972 0.70450568 0.12363931 0.28402508 0.93929582
5:    0.45252615 0.62184475 0.01830906 0.51802005 0.68914287 0.05888178
5:    0.11528453 0.3683693  0.94445876 0.248798   0.02160267 0.80927847
5:    0.84924838 0.5366955  0.89304722 0.17903407]]
5: 
5:  [[0.27466862 0.02044973 0.58343228 0.88738875 0.60640126 0.08028582
5:    0.69322354 0.67594061 0.16674421 0.82898429 0.91239036 0.39169909
5:    0.30876412 0.57467375 0.97251719 0.72496312 0.79006255 0.76858347
5:    0.95699396 0.8161922  0.26831424 0.23130105 0.24209064 0.21293491
5:    0.20826341 0.93450238 0.95166244 0.0898544  0.06407435 0.85703032
5:    0.56374186 0.21156632 0.46181669 0.0737463  0.98221668 0.13995143
5:    0.80655334 0.95817982 0.71101111 0.08855097 0.58998498 0.1292165
5:    0.86695003 0.67282636 0.94205615 0.14588586 0.45368436 0.54356312
5:    0.10833803 0.67690569 0.56643561 0.34804206 0.41291738 0.79622892
5:    0.35641322 0.97263315 0.91818463 0.07808612 0.85241683 0.67377893
5:    0.0553195  0.95536614 0.83348698 0.83268396 0.18005249 0.63537484
5:    0.74220923 0.18045347 0.22194477 0.69980925 0.79450432 0.95335483
5:    0.3086499  0.54863358 0.14681276 0.43689516 0.26112068 0.34880018
5:    0.8033656  0.80322972 0.70450568 0.12363931 0.28402508 0.93929582
5:    0.45252615 0.62184475 0.01830906 0.51802005 0.68914287 0.05888178
5:    0.11528453 0.3683693  0.94445876 0.248798   0.02160267 0.80927847
5:    0.84924838 0.5366955  0.89304722 0.17903407]
5:   [0.27466862 0.02044973 0.58343228 0.88738875 0.60640126 0.08028582
5:    0.69322354 0.67594061 0.16674421 0.82898429 0.91239036 0.39169909
5:    0.30876412 0.57467375 0.97251719 0.72496312 0.79006255 0.76858347
5:    0.95699396 0.8161922  0.26831424 0.23130105 0.24209064 0.21293491
5:    0.20826341 0.93450238 0.95166244 0.0898544  0.06407435 0.85703032
5:    0.56374186 0.21156632 0.46181669 0.0737463  0.98221668 0.13995143
5:    0.80655334 0.95817982 0.71101111 0.08855097 0.58998498 0.1292165
5:    0.86695003 0.67282636 0.94205615 0.14588586 0.45368436 0.54356312
5:    0.10833803 0.67690569 0.56643561 0.34804206 0.41291738 0.79622892
5:    0.35641322 0.97263315 0.91818463 0.07808612 0.85241683 0.67377893
5:    0.0553195  0.95536614 0.83348698 0.83268396 0.18005249 0.63537484
5:    0.74220923 0.18045347 0.22194477 0.69980925 0.79450432 0.95335483
5:    0.3086499  0.54863358 0.14681276 0.43689516 0.26112068 0.34880018
5:    0.8033656  0.80322972 0.70450568 0.12363931 0.28402508 0.93929582
5:    0.45252615 0.62184475 0.01830906 0.51802005 0.68914287 0.05888178
5:    0.11528453 0.3683693  0.94445876 0.248798   0.02160267 0.80927847
5:    0.84924838 0.5366955  0.89304722 0.17903407]
5:   [0.27466862 0.02044973 0.58343228 0.88738875 0.60640126 0.08028582
5:    0.69322354 0.67594061 0.16674421 0.82898429 0.91239036 0.39169909
5:    0.30876412 0.57467375 0.97251719 0.72496312 0.79006255 0.76858347
5:    0.95699396 0.8161922  0.26831424 0.23130105 0.24209064 0.21293491
5:    0.20826341 0.93450238 0.95166244 0.0898544  0.06407435 0.85703032
5:    0.56374186 0.21156632 0.46181669 0.0737463  0.98221668 0.13995143
5:    0.80655334 0.95817982 0.71101111 0.08855097 0.58998498 0.1292165
5:    0.86695003 0.67282636 0.94205615 0.14588586 0.45368436 0.54356312
5:    0.10833803 0.67690569 0.56643561 0.34804206 0.41291738 0.79622892
5:    0.35641322 0.97263315 0.91818463 0.07808612 0.85241683 0.67377893
5:    0.0553195  0.95536614 0.83348698 0.83268396 0.18005249 0.63537484
5:    0.74220923 0.18045347 0.22194477 0.69980925 0.79450432 0.95335483
5:    0.3086499  0.54863358 0.14681276 0.43689516 0.26112068 0.34880018
5:    0.8033656  0.80322972 0.70450568 0.12363931 0.28402508 0.93929582
5:    0.45252615 0.62184475 0.01830906 0.51802005 0.68914287 0.05888178
5:    0.11528453 0.3683693  0.94445876 0.248798   0.02160267 0.80927847
5:    0.84924838 0.5366955  0.89304722 0.17903407]]]
5: 
5: Mismatched elements: 600 / 600 (100%)
5: Max absolute difference among violations: 0.98221668
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[[1.274669e-15, 1.020450e-15, 1.583432e-15, 1.887389e-15,
5:          1.606401e-15, 1.080286e-15, 1.693224e-15, 1.675941e-15,
5:          1.166744e-15, 1.828984e-15, 1.912390e-15, 1.391699e-15,...
5:  DESIRED: array([[[0.274669, 0.02045 , 0.583432, 0.887389, 0.606401, 0.080286,
5:          0.693224, 0.675941, 0.166744, 0.828984, 0.91239 , 0.391699,
5:          0.308764, 0.574674, 0.972517, 0.724963, 0.790063, 0.768583,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 1 
5: static comp grad out tensor:
5: [[[6.76838177e-043 2.90699771e-033 9.94614842e-043 4.43572665e-038
5:    2.21590580e-052 8.89429220e+130 5.35078618e-038 7.72338083e-043
5:    6.52744203e-038 1.08551654e-042 5.05818247e-038 3.70155588e-033
5:    7.10598732e-038 2.90706588e-033 3.69626613e-033 3.11935210e-033
5:    4.27782145e-033 4.65704807e-033 2.21219193e-052 4.27179386e-033
5:    1.38323341e-047 7.41134973e-038 5.05807394e-038 3.70155588e-033
5:    1.52031679e-047 1.08466212e-042 3.69626613e-033 3.11929882e-033
5:    4.66300743e-033 6.23595945e-038 4.08598951e-033 4.27179386e-033
5:    1.58609411e-047 1.54323383e-037 3.70231706e-033 2.48953249e-047
5:    4.27407748e-033 3.05566419e-057 1.31451812e-047 8.61676353e-043
5:    7.57000514e-033 6.23588223e-038 9.81307283e-038 1.57588669e-052
5:    3.12007759e-033 1.51351114e-037 4.85565402e-033 1.08499105e-042
5:    7.11748946e-038 8.94194746e+130 3.85476071e-057 2.90706295e-033
5:    1.11004950e-047 3.50368487e-033 4.47197251e-033 7.25572452e-043
5:    3.12002118e-033 5.05689912e-038 4.85342955e-033 4.43583516e-038
5:    7.11748946e-038 8.94194746e+130 7.07653050e-057 7.76264864e-033
5:    1.51927924e-047 6.22436656e-038 4.47192198e-033 1.72428237e-047
5:    4.27788042e-033 1.03279314e-047 4.43572649e-038 1.68441086e-066
5:    5.26530357e-052 1.65637099e-047 6.52400263e-038 4.85563619e-033
5:    7.10827452e-038 4.66001279e-033 7.18565443e-033 2.10779187e-052
5:    4.85563896e-033 1.06969536e-037 1.89894679e-052 6.60933885e-033
5:    1.12912917e-042 6.32818919e-052 7.86993293e-057 5.05116851e-038
5:    2.90699771e-033 4.66001279e-033 2.90699786e-033 1.17822398e-047
5:    4.85559460e-033 4.76072965e-038 4.00996277e-052 3.70228466e-033
5:    2.90699772e-033 1.79701629e-052 1.18459553e-037 5.39558643e-062]]]
5:  eager grad out tensor:
5: [[[2.54933725e-15 2.04089947e-15 3.16686456e-15 3.77477750e-15
5:    3.21280252e-15 2.16057164e-15 3.38644709e-15 3.35188123e-15
5:    2.33348842e-15 3.65796858e-15 3.82478072e-15 2.78339817e-15
5:    2.61752823e-15 3.14934750e-15 3.94503438e-15 3.44992624e-15
5:    3.58012510e-15 3.53716694e-15 3.91398791e-15 3.63238440e-15
5:    2.53662848e-15 2.46260210e-15 2.48418129e-15 2.42586981e-15
5:    2.41652682e-15 3.86900476e-15 3.90332489e-15 2.17970881e-15
5:    2.12814870e-15 3.71406064e-15 3.12748373e-15 2.42313264e-15
5:    2.92363337e-15 2.14749260e-15 3.96443337e-15 2.27990286e-15
5:    3.61310667e-15 3.91635964e-15 3.42202223e-15 2.17710195e-15
5:    3.17996996e-15 2.25843300e-15 3.73390007e-15 3.34565271e-15
5:    3.88411230e-15 2.29177171e-15 2.90736872e-15 3.08712625e-15
5:    2.21667606e-15 3.35381138e-15 3.13287123e-15 2.69608412e-15
5:    2.82583477e-15 3.59245784e-15 2.71282645e-15 3.94526629e-15
5:    3.83636927e-15 2.15617223e-15 3.70494454e-15 3.34755787e-15
5:    2.11063899e-15 3.91073227e-15 3.66697396e-15 3.66536793e-15
5:    3.54015747e-15 4.90612451e-15 5.22662770e-15 3.54136040e-15
5:    3.66583430e-15 5.09942774e-15 5.38351295e-15 5.86006448e-15
5:    3.92594969e-15 4.64590075e-15 3.44043827e-15 4.31068548e-15
5:    3.78336205e-15 4.04640054e-15 5.41009680e-15 5.40968917e-15
5:    5.11351704e-15 3.37091793e-15 3.85207524e-15 5.81788747e-15
5:    4.35757845e-15 4.86553425e-15 3.05492717e-15 4.55406014e-15
5:    5.06742861e-15 3.17664535e-15 3.34585360e-15 4.10510789e-15
5:    5.83337629e-15 3.74639401e-15 3.06480802e-15 5.42783542e-15
5:    5.54774513e-15 4.61008650e-15 5.67914166e-15 3.53710222e-15]]]
5: 
5: Mismatched elements: 100 / 100 (100%)
5: Max absolute difference among violations: 8.94194746e+130
5: Max relative difference among violations: 4.11663841e+145
5:  ACTUAL: array([[[6.768382e-043, 2.906998e-033, 9.946148e-043, 4.435727e-038,
5:          2.215906e-052, 8.894292e+130, 5.350786e-038, 7.723381e-043,
5:          6.527442e-038, 1.085517e-042, 5.058182e-038, 3.701556e-033,...
5:  DESIRED: array([[[2.549337e-15, 2.040899e-15, 3.166865e-15, 3.774777e-15,
5:          3.212803e-15, 2.160572e-15, 3.386447e-15, 3.351881e-15,
5:          2.333488e-15, 3.657969e-15, 3.824781e-15, 2.783398e-15,...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_1)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[          inf, 6.376150e+094, 2.954514e+095, 7.623527e+070,
5:          3.846470e+138,           inf, 7.778853e+122, 5.927256e-092,
5:          0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000,...
5:  DESIRED: array([[[6.007969e-02, 4.718862e-03, 1.223461e-01, 4.248121e-02,
5:          1.044858e-01, 2.472815e-01, 2.256950e-01, 8.556311e-02,
5:          1.510760e-01, 1.244100e-01, 3.993570e-01, 2.573475e-01,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 892, in check
5:     self.check_jit_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1276, in check_jit_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check jit comp grad out failed. Mismatch between jit comp and eager on Place(cpu), when enable_fw_comp is True, enable_rev_comp is True,the grad out tensor's index is : 0 
5: jit comp grad out tensor:
5: [[[[5.34986865e-038]
5:    [5.34986865e-038]
5:    [5.34986865e-038]
5:    [5.34986865e-038]]]
5: 
5: 
5:  [[[3.76177421e+179]
5:    [3.76177421e+179]
5:    [3.76177421e+179]
5:    [3.76177421e+179]]]
5: 
5: 
5:  [[[2.95068686e+179]
5:    [2.95068686e+179]
5:    [2.95068686e+179]
5:    [2.95068686e+179]]]
5: 
5: 
5:  [[[3.59840806e+179]
5:    [3.59840806e+179]
5:    [3.59840806e+179]
5:    [3.59840806e+179]]]
5: 
5: 
5:  [[[2.95385021e+179]
5:    [2.95385021e+179]
5:    [2.95385021e+179]
5:    [2.95385021e+179]]]
5: 
5: 
5:  [[[2.94878488e+179]
5:    [2.94878488e+179]
5:    [2.94878488e+179]
5:    [2.94878488e+179]]]
5: 
5: 
5:  [[[6.71435356e+179]
5:    [6.71435356e+179]
5:    [6.71435356e+179]
5:    [6.71435356e+179]]]
5: 
5: 
5:  [[[9.51354651e-043]
5:    [9.51354651e-043]
5:    [9.51354651e-043]
5:    [9.51354651e-043]]]
5: 
5: 
5:  [[[3.92321469e+179]
5:    [3.92321469e+179]
5:    [3.92321469e+179]
5:    [3.92321469e+179]]]
5: 
5: 
5:  [[[3.59904866e+179]
5:    [3.59904866e+179]
5:    [3.59904866e+179]
5:    [3.59904866e+179]]]
5: 
5: 
5:  [[[3.12300049e-033]
5:    [3.12300049e-033]
5:    [3.12300049e-033]
5:    [3.12300049e-033]]]
5: 
5: 
5:  [[[1.06505581e+094]
5:    [1.06505581e+094]
5:    [1.06505581e+094]
5:    [1.06505581e+094]]]
5: 
5: 
5:  [[[3.27359154e+179]
5:    [3.27359154e+179]
5:    [3.27359154e+179]
5:    [3.27359154e+179]]]
5: 
5: 
5:  [[[3.92576224e+179]
5:    [3.92576224e+179]
5:    [3.92576224e+179]
5:    [3.92576224e+179]]]
5: 
5: 
5:  [[[2.79113147e+179]
5:    [2.79113147e+179]
5:    [2.79113147e+179]
5:    [2.79113147e+179]]]
5: 
5: 
5:  [[[4.08722199e+179]
5:    [4.08722199e+179]
5:    [4.08722199e+179]
5:    [4.08722199e+179]]]
5: 
5: 
5:  [[[3.92512160e+179]
5:    [3.92512160e+179]
5:    [3.92512160e+179]
5:    [3.92512160e+179]]]
5: 
5: 
5:  [[[4.08341304e+179]
5:    [4.08341304e+179]
5:    [4.08341304e+179]
5:    [4.08341304e+179]]]
5: 
5: 
5:  [[[7.18146955e-033]
5:    [7.18146955e-033]
5:    [7.18146955e-033]
5:    [7.18146955e-033]]]
5: 
5: 
5:  [[[5.89691676e+179]
5:    [5.89691676e+179]
5:    [5.89691676e+179]
5:    [5.89691676e+179]]]
5: 
5: 
5:  [[[2.62775991e+179]
5:    [2.62775991e+179]
5:    [2.62775991e+179]
5:    [2.62775991e+179]]]
5: 
5: 
5:  [[[4.08530759e+179]
5:    [4.08530759e+179]
5:    [4.08530759e+179]
5:    [4.08530759e+179]]]
5: 
5: 
5:  [[[3.92511668e+179]
5:    [3.92511668e+179]
5:    [3.92511668e+179]
5:    [3.92511668e+179]]]
5: 
5: 
5:  [[[2.79112409e+179]
5:    [2.79112409e+179]
5:    [2.79112409e+179]
5:    [2.79112409e+179]]]
5: 
5: 
5:  [[[3.43694832e+179]
5:    [3.43694832e+179]
5:    [3.43694832e+179]
5:    [3.43694832e+179]]]
5: 
5: 
5:  [[[3.88666432e-033]
5:    [3.88666432e-033]
5:    [3.88666432e-033]
5:    [3.88666432e-033]]]
5: 
5: 
5:  [[[8.00854219e+179]
5:    [8.00854219e+179]
5:    [8.00854219e+179]
5:    [8.00854219e+179]]]
5: 
5: 
5:  [[[1.06878740e-037]
5:    [1.06878740e-037]
5:    [1.06878740e-037]
5:    [1.06878740e-037]]]
5: 
5: 
5:  [[[3.11278473e+179]
5:    [3.11278473e+179]
5:    [3.11278473e+179]
5:    [3.11278473e+179]]]
5: 
5: 
5:  [[[3.43758644e+179]
5:    [3.43758644e+179]
5:    [3.43758644e+179]
5:    [3.43758644e+179]]]]
5:  eager grad out out tensor:
5: [[[[9.30684868e+242]
5:    [9.30684868e+242]
5:    [9.30684868e+242]
5:    [9.30684868e+242]]]
5: 
5: 
5:  [[[1.01820900e+277]
5:    [1.01820900e+277]
5:    [1.01820900e+277]
5:    [1.01820900e+277]]]
5: 
5: 
5:  [[[6.93788733e-310]
5:    [6.93788733e-310]
5:    [6.93788733e-310]
5:    [6.93788733e-310]]]
5: 
5: 
5:  [[[8.90874679e+194]
5:    [8.90874679e+194]
5:    [8.90874679e+194]
5:    [8.90874679e+194]]]
5: 
5: 
5:  [[[8.32480250e-309]
5:    [8.32480250e-309]
5:    [8.32480250e-309]
5:    [8.32480250e-309]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[9.01816819e-309]
5:    [9.01816819e-309]
5:    [9.01816819e-309]
5:    [9.01816819e-309]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[9.01816832e-309]
5:    [9.01816832e-309]
5:    [9.01816832e-309]
5:    [9.01816832e-309]]]
5: 
5: 
5:  [[[1.04052105e-308]
5:    [1.04052105e-308]
5:    [1.04052105e-308]
5:    [1.04052105e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052116e-308]
5:    [1.04052116e-308]
5:    [1.04052116e-308]
5:    [1.04052116e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]
5:    [1.04052108e-308]]]
5: 
5: 
5:  [[[1.04052109e-308]
5:    [1.04052109e-308]
5:    [1.04052109e-308]
5:    [1.04052109e-308]]]
5: 
5: 
5:  [[[1.04052109e-308]
5:    [1.04052109e-308]
5:    [1.04052109e-308]
5:    [1.04052109e-308]]]]
5: 
5: Mismatched elements: 100 / 120 (83.3%)
5: Max absolute difference among violations: 1.018209e+277
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[[5.349869e-038],
5:          [5.349869e-038],
5:          [5.349869e-038],...
5:  DESIRED: array([[[[9.306849e+242],
5:          [9.306849e+242],
5:          [9.306849e+242],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[[ 3.18865112e-313  3.18865112e-313  3.18865112e-313  3.18865112e-313
5:      3.18865112e-313]]
5: 
5:   [[ 3.18865112e-313  3.18865112e-313  3.18865112e-313  3.18865112e-313
5:      3.18865112e-313]]
5: 
5:   [[ 3.18865112e-313  3.18865112e-313  3.18865112e-313  3.18865112e-313
5:      3.18865112e-313]]]
5: 
5: 
5:  [[[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]]
5: 
5: 
5:  [[[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]]
5: 
5: 
5:  [[[-8.27030588e+003 -8.27030588e+003 -8.27030588e+003 -8.27030588e+003
5:     -8.27030588e+003]]
5: 
5:   [[-8.27030588e+003 -8.27030588e+003 -8.27030588e+003 -8.27030588e+003
5:     -8.27030588e+003]]
5: 
5:   [[-8.27030588e+003 -8.27030588e+003 -8.27030588e+003 -8.27030588e+003
5:     -8.27030588e+003]]]
5: 
5: 
5:  [[[ 4.72585285e+257  4.72585285e+257  4.72585285e+257  4.72585285e+257
5:      4.72585285e+257]]
5: 
5:   [[ 4.72585285e+257  4.72585285e+257  4.72585285e+257  4.72585285e+257
5:      4.72585285e+257]]
5: 
5:   [[ 4.72585285e+257  4.72585285e+257  4.72585285e+257  4.72585285e+257
5:      4.72585285e+257]]]
5: 
5: 
5:  [[[ 2.51312285e+180  2.51312285e+180  2.51312285e+180  2.51312285e+180
5:      2.51312285e+180]]
5: 
5:   [[ 2.51312285e+180  2.51312285e+180  2.51312285e+180  2.51312285e+180
5:      2.51312285e+180]]
5: 
5:   [[ 2.51312285e+180  2.51312285e+180  2.51312285e+180  2.51312285e+180
5:      2.51312285e+180]]]
5: 
5: 
5:  [[[ 3.71115879e+006  3.71115879e+006  3.71115879e+006  3.71115879e+006
5:      3.71115879e+006]]
5: 
5:   [[ 3.71115879e+006  3.71115879e+006  3.71115879e+006  3.71115879e+006
5:      3.71115879e+006]]
5: 
5:   [[ 3.71115879e+006  3.71115879e+006  3.71115879e+006  3.71115879e+006
5:      3.71115879e+006]]]
5: 
5: 
5:  [[[ 1.38067030e+253  1.38067030e+253  1.38067030e+253  1.38067030e+253
5:      1.38067030e+253]]
5: 
5:   [[ 1.38067030e+253  1.38067030e+253  1.38067030e+253  1.38067030e+253
5:      1.38067030e+253]]
5: 
5:   [[ 1.38067030e+253  1.38067030e+253  1.38067030e+253  1.38067030e+253
5:      1.38067030e+253]]]
5: 
5: 
5:  [[[ 7.34514290e+223  7.34514290e+223  7.34514290e+223  7.34514290e+223
5:      7.34514290e+223]]
5: 
5:   [[ 7.34514290e+223  7.34514290e+223  7.34514290e+223  7.34514290e+223
5:      7.34514290e+223]]
5: 
5:   [[ 7.34514290e+223  7.34514290e+223  7.34514290e+223  7.34514290e+223
5:      7.34514290e+223]]]
5: 
5: 
5:  [[[ 1.12857358e+277  1.12857358e+277  1.12857358e+277  1.12857358e+277
5:      1.12857358e+277]]
5: 
5:   [[ 1.12857358e+277  1.12857358e+277  1.12857358e+277  1.12857358e+277
5:      1.12857358e+277]]
5: 
5:   [[ 1.12857358e+277  1.12857358e+277  1.12857358e+277  1.12857358e+277
5:      1.12857358e+277]]]
5: 
5: 
5:  [[[-8.57750586e+003 -8.57750586e+003 -8.57750586e+003 -8.57750586e+003
5:     -8.57750586e+003]]
5: 
5:   [[-8.57750586e+003 -8.57750586e+003 -8.57750586e+003 -8.57750586e+003
5:     -8.57750586e+003]]
5: 
5:   [[-8.57750586e+003 -8.57750586e+003 -8.57750586e+003 -8.57750586e+003
5:     -8.57750586e+003]]]
5: 
5: 
5:  [[[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]]
5: 
5: 
5:  [[[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]]
5: 
5: 
5:  [[[ 4.10698629e+059  4.10698629e+059  4.10698629e+059  4.10698629e+059
5:      4.10698629e+059]]
5: 
5:   [[ 4.10698629e+059  4.10698629e+059  4.10698629e+059  4.10698629e+059
5:      4.10698629e+059]]
5: 
5:   [[ 4.10698629e+059  4.10698629e+059  4.10698629e+059  4.10698629e+059
5:      4.10698629e+059]]]
5: 
5: 
5:  [[[ 2.86530675e+161  2.86530675e+161  2.86530675e+161  2.86530675e+161
5:      2.86530675e+161]]
5: 
5:   [[ 2.86530675e+161  2.86530675e+161  2.86530675e+161  2.86530675e+161
5:      2.86530675e+161]]
5: 
5:   [[ 2.86530675e+161  2.86530675e+161  2.86530675e+161  2.86530675e+161
5:      2.86530675e+161]]]
5: 
5: 
5:  [[[ 4.79137541e+276  4.79137541e+276  4.79137541e+276  4.79137541e+276
5:      4.79137541e+276]]
5: 
5:   [[ 4.79137541e+276  4.79137541e+276  4.79137541e+276  4.79137541e+276
5:      4.79137541e+276]]
5: 
5:   [[ 4.79137541e+276  4.79137541e+276  4.79137541e+276  4.79137541e+276
5:      4.79137541e+276]]]
5: 
5: 
5:  [[[ 6.32299181e+233  6.32299181e+233  6.32299181e+233  6.32299181e+233
5:      6.32299181e+233]]
5: 
5:   [[ 6.32299181e+233  6.32299181e+233  6.32299181e+233  6.32299181e+233
5:      6.32299181e+233]]
5: 
5:   [[ 6.32299181e+233  6.32299181e+233  6.32299181e+233  6.32299181e+233
5:      6.32299181e+233]]]
5: 
5: 
5:  [[[-5.80251978e-050 -5.80251978e-050 -5.80251978e-050 -5.80251978e-050
5:     -5.80251978e-050]]
5: 
5:   [[-5.80251978e-050 -5.80251978e-050 -5.80251978e-050 -5.80251978e-050
5:     -5.80251978e-050]]
5: 
5:   [[-5.80251978e-050 -5.80251978e-050 -5.80251978e-050 -5.80251978e-050
5:     -5.80251978e-050]]]
5: 
5: 
5:  [[[ 1.38067030e+253  1.38067030e+253  1.38067030e+253  1.38067030e+253
5:      1.38067030e+253]]
5: 
5:   [[ 1.38067030e+253  1.38067030e+253  1.38067030e+253  1.38067030e+253
5:      1.38067030e+253]]
5: 
5:   [[ 1.38067030e+253  1.38067030e+253  1.38067030e+253  1.38067030e+253
5:      1.38067030e+253]]]
5: 
5: 
5:  [[[ 5.50656807e+213  5.50656807e+213  5.50656807e+213  5.50656807e+213
5:      5.50656807e+213]]
5: 
5:   [[ 5.50656807e+213  5.50656807e+213  5.50656807e+213  5.50656807e+213
5:      5.50656807e+213]]
5: 
5:   [[ 5.50656807e+213  5.50656807e+213  5.50656807e+213  5.50656807e+213
5:      5.50656807e+213]]]
5: 
5: 
5:  [[[ 1.12857358e+277  1.12857358e+277  1.12857358e+277  1.12857358e+277
5:      1.12857358e+277]]
5: 
5:   [[ 1.12857358e+277  1.12857358e+277  1.12857358e+277  1.12857358e+277
5:      1.12857358e+277]]
5: 
5:   [[ 1.12857358e+277  1.12857358e+277  1.12857358e+277  1.12857358e+277
5:      1.12857358e+277]]]
5: 
5: 
5:  [[[ 6.00190289e+175  6.00190289e+175  6.00190289e+175  6.00190289e+175
5:      6.00190289e+175]]
5: 
5:   [[ 6.00190289e+175  6.00190289e+175  6.00190289e+175  6.00190289e+175
5:      6.00190289e+175]]
5: 
5:   [[ 6.00190289e+175  6.00190289e+175  6.00190289e+175  6.00190289e+175
5:      6.00190289e+175]]]
5: 
5: 
5:  [[[ 9.82220279e+252  9.82220279e+252  9.82220279e+252  9.82220279e+252
5:      9.82220279e+252]]
5: 
5:   [[ 9.82220279e+252  9.82220279e+252  9.82220279e+252  9.82220279e+252
5:      9.82220279e+252]]
5: 
5:   [[ 9.82220279e+252  9.82220279e+252  9.82220279e+252  9.82220279e+252
5:      9.82220279e+252]]]
5: 
5: 
5:  [[[ 3.99461109e+252  3.99461109e+252  3.99461109e+252  3.99461109e+252
5:      3.99461109e+252]]
5: 
5:   [[ 3.99461109e+252  3.99461109e+252  3.99461109e+252  3.99461109e+252
5:      3.99461109e+252]]
5: 
5:   [[ 3.99461109e+252  3.99461109e+252  3.99461109e+252  3.99461109e+252
5:      3.99461109e+252]]]
5: 
5: 
5:  [[[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]]
5: 
5: 
5:  [[[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]]
5: 
5: 
5:  [[[ 2.31611866e+077  2.31611866e+077  2.31611866e+077  2.31611866e+077
5:      2.31611866e+077]]
5: 
5:   [[ 2.31611866e+077  2.31611866e+077  2.31611866e+077  2.31611866e+077
5:      2.31611866e+077]]
5: 
5:   [[ 2.31611866e+077  2.31611866e+077  2.31611866e+077  2.31611866e+077
5:      2.31611866e+077]]]
5: 
5: 
5:  [[[ 2.85678800e+151  2.85678800e+151  2.85678800e+151  2.85678800e+151
5:      2.85678800e+151]]
5: 
5:   [[ 2.85678800e+151  2.85678800e+151  2.85678800e+151  2.85678800e+151
5:      2.85678800e+151]]
5: 
5:   [[ 2.85678800e+151  2.85678800e+151  2.85678800e+151  2.85678800e+151
5:      2.85678800e+151]]]
5: 
5: 
5:  [[[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]]
5: 
5: 
5:  [[[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]]]
5:  eager grad out tensor:
5: [[[[3.72273947e+243 3.72273947e+243 3.72273947e+243 3.72273947e+243
5:     3.72273947e+243]]
5: 
5:   [[3.72273947e+243 3.72273947e+243 3.72273947e+243 3.72273947e+243
5:     3.72273947e+243]]
5: 
5:   [[3.72273947e+243 3.72273947e+243 3.72273947e+243 3.72273947e+243
5:     3.72273947e+243]]]
5: 
5: 
5:  [[[4.07283600e+277 4.07283600e+277 4.07283600e+277 4.07283600e+277
5:     4.07283600e+277]]
5: 
5:   [[4.07283600e+277 4.07283600e+277 4.07283600e+277 4.07283600e+277
5:     4.07283600e+277]]
5: 
5:   [[4.07283600e+277 4.07283600e+277 4.07283600e+277 4.07283600e+277
5:     4.07283600e+277]]]
5: 
5: 
5:  [[[2.77515493e-309 2.77515493e-309 2.77515493e-309 2.77515493e-309
5:     2.77515493e-309]]
5: 
5:   [[2.77515493e-309 2.77515493e-309 2.77515493e-309 2.77515493e-309
5:     2.77515493e-309]]
5: 
5:   [[2.77515493e-309 2.77515493e-309 2.77515493e-309 2.77515493e-309
5:     2.77515493e-309]]]
5: 
5: 
5:  [[[3.56349872e+195 3.56349872e+195 3.56349872e+195 3.56349872e+195
5:     3.56349872e+195]]
5: 
5:   [[3.56349872e+195 3.56349872e+195 3.56349872e+195 3.56349872e+195
5:     3.56349872e+195]]
5: 
5:   [[3.56349872e+195 3.56349872e+195 3.56349872e+195 3.56349872e+195
5:     3.56349872e+195]]]
5: 
5: 
5:  [[[3.32992100e-308 3.32992100e-308 3.32992100e-308 3.32992100e-308
5:     3.32992100e-308]]
5: 
5:   [[3.32992100e-308 3.32992100e-308 3.32992100e-308 3.32992100e-308
5:     3.32992100e-308]]
5: 
5:   [[3.32992100e-308 3.32992100e-308 3.32992100e-308 3.32992100e-308
5:     3.32992100e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[3.60726728e-308 3.60726728e-308 3.60726728e-308 3.60726728e-308
5:     3.60726728e-308]]
5: 
5:   [[3.60726728e-308 3.60726728e-308 3.60726728e-308 3.60726728e-308
5:     3.60726728e-308]]
5: 
5:   [[3.60726728e-308 3.60726728e-308 3.60726728e-308 3.60726728e-308
5:     3.60726728e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[3.60726733e-308 3.60726733e-308 3.60726733e-308 3.60726733e-308
5:     3.60726733e-308]]
5: 
5:   [[3.60726733e-308 3.60726733e-308 3.60726733e-308 3.60726733e-308
5:     3.60726733e-308]]
5: 
5:   [[3.60726733e-308 3.60726733e-308 3.60726733e-308 3.60726733e-308
5:     3.60726733e-308]]]
5: 
5: 
5:  [[[4.16208422e-308 4.16208422e-308 4.16208422e-308 4.16208422e-308
5:     4.16208422e-308]]
5: 
5:   [[4.16208422e-308 4.16208422e-308 4.16208422e-308 4.16208422e-308
5:     4.16208422e-308]]
5: 
5:   [[4.16208422e-308 4.16208422e-308 4.16208422e-308 4.16208422e-308
5:     4.16208422e-308]]]
5: 
5: 
5:  [[[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]]
5: 
5: 
5:  [[[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]]
5: 
5: 
5:  [[[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]]
5: 
5: 
5:  [[[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]
5: 
5:   [[4.16208433e-308 4.16208433e-308 4.16208433e-308 4.16208433e-308
5:     4.16208433e-308]]]
5: 
5: 
5:  [[[4.16208432e-308 4.16208432e-308 4.16208432e-308 4.16208432e-308
5:     4.16208432e-308]]
5: 
5:   [[4.16208432e-308 4.16208432e-308 4.16208432e-308 4.16208432e-308
5:     4.16208432e-308]]
5: 
5:   [[4.16208432e-308 4.16208432e-308 4.16208432e-308 4.16208432e-308
5:     4.16208432e-308]]]
5: 
5: 
5:  [[[4.16208432e-308 4.16208432e-308 4.16208432e-308 4.16208432e-308
5:     4.16208432e-308]]
5: 
5:   [[4.16208432e-308 4.16208432e-308 4.16208432e-308 4.16208432e-308
5:     4.16208432e-308]]
5: 
5:   [[4.16208432e-308 4.16208432e-308 4.16208432e-308 4.16208432e-308
5:     4.16208432e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208462e-308 4.16208462e-308 4.16208462e-308 4.16208462e-308
5:     4.16208462e-308]]
5: 
5:   [[4.16208462e-308 4.16208462e-308 4.16208462e-308 4.16208462e-308
5:     4.16208462e-308]]
5: 
5:   [[4.16208462e-308 4.16208462e-308 4.16208462e-308 4.16208462e-308
5:     4.16208462e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]
5: 
5:   [[4.16208431e-308 4.16208431e-308 4.16208431e-308 4.16208431e-308
5:     4.16208431e-308]]]
5: 
5: 
5:  [[[4.16208430e-308 4.16208430e-308 4.16208430e-308 4.16208430e-308
5:     4.16208430e-308]]
5: 
5:   [[4.16208430e-308 4.16208430e-308 4.16208430e-308 4.16208430e-308
5:     4.16208430e-308]]
5: 
5:   [[4.16208430e-308 4.16208430e-308 4.16208430e-308 4.16208430e-308
5:     4.16208430e-308]]]
5: 
5: 
5:  [[[4.16208430e-308 4.16208430e-308 4.16208430e-308 4.16208430e-308
5:     4.16208430e-308]]
5: 
5:   [[4.16208430e-308 4.16208430e-308 4.16208430e-308 4.16208430e-308
5:     4.16208430e-308]]
5: 
5:   [[4.16208430e-308 4.16208430e-308 4.16208430e-308 4.16208430e-308
5:     4.16208430e-308]]]
5: 
5: 
5:  [[[4.16208436e-308 4.16208436e-308 4.16208436e-308 4.16208436e-308
5:     4.16208436e-308]]
5: 
5:   [[4.16208436e-308 4.16208436e-308 4.16208436e-308 4.16208436e-308
5:     4.16208436e-308]]
5: 
5:   [[4.16208436e-308 4.16208436e-308 4.16208436e-308 4.16208436e-308
5:     4.16208436e-308]]]
5: 
5: 
5:  [[[4.16208437e-308 4.16208437e-308 4.16208437e-308 4.16208437e-308
5:     4.16208437e-308]]
5: 
5:   [[4.16208437e-308 4.16208437e-308 4.16208437e-308 4.16208437e-308
5:     4.16208437e-308]]
5: 
5:   [[4.16208437e-308 4.16208437e-308 4.16208437e-308 4.16208437e-308
5:     4.16208437e-308]]]]
5: 
5: Mismatched elements: 435 / 450 (96.7%)
5: Max absolute difference among violations: 4.072836e+277
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[[ 3.188651e-313,  3.188651e-313,  3.188651e-313,
5:            3.188651e-313,  3.188651e-313]],
5: ...
5:  DESIRED: array([[[[3.722739e+243, 3.722739e+243, 3.722739e+243, 3.722739e+243,
5:           3.722739e+243]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[[4.19345623e-236 4.19345623e-236 4.19345623e-236 4.19345623e-236
5:     4.19345623e-236]]
5: 
5:   [[4.19345623e-236 4.19345623e-236 4.19345623e-236 4.19345623e-236
5:     4.19345623e-236]]
5: 
5:   [[4.19345623e-236 4.19345623e-236 4.19345623e-236 4.19345623e-236
5:     4.19345623e-236]]]
5: 
5: 
5:  [[[9.91987780e-316 9.91987780e-316 9.91987780e-316 9.91987780e-316
5:     9.91987780e-316]]
5: 
5:   [[9.91987780e-316 9.91987780e-316 9.91987780e-316 9.91987780e-316
5:     9.91987780e-316]]
5: 
5:   [[9.91987780e-316 9.91987780e-316 9.91987780e-316 9.91987780e-316
5:     9.91987780e-316]]]
5: 
5: 
5:  [[[9.91993867e-316 9.91993867e-316 9.91993867e-316 9.91993867e-316
5:     9.91993867e-316]]
5: 
5:   [[9.91993867e-316 9.91993867e-316 9.91993867e-316 9.91993867e-316
5:     9.91993867e-316]]
5: 
5:   [[9.91993867e-316 9.91993867e-316 9.91993867e-316 9.91993867e-316
5:     9.91993867e-316]]]
5: 
5: 
5:  [[[4.96001682e-316 4.96001682e-316 4.96001682e-316 4.96001682e-316
5:     4.96001682e-316]]
5: 
5:   [[4.96001682e-316 4.96001682e-316 4.96001682e-316 4.96001682e-316
5:     4.96001682e-316]]
5: 
5:   [[4.96001682e-316 4.96001682e-316 4.96001682e-316 4.96001682e-316
5:     4.96001682e-316]]]
5: 
5: 
5:  [[[5.94857286e+228 5.94857286e+228 5.94857286e+228 5.94857286e+228
5:     5.94857286e+228]]
5: 
5:   [[5.94857286e+228 5.94857286e+228 5.94857286e+228 5.94857286e+228
5:     5.94857286e+228]]
5: 
5:   [[5.94857286e+228 5.94857286e+228 5.94857286e+228 5.94857286e+228
5:     5.94857286e+228]]]
5: 
5: 
5:  [[[9.91990369e-316 9.91990369e-316 9.91990369e-316 9.91990369e-316
5:     9.91990369e-316]]
5: 
5:   [[9.91990369e-316 9.91990369e-316 9.91990369e-316 9.91990369e-316
5:     9.91990369e-316]]
5: 
5:   [[9.91990369e-316 9.91990369e-316 9.91990369e-316 9.91990369e-316
5:     9.91990369e-316]]]
5: 
5: 
5:  [[[4.96002813e-316 4.96002813e-316 4.96002813e-316 4.96002813e-316
5:     4.96002813e-316]]
5: 
5:   [[4.96002813e-316 4.96002813e-316 4.96002813e-316 4.96002813e-316
5:     4.96002813e-316]]
5: 
5:   [[4.96002813e-316 4.96002813e-316 4.96002813e-316 4.96002813e-316
5:     4.96002813e-316]]]
5: 
5: 
5:  [[[5.65714137e-316 5.65714137e-316 5.65714137e-316 5.65714137e-316
5:     5.65714137e-316]]
5: 
5:   [[5.65714137e-316 5.65714137e-316 5.65714137e-316 5.65714137e-316
5:     5.65714137e-316]]
5: 
5:   [[5.65714137e-316 5.65714137e-316 5.65714137e-316 5.65714137e-316
5:     5.65714137e-316]]]
5: 
5: 
5:  [[[4.95772751e-316 4.95772751e-316 4.95772751e-316 4.95772751e-316
5:     4.95772751e-316]]
5: 
5:   [[4.95772751e-316 4.95772751e-316 4.95772751e-316 4.95772751e-316
5:     4.95772751e-316]]
5: 
5:   [[4.95772751e-316 4.95772751e-316 4.95772751e-316 4.95772751e-316
5:     4.95772751e-316]]]
5: 
5: 
5:  [[[1.06166504e-315 1.06166504e-315 1.06166504e-315 1.06166504e-315
5:     1.06166504e-315]]
5: 
5:   [[1.06166504e-315 1.06166504e-315 1.06166504e-315 1.06166504e-315
5:     1.06166504e-315]]
5: 
5:   [[1.06166504e-315 1.06166504e-315 1.06166504e-315 1.06166504e-315
5:     1.06166504e-315]]]
5: 
5: 
5:  [[[1.99034017e+161 1.99034017e+161 1.99034017e+161 1.99034017e+161
5:     1.99034017e+161]]
5: 
5:   [[1.99034017e+161 1.99034017e+161 1.99034017e+161 1.99034017e+161
5:     1.99034017e+161]]
5: 
5:   [[1.99034017e+161 1.99034017e+161 1.99034017e+161 1.99034017e+161
5:     1.99034017e+161]]]
5: 
5: 
5:  [[[9.08420492e-280 9.08420492e-280 9.08420492e-280 9.08420492e-280
5:     9.08420492e-280]]
5: 
5:   [[9.08420492e-280 9.08420492e-280 9.08420492e-280 9.08420492e-280
5:     9.08420492e-280]]
5: 
5:   [[9.08420492e-280 9.08420492e-280 9.08420492e-280 9.08420492e-280
5:     9.08420492e-280]]]
5: 
5: 
5:  [[[3.45043490e+175 3.45043490e+175 3.45043490e+175 3.45043490e+175
5:     3.45043490e+175]]
5: 
5:   [[3.45043490e+175 3.45043490e+175 3.45043490e+175 3.45043490e+175
5:     3.45043490e+175]]
5: 
5:   [[3.45043490e+175 3.45043490e+175 3.45043490e+175 3.45043490e+175
5:     3.45043490e+175]]]
5: 
5: 
5:  [[[1.99446555e+161 1.99446555e+161 1.99446555e+161 1.99446555e+161
5:     1.99446555e+161]]
5: 
5:   [[1.99446555e+161 1.99446555e+161 1.99446555e+161 1.99446555e+161
5:     1.99446555e+161]]
5: 
5:   [[1.99446555e+161 1.99446555e+161 1.99446555e+161 1.99446555e+161
5:     1.99446555e+161]]]
5: 
5: 
5:  [[[5.26047668e+170 5.26047668e+170 5.26047668e+170 5.26047668e+170
5:     5.26047668e+170]]
5: 
5:   [[5.26047668e+170 5.26047668e+170 5.26047668e+170 5.26047668e+170
5:     5.26047668e+170]]
5: 
5:   [[5.26047668e+170 5.26047668e+170 5.26047668e+170 5.26047668e+170
5:     5.26047668e+170]]]
5: 
5: 
5:  [[[7.71521092e+044 7.71521092e+044 7.71521092e+044 7.71521092e+044
5:     7.71521092e+044]]
5: 
5:   [[7.71521092e+044 7.71521092e+044 7.71521092e+044 7.71521092e+044
5:     7.71521092e+044]]
5: 
5:   [[7.71521092e+044 7.71521092e+044 7.71521092e+044 7.71521092e+044
5:     7.71521092e+044]]]
5: 
5: 
5:  [[[6.93787085e-310 6.93787085e-310 6.93787085e-310 6.93787085e-310
5:     6.93787085e-310]]
5: 
5:   [[6.93787085e-310 6.93787085e-310 6.93787085e-310 6.93787085e-310
5:     6.93787085e-310]]
5: 
5:   [[6.93787085e-310 6.93787085e-310 6.93787085e-310 6.93787085e-310
5:     6.93787085e-310]]]
5: 
5: 
5:  [[[9.91973625e-316 9.91973625e-316 9.91973625e-316 9.91973625e-316
5:     9.91973625e-316]]
5: 
5:   [[9.91973625e-316 9.91973625e-316 9.91973625e-316 9.91973625e-316
5:     9.91973625e-316]]
5: 
5:   [[9.91973625e-316 9.91973625e-316 9.91973625e-316 9.91973625e-316
5:     9.91973625e-316]]]
5: 
5: 
5:  [[[3.48191142e+227 3.48191142e+227 3.48191142e+227 3.48191142e+227
5:     3.48191142e+227]]
5: 
5:   [[3.48191142e+227 3.48191142e+227 3.48191142e+227 3.48191142e+227
5:     3.48191142e+227]]
5: 
5:   [[3.48191142e+227 3.48191142e+227 3.48191142e+227 3.48191142e+227
5:     3.48191142e+227]]]
5: 
5: 
5:  [[[1.99108518e+209 1.99108518e+209 1.99108518e+209 1.99108518e+209
5:     1.99108518e+209]]
5: 
5:   [[1.99108518e+209 1.99108518e+209 1.99108518e+209 1.99108518e+209
5:     1.99108518e+209]]
5: 
5:   [[1.99108518e+209 1.99108518e+209 1.99108518e+209 1.99108518e+209
5:     1.99108518e+209]]]
5: 
5: 
5:  [[[4.49651168e+251 4.49651168e+251 4.49651168e+251 4.49651168e+251
5:     4.49651168e+251]]
5: 
5:   [[4.49651168e+251 4.49651168e+251 4.49651168e+251 4.49651168e+251
5:     4.49651168e+251]]
5: 
5:   [[4.49651168e+251 4.49651168e+251 4.49651168e+251 4.49651168e+251
5:     4.49651168e+251]]]
5: 
5: 
5:  [[[2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:     2.88067937e+214]]
5: 
5:   [[2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:     2.88067937e+214]]
5: 
5:   [[2.88067937e+214 2.88067937e+214 2.88067937e+214 2.88067937e+214
5:     2.88067937e+214]]]
5: 
5: 
5:  [[[1.96502797e+214 1.96502797e+214 1.96502797e+214 1.96502797e+214
5:     1.96502797e+214]]
5: 
5:   [[1.96502797e+214 1.96502797e+214 1.96502797e+214 1.96502797e+214
5:     1.96502797e+214]]
5: 
5:   [[1.96502797e+214 1.96502797e+214 1.96502797e+214 1.96502797e+214
5:     1.96502797e+214]]]
5: 
5: 
5:  [[[3.48191142e+227 3.48191142e+227 3.48191142e+227 3.48191142e+227
5:     3.48191142e+227]]
5: 
5:   [[3.48191142e+227 3.48191142e+227 3.48191142e+227 3.48191142e+227
5:     3.48191142e+227]]
5: 
5:   [[3.48191142e+227 3.48191142e+227 3.48191142e+227 3.48191142e+227
5:     3.48191142e+227]]]
5: 
5: 
5:  [[[1.99108518e+209 1.99108518e+209 1.99108518e+209 1.99108518e+209
5:     1.99108518e+209]]
5: 
5:   [[1.99108518e+209 1.99108518e+209 1.99108518e+209 1.99108518e+209
5:     1.99108518e+209]]
5: 
5:   [[1.99108518e+209 1.99108518e+209 1.99108518e+209 1.99108518e+209
5:     1.99108518e+209]]]
5: 
5: 
5:  [[[4.49651168e+251 4.49651168e+251 4.49651168e+251 4.49651168e+251
5:     4.49651168e+251]]
5: 
5:   [[4.49651168e+251 4.49651168e+251 4.49651168e+251 4.49651168e+251
5:     4.49651168e+251]]
5: 
5:   [[4.49651168e+251 4.49651168e+251 4.49651168e+251 4.49651168e+251
5:     4.49651168e+251]]]
5: 
5: 
5:  [[[6.96953852e+199 6.96953852e+199 6.96953852e+199 6.96953852e+199
5:     6.96953852e+199]]
5: 
5:   [[6.96953852e+199 6.96953852e+199 6.96953852e+199 6.96953852e+199
5:     6.96953852e+199]]
5: 
5:   [[6.96953852e+199 6.96953852e+199 6.96953852e+199 6.96953852e+199
5:     6.96953852e+199]]]
5: 
5: 
5:  [[[5.82663669e+252 5.82663669e+252 5.82663669e+252 5.82663669e+252
5:     5.82663669e+252]]
5: 
5:   [[5.82663669e+252 5.82663669e+252 5.82663669e+252 5.82663669e+252
5:     5.82663669e+252]]
5: 
5:   [[5.82663669e+252 5.82663669e+252 5.82663669e+252 5.82663669e+252
5:     5.82663669e+252]]]
5: 
5: 
5:  [[[5.44345563e+257 5.44345563e+257 5.44345563e+257 5.44345563e+257
5:     5.44345563e+257]]
5: 
5:   [[5.44345563e+257 5.44345563e+257 5.44345563e+257 5.44345563e+257
5:     5.44345563e+257]]
5: 
5:   [[5.44345563e+257 5.44345563e+257 5.44345563e+257 5.44345563e+257
5:     5.44345563e+257]]]
5: 
5: 
5:  [[[1.36175072e+069 1.36175072e+069 1.36175072e+069 1.36175072e+069
5:     1.36175072e+069]]
5: 
5:   [[1.36175072e+069 1.36175072e+069 1.36175072e+069 1.36175072e+069
5:     1.36175072e+069]]
5: 
5:   [[1.36175072e+069 1.36175072e+069 1.36175072e+069 1.36175072e+069
5:     1.36175072e+069]]]]
5:  eager grad out tensor:
5: [[[[ 1.06303852e-315  1.06303852e-315  1.06303852e-315  1.06303852e-315
5:      1.06303852e-315]]
5: 
5:   [[ 1.06303852e-315  1.06303852e-315  1.06303852e-315  1.06303852e-315
5:      1.06303852e-315]]
5: 
5:   [[ 1.06303852e-315  1.06303852e-315  1.06303852e-315  1.06303852e-315
5:      1.06303852e-315]]]
5: 
5: 
5:  [[[ 1.27546045e-312  1.27546045e-312  1.27546045e-312  1.27546045e-312
5:      1.27546045e-312]]
5: 
5:   [[ 1.27546045e-312  1.27546045e-312  1.27546045e-312  1.27546045e-312
5:      1.27546045e-312]]
5: 
5:   [[ 1.27546045e-312  1.27546045e-312  1.27546045e-312  1.27546045e-312
5:      1.27546045e-312]]]
5: 
5: 
5:  [[[ 1.27546045e-312  1.27546045e-312  1.27546045e-312  1.27546045e-312
5:      1.27546045e-312]]
5: 
5:   [[ 1.27546045e-312  1.27546045e-312  1.27546045e-312  1.27546045e-312
5:      1.27546045e-312]]
5: 
5:   [[ 1.27546045e-312  1.27546045e-312  1.27546045e-312  1.27546045e-312
5:      1.27546045e-312]]]
5: 
5: 
5:  [[[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]
5: 
5:   [[ 9.61987860e+228  9.61987860e+228  9.61987860e+228  9.61987860e+228
5:      9.61987860e+228]]]
5: 
5: 
5:  [[[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]
5: 
5:   [[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]
5: 
5:   [[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]]
5: 
5: 
5:  [[[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]
5: 
5:   [[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]
5: 
5:   [[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]]
5: 
5: 
5:  [[[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]
5: 
5:   [[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]
5: 
5:   [[ 3.84795144e+229  3.84795144e+229  3.84795144e+229  3.84795144e+229
5:      3.84795144e+229]]]
5: 
5: 
5:  [[[ 2.03641800e+277  2.03641800e+277  2.03641800e+277  2.03641800e+277
5:      2.03641800e+277]]
5: 
5:   [[ 2.03641800e+277  2.03641800e+277  2.03641800e+277  2.03641800e+277
5:      2.03641800e+277]]
5: 
5:   [[ 2.03641800e+277  2.03641800e+277  2.03641800e+277  2.03641800e+277
5:      2.03641800e+277]]]
5: 
5: 
5:  [[[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]
5: 
5:   [[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]
5: 
5:   [[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]]
5: 
5: 
5:  [[[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]
5: 
5:   [[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]
5: 
5:   [[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]]
5: 
5: 
5:  [[[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]
5: 
5:   [[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]
5: 
5:   [[ 4.07283600e+277  4.07283600e+277  4.07283600e+277  4.07283600e+277
5:      4.07283600e+277]]]
5: 
5: 
5:  [[[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]
5: 
5:   [[ 1.01820900e+277  1.01820900e+277  1.01820900e+277  1.01820900e+277
5:      1.01820900e+277]]]
5: 
5: 
5:  [[[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]
5: 
5:   [[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]
5: 
5:   [[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]]
5: 
5: 
5:  [[[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]
5: 
5:   [[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]
5: 
5:   [[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]]
5: 
5: 
5:  [[[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]
5: 
5:   [[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]
5: 
5:   [[-3.30812235e+004 -3.30812235e+004 -3.30812235e+004 -3.30812235e+004
5:     -3.30812235e+004]]]
5: 
5: 
5:  [[[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]
5: 
5:   [[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]
5: 
5:   [[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]]
5: 
5: 
5:  [[[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]
5: 
5:   [[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]
5: 
5:   [[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]]
5: 
5: 
5:  [[[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]
5: 
5:   [[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]
5: 
5:   [[ 1.89034114e+258  1.89034114e+258  1.89034114e+258  1.89034114e+258
5:      1.89034114e+258]]]
5: 
5: 
5:  [[[ 1.41775586e+258  1.41775586e+258  1.41775586e+258  1.41775586e+258
5:      1.41775586e+258]]
5: 
5:   [[ 1.41775586e+258  1.41775586e+258  1.41775586e+258  1.41775586e+258
5:      1.41775586e+258]]
5: 
5:   [[ 1.41775586e+258  1.41775586e+258  1.41775586e+258  1.41775586e+258
5:      1.41775586e+258]]]
5: 
5: 
5:  [[[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]
5: 
5:   [[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]
5: 
5:   [[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]]
5: 
5: 
5:  [[[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]
5: 
5:   [[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]
5: 
5:   [[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]]
5: 
5: 
5:  [[[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]
5: 
5:   [[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]
5: 
5:   [[ 1.00524914e+181  1.00524914e+181  1.00524914e+181  1.00524914e+181
5:      1.00524914e+181]]]
5: 
5: 
5:  [[[ 5.02624570e+180  5.02624570e+180  5.02624570e+180  5.02624570e+180
5:      5.02624570e+180]]
5: 
5:   [[ 5.02624570e+180  5.02624570e+180  5.02624570e+180  5.02624570e+180
5:      5.02624570e+180]]
5: 
5:   [[ 5.02624570e+180  5.02624570e+180  5.02624570e+180  5.02624570e+180
5:      5.02624570e+180]]]
5: 
5: 
5:  [[[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]
5: 
5:   [[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]
5: 
5:   [[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]]
5: 
5: 
5:  [[[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]
5: 
5:   [[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]
5: 
5:   [[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]]
5: 
5: 
5:  [[[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]
5: 
5:   [[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]
5: 
5:   [[ 1.48446352e+007  1.48446352e+007  1.48446352e+007  1.48446352e+007
5:      1.48446352e+007]]]
5: 
5: 
5:  [[[ 4.14201090e+253  4.14201090e+253  4.14201090e+253  4.14201090e+253
5:      4.14201090e+253]]
5: 
5:   [[ 4.14201090e+253  4.14201090e+253  4.14201090e+253  4.14201090e+253
5:      4.14201090e+253]]
5: 
5:   [[ 4.14201090e+253  4.14201090e+253  4.14201090e+253  4.14201090e+253
5:      4.14201090e+253]]]
5: 
5: 
5:  [[[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]
5: 
5:   [[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]
5: 
5:   [[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]]
5: 
5: 
5:  [[[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]
5: 
5:   [[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]
5: 
5:   [[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]]
5: 
5: 
5:  [[[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]
5: 
5:   [[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]
5: 
5:   [[ 5.52268119e+253  5.52268119e+253  5.52268119e+253  5.52268119e+253
5:      5.52268119e+253]]]]
5: 
5: Mismatched elements: 405 / 450 (90%)
5: Max absolute difference among violations: 4.072836e+277
5: Max relative difference among violations: 3.02904829e+244
5:  ACTUAL: array([[[[4.193456e-236, 4.193456e-236, 4.193456e-236, 4.193456e-236,
5:           4.193456e-236]],
5: ...
5:  DESIRED: array([[[[ 1.063039e-315,  1.063039e-315,  1.063039e-315,
5:            1.063039e-315,  1.063039e-315]],
5: ...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_commonuse_2)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[[6.380579e-162, 3.815496e-181, 2.312592e-239, 4.257599e-133,
5:           4.174101e-157],
5:          [2.009436e-175, 1.201614e-194, 7.283045e-253, 1.340846e-146,...
5:  DESIRED: array([[[[2.449709e-01, 3.097668e-01, 2.512096e-01, 5.649313e-02,
5:           9.276286e-02],
5:          [7.420794e-01, 9.383629e-01, 7.609783e-01, 1.711321e-01,...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_scalar)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [3.22560391e+253]
5:  eager grad out tensor:
5: [3.75449371e+277]
5: 
5: Mismatched elements: 1 / 1 (100%)
5: Max absolute difference among violations: 3.75449371e+277
5: Max relative difference among violations: 1.
5:  ACTUAL: array([3.225604e+253])
5:  DESIRED: array([3.754494e+277])
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_scalar)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 892, in check
5:     self.check_jit_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1276, in check_jit_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check jit comp grad out failed. Mismatch between jit comp and eager on Place(cpu), when enable_fw_comp is True, enable_rev_comp is True,the grad out tensor's index is : 1 
5: jit comp grad out tensor:
5: [9.93669318e+247]
5:  eager grad out out tensor:
5: [4.10453681e-289]
5: 
5: Mismatched elements: 1 / 1 (100%)
5: Max absolute difference among violations: 9.93669318e+247
5: Max relative difference among violations: inf
5:  ACTUAL: array([9.936693e+247])
5:  DESIRED: array([4.104537e-289])
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_scalar)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 120 / 120 (100%)
5: Max absolute difference among violations: 0.67736189
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[[0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000],
5:         [0.000000e+000, 0.000000e+000, 0.000000e+000, 1.648214e-136],
5:         [0.000000e+000, 1.648214e-136, 0.000000e+000, 1.648214e-136]],...
5:  DESIRED: array([[[0.146133, 0.188526, 0.502984, 0.379556],
5:         [0.227051, 0.368198, 0.470588, 0.618513],
5:         [0.393779, 0.157784, 0.506332, 0.527402]],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestElementwiseMulOp_xsize_lessthan_ysize)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 68, in test_check_grad_ingore_x
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[[[4.86079648e-316 5.00078227e-316 2.91498731e-322 2.51968629e+180
5:     4.34256169e-114 9.53546696e-322 5.00078227e-316 6.93786520e-310
5:     1.17075772e+214 1.16563652e+253]
5:    [2.13375552e-081 5.26488672e+170 2.27551609e+161 5.82281285e+252
5:     3.99910968e+252 9.78188318e+199 1.25013748e+243 1.04918961e-153
5:     6.81778292e-038 1.03336026e+243]
5:    [6.98896783e+228 9.06661261e-315 7.11454530e-322 2.37151510e-322
5:     5.00074670e-316 4.19955799e-322 4.19955799e-322 5.96531086e-114
5:     1.18575755e-321 4.74303020e-322]
5:    [5.00079492e-316 1.17119059e+166 5.91870742e-061 5.82663669e+252
5:     6.07858039e+247 6.80552218e+212 1.17996670e-095 1.17074098e+214
5:     4.88065533e+252 2.41784260e+198]
5:    [5.58902083e-109 3.21142670e-322 5.00071824e-316 5.00077279e-316
5:     4.94065646e-324 2.12199579e-314 4.99620169e-316 2.96439388e-323
5:     1.97626258e-320 6.32404027e-322]
5:    [5.00077595e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00060678e-316 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     3.90311860e-322 3.21142670e-322 4.99936331e-316 5.66144369e-316
5:     4.94065646e-324 6.36598737e-314]
5:    [4.99735267e-316 2.96439388e-323 2.07112319e-320 6.32404027e-322
5:     5.00080282e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00063049e-316 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     5.43472210e-322 2.06025374e-321]
5:    [4.86079648e-316 5.00042180e-316 2.32210854e-322 5.16386450e-109
5:     5.00081073e-316 7.16395186e-322 5.00165815e-316 5.00074907e-316
5:     1.17075772e+214 1.16563652e+253]]
5: 
5:   [[4.86079648e-316 5.00078227e-316 2.91498731e-322 2.51968629e+180
5:     4.34256169e-114 9.53546696e-322 5.00078227e-316 6.93786520e-310
5:     1.17075772e+214 1.16563652e+253]
5:    [2.13375552e-081 5.26488672e+170 2.27551609e+161 5.82281285e+252
5:     3.99910968e+252 9.78188318e+199 1.25013748e+243 1.04918961e-153
5:     6.81778292e-038 1.03336026e+243]
5:    [6.98896783e+228 9.06661261e-315 7.11454530e-322 2.37151510e-322
5:     5.00074670e-316 4.19955799e-322 4.19955799e-322 5.96531086e-114
5:     1.18575755e-321 4.74303020e-322]
5:    [5.00079492e-316 1.17119059e+166 5.91870742e-061 5.82663669e+252
5:     6.07858039e+247 6.80552218e+212 1.17996670e-095 1.17074098e+214
5:     4.88065533e+252 2.41784260e+198]
5:    [5.58902083e-109 3.21142670e-322 5.00071824e-316 5.00077279e-316
5:     4.94065646e-324 2.12199579e-314 4.99620169e-316 2.96439388e-323
5:     1.97626258e-320 6.32404027e-322]
5:    [5.00077595e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00060678e-316 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     3.90311860e-322 3.21142670e-322 4.99936331e-316 5.66144369e-316
5:     4.94065646e-324 6.36598737e-314]
5:    [4.99735267e-316 2.96439388e-323 2.07112319e-320 6.32404027e-322
5:     5.00080282e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00063049e-316 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     5.43472210e-322 2.06025374e-321]
5:    [4.86079648e-316 5.00042180e-316 2.32210854e-322 5.16386450e-109
5:     5.00081073e-316 7.16395186e-322 5.00165815e-316 5.00074907e-316
5:     1.17075772e+214 1.16563652e+253]]]
5: 
5: 
5:  [[[4.86079648e-316 5.00078227e-316 2.91498731e-322 2.51968629e+180
5:     4.34256169e-114 9.53546696e-322 5.00078227e-316 6.93786520e-310
5:     1.17075772e+214 1.16563652e+253]
5:    [2.13375552e-081 5.26488672e+170 2.27551609e+161 5.82281285e+252
5:     3.99910968e+252 9.78188318e+199 1.25013748e+243 1.04918961e-153
5:     6.81778292e-038 1.03336026e+243]
5:    [6.98896783e+228 9.06661261e-315 7.11454530e-322 2.37151510e-322
5:     5.00074670e-316 4.19955799e-322 4.19955799e-322 5.96531086e-114
5:     1.18575755e-321 4.74303020e-322]
5:    [5.00079492e-316 1.17119059e+166 5.91870742e-061 5.82663669e+252
5:     6.07858039e+247 6.80552218e+212 1.17996670e-095 1.17074098e+214
5:     4.88065533e+252 2.41784260e+198]
5:    [5.58902083e-109 3.21142670e-322 5.00071824e-316 5.00077279e-316
5:     4.94065646e-324 2.12199579e-314 4.99620169e-316 2.96439388e-323
5:     1.97626258e-320 6.32404027e-322]
5:    [5.00077595e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00060678e-316 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     3.90311860e-322 3.21142670e-322 4.99936331e-316 5.66144369e-316
5:     4.94065646e-324 6.36598737e-314]
5:    [4.99735267e-316 2.96439388e-323 2.07112319e-320 6.32404027e-322
5:     5.00080282e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00063049e-316 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     5.43472210e-322 2.06025374e-321]
5:    [4.86079648e-316 5.00042180e-316 2.32210854e-322 5.16386450e-109
5:     5.00081073e-316 7.16395186e-322 5.00165815e-316 5.00074907e-316
5:     1.17075772e+214 1.16563652e+253]]
5: 
5:   [[4.86079648e-316 5.00078227e-316 2.91498731e-322 2.51968629e+180
5:     4.34256169e-114 9.53546696e-322 5.00078227e-316 6.93786520e-310
5:     1.17075772e+214 1.16563652e+253]
5:    [2.13375552e-081 5.26488672e+170 2.27551609e+161 5.82281285e+252
5:     3.99910968e+252 9.78188318e+199 1.25013748e+243 1.04918961e-153
5:     6.81778292e-038 1.03336026e+243]
5:    [6.98896783e+228 9.06661261e-315 7.11454530e-322 2.37151510e-322
5:     5.00074670e-316 4.19955799e-322 4.19955799e-322 5.96531086e-114
5:     1.18575755e-321 4.74303020e-322]
5:    [5.00079492e-316 1.17119059e+166 5.91870742e-061 5.82663669e+252
5:     6.07858039e+247 6.80552218e+212 1.17996670e-095 1.17074098e+214
5:     4.88065533e+252 2.41784260e+198]
5:    [5.58902083e-109 3.21142670e-322 5.00071824e-316 5.00077279e-316
5:     4.94065646e-324 2.12199579e-314 4.99620169e-316 2.96439388e-323
5:     1.97626258e-320 6.32404027e-322]
5:    [5.00077595e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00060678e-316 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     3.90311860e-322 3.21142670e-322 4.99936331e-316 5.66144369e-316
5:     4.94065646e-324 6.36598737e-314]
5:    [4.99735267e-316 2.96439388e-323 2.07112319e-320 6.32404027e-322
5:     5.00080282e-316 0.00000000e+000 5.06911353e-321 0.00000000e+000
5:     5.00063049e-316 0.00000000e+000]
5:    [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000
5:     0.00000000e+000 0.00000000e+000 0.00000000e+000 3.45845952e-323
5:     5.43472210e-322 2.06025374e-321]
5:    [4.86079648e-316 5.00042180e-316 2.32210854e-322 5.16386450e-109
5:     5.00081073e-316 7.16395186e-322 5.00165815e-316 5.00074907e-316
5:     1.17075772e+214 1.16563652e+253]]]]
5:  eager grad out tensor:
5: [[[[6.81810591e-322 0.00000000e+000 3.16202013e-322 9.53546696e-322
5:     4.98860454e-316 6.93786520e-310 7.33952899e+223 5.20547564e-320
5:     4.31853927e-316 7.16395186e-322]
5:    [4.31882465e-316 4.98860454e-316 2.91498731e-322 0.00000000e+000
5:     1.67982320e-322 4.79243676e-322 4.31882465e-316 6.93786520e-310
5:     3.21142670e-322 0.00000000e+000]
5:    [4.31881279e-316 2.42092166e-322 4.31864836e-316 4.31882465e-316
5:     1.48219694e-322 0.00000000e+000 1.24899795e-320 2.37151510e-322
5:     4.31878749e-316 6.42285340e-323]
5:    [5.04870761e+223 2.42748677e-312 1.27271310e-320 2.37151510e-322
5:     4.98867094e-316 1.48219694e-323 3.77081128e-317 0.00000000e+000
5:     2.12199579e-314 1.19069821e-321]
5:    [4.98863300e-316 6.93786520e-310 4.71061266e+257 5.73116149e-322
5:     1.58101007e-322 9.53546696e-322 4.31864836e-316 4.98863300e-316
5:     2.25824004e-306 0.00000000e+000]
5:    [3.95252517e-322 7.16395186e-322 4.31864836e-316 6.93786520e-310
5:     7.90505033e-323 0.00000000e+000 4.31883097e-316 4.79243676e-322
5:     4.31864836e-316 6.93786520e-310]
5:    [1.01246393e-307 0.00000000e+000 2.32210854e-322 2.42092166e-322
5:     4.31864836e-316 6.93786520e-310 5.62323231e+175 1.39672358e-319
5:     1.41500401e-320 2.37151510e-322]
5:    [4.98867332e-316 7.41098469e-323 3.01467462e+161 1.64254070e-306
5:     4.31879856e-316 2.45550626e-321 4.98865988e-316 6.93786520e-310
5:     8.32187719e-315 0.00000000e+000]
5:    [4.98915236e-316 2.21835475e-321 4.98865118e-316 4.98865988e-316
5:     4.34777768e-322 0.00000000e+000 4.94065646e-324 1.98120324e-321
5:     4.98865118e-316 6.93786520e-310]
5:    [4.39718425e-322 0.00000000e+000 4.99202189e-316 1.74405173e-321
5:     4.98865118e-316 6.93786520e-310 4.39718425e-322 0.00000000e+000
5:     1.48219694e-322 1.50690022e-321]]
5: 
5:   [[6.81810591e-322 0.00000000e+000 3.16202013e-322 9.53546696e-322
5:     4.98860454e-316 6.93786520e-310 7.33952899e+223 5.20547564e-320
5:     4.31853927e-316 7.16395186e-322]
5:    [4.31882465e-316 4.98860454e-316 2.91498731e-322 0.00000000e+000
5:     1.67982320e-322 4.79243676e-322 4.31882465e-316 6.93786520e-310
5:     3.21142670e-322 0.00000000e+000]
5:    [4.31881279e-316 2.42092166e-322 4.31864836e-316 4.31882465e-316
5:     1.48219694e-322 0.00000000e+000 1.24899795e-320 2.37151510e-322
5:     4.31878749e-316 6.42285340e-323]
5:    [5.04870761e+223 2.42748677e-312 1.27271310e-320 2.37151510e-322
5:     4.98867094e-316 1.48219694e-323 3.77081128e-317 0.00000000e+000
5:     2.12199579e-314 1.19069821e-321]
5:    [4.98863300e-316 6.93786520e-310 4.71061266e+257 5.73116149e-322
5:     1.58101007e-322 9.53546696e-322 4.31864836e-316 4.98863300e-316
5:     2.25824004e-306 0.00000000e+000]
5:    [3.95252517e-322 7.16395186e-322 4.31864836e-316 6.93786520e-310
5:     7.90505033e-323 0.00000000e+000 4.31883097e-316 4.79243676e-322
5:     4.31864836e-316 6.93786520e-310]
5:    [1.01246393e-307 0.00000000e+000 2.32210854e-322 2.42092166e-322
5:     4.31864836e-316 6.93786520e-310 5.62323231e+175 1.39672358e-319
5:     1.41500401e-320 2.37151510e-322]
5:    [4.98867332e-316 7.41098469e-323 3.01467462e+161 1.64254070e-306
5:     4.31879856e-316 2.45550626e-321 4.98865988e-316 6.93786520e-310
5:     8.32187719e-315 0.00000000e+000]
5:    [4.98915236e-316 2.21835475e-321 4.98865118e-316 4.98865988e-316
5:     4.34777768e-322 0.00000000e+000 4.94065646e-324 1.98120324e-321
5:     4.98865118e-316 6.93786520e-310]
5:    [4.39718425e-322 0.00000000e+000 4.99202189e-316 1.74405173e-321
5:     4.98865118e-316 6.93786520e-310 4.39718425e-322 0.00000000e+000
5:     1.48219694e-322 1.50690022e-321]]]
5: 
5: 
5:  [[[6.81810591e-322 0.00000000e+000 3.16202013e-322 9.53546696e-322
5:     4.98860454e-316 6.93786520e-310 7.33952899e+223 5.20547564e-320
5:     4.31853927e-316 7.16395186e-322]
5:    [4.31882465e-316 4.98860454e-316 2.91498731e-322 0.00000000e+000
5:     1.67982320e-322 4.79243676e-322 4.31882465e-316 6.93786520e-310
5:     3.21142670e-322 0.00000000e+000]
5:    [4.31881279e-316 2.42092166e-322 4.31864836e-316 4.31882465e-316
5:     1.48219694e-322 0.00000000e+000 1.24899795e-320 2.37151510e-322
5:     4.31878749e-316 6.42285340e-323]
5:    [5.04870761e+223 2.42748677e-312 1.27271310e-320 2.37151510e-322
5:     4.98867094e-316 1.48219694e-323 3.77081128e-317 0.00000000e+000
5:     2.12199579e-314 1.19069821e-321]
5:    [4.98863300e-316 6.93786520e-310 4.71061266e+257 5.73116149e-322
5:     1.58101007e-322 9.53546696e-322 4.31864836e-316 4.98863300e-316
5:     2.25824004e-306 0.00000000e+000]
5:    [3.95252517e-322 7.16395186e-322 4.31864836e-316 6.93786520e-310
5:     7.90505033e-323 0.00000000e+000 4.31883097e-316 4.79243676e-322
5:     4.31864836e-316 6.93786520e-310]
5:    [1.01246393e-307 0.00000000e+000 2.32210854e-322 2.42092166e-322
5:     4.31864836e-316 6.93786520e-310 5.62323231e+175 1.39672358e-319
5:     1.41500401e-320 2.37151510e-322]
5:    [4.98867332e-316 7.41098469e-323 3.01467462e+161 1.64254070e-306
5:     4.31879856e-316 2.45550626e-321 4.98865988e-316 6.93786520e-310
5:     8.32187719e-315 0.00000000e+000]
5:    [4.98915236e-316 2.21835475e-321 4.98865118e-316 4.98865988e-316
5:     4.34777768e-322 0.00000000e+000 4.94065646e-324 1.98120324e-321
5:     4.98865118e-316 6.93786520e-310]
5:    [4.39718425e-322 0.00000000e+000 4.99202189e-316 1.74405173e-321
5:     4.98865118e-316 6.93786520e-310 4.39718425e-322 0.00000000e+000
5:     1.48219694e-322 1.50690022e-321]]
5: 
5:   [[6.81810591e-322 0.00000000e+000 3.16202013e-322 9.53546696e-322
5:     4.98860454e-316 6.93786520e-310 7.33952899e+223 5.20547564e-320
5:     4.31853927e-316 7.16395186e-322]
5:    [4.31882465e-316 4.98860454e-316 2.91498731e-322 0.00000000e+000
5:     1.67982320e-322 4.79243676e-322 4.31882465e-316 6.93786520e-310
5:     3.21142670e-322 0.00000000e+000]
5:    [4.31881279e-316 2.42092166e-322 4.31864836e-316 4.31882465e-316
5:     1.48219694e-322 0.00000000e+000 1.24899795e-320 2.37151510e-322
5:     4.31878749e-316 6.42285340e-323]
5:    [5.04870761e+223 2.42748677e-312 1.27271310e-320 2.37151510e-322
5:     4.98867094e-316 1.48219694e-323 3.77081128e-317 0.00000000e+000
5:     2.12199579e-314 1.19069821e-321]
5:    [4.98863300e-316 6.93786520e-310 4.71061266e+257 5.73116149e-322
5:     1.58101007e-322 9.53546696e-322 4.31864836e-316 4.98863300e-316
5:     2.25824004e-306 0.00000000e+000]
5:    [3.95252517e-322 7.16395186e-322 4.31864836e-316 6.93786520e-310
5:     7.90505033e-323 0.00000000e+000 4.31883097e-316 4.79243676e-322
5:     4.31864836e-316 6.93786520e-310]
5:    [1.01246393e-307 0.00000000e+000 2.32210854e-322 2.42092166e-322
5:     4.31864836e-316 6.93786520e-310 5.62323231e+175 1.39672358e-319
5:     1.41500401e-320 2.37151510e-322]
5:    [4.98867332e-316 7.41098469e-323 3.01467462e+161 1.64254070e-306
5:     4.31879856e-316 2.45550626e-321 4.98865988e-316 6.93786520e-310
5:     8.32187719e-315 0.00000000e+000]
5:    [4.98915236e-316 2.21835475e-321 4.98865118e-316 4.98865988e-316
5:     4.34777768e-322 0.00000000e+000 4.94065646e-324 1.98120324e-321
5:     4.98865118e-316 6.93786520e-310]
5:    [4.39718425e-322 0.00000000e+000 4.99202189e-316 1.74405173e-321
5:     4.98865118e-316 6.93786520e-310 4.39718425e-322 0.00000000e+000
5:     1.48219694e-322 1.50690022e-321]]]]
5: 
5: Mismatched elements: 100 / 400 (25%)
5: Max absolute difference among violations: 4.71061266e+257
5: Max relative difference among violations: inf
5:  ACTUAL: array([[[[4.860796e-316, 5.000782e-316, 2.914987e-322, 2.519686e+180,
5:           4.342562e-114, 9.535467e-322, 5.000782e-316, 6.937865e-310,
5:           1.170758e+214, 1.165637e+253],...
5:  DESIRED: array([[[[6.818106e-322, 0.000000e+000, 3.162020e-322, 9.535467e-322,
5:           4.988605e-316, 6.937865e-310, 7.339529e+223, 5.205476e-320,
5:           4.318539e-316, 7.163952e-322],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestElementwiseMulOp_xsize_lessthan_ysize)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 80, in test_check_grad_ingore_y
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3043, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 901, in check
5:     self.check_jit_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1276, in check_jit_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check jit comp grad out failed. Mismatch between jit comp and eager on Place(cpu), when enable_fw_comp is True, enable_rev_comp is True,the grad out tensor's index is : 0 
5: jit comp grad out tensor:
5: [[5.38162829e-104 5.36653903e-104             inf 1.55003230e-123
5:               inf 1.39937235e-152 5.38163585e-104             inf
5:   5.38162829e-104 4.55722521e-133]
5:  [1.34765799e-123 1.27655709e-123 5.47582069e-075 7.43053201e-085
5:   2.20645280e-152 1.52436699e-132 5.06745525e-075             inf
5:   4.64836793e-075 1.95000662e-152]
5:  [2.13833808e-132 3.06130471e-095             inf 1.27649033e-123
5:   1.17847712e-142 2.55490146e-075 7.43073490e-085 4.64836793e-075
5:   1.18579201e-171 1.95732913e-123]
5:  [5.05442060e-075 8.42136930e-114 1.10022221e-151 7.99617174e+261
5:   1.17986777e-084 2.41024121e-123             inf 3.86386599e-124
5:               inf 2.74622507e-094]
5:  [2.74531868e-094             inf 1.64986164e-114             inf
5:   8.46376310e-066             inf 1.34870395e-123 2.58141227e-133
5:   2.35843043e-065 2.84667466e-132]
5:  [            inf 1.34765793e-123 4.33687540e-112             inf
5:   5.38162829e-104 7.12757476e-105             inf 1.27649827e-123
5:   8.45938630e-066 1.27649428e-123]
5:  [3.54003585e-142             inf             inf 7.81429176e-152
5:   1.95000662e-152 7.99617174e+261 8.49225769e-123 5.47587435e-075
5:               inf 5.05442060e-075]
5:  [2.55490146e-075 1.49511581e-093 2.85942135e-075 8.36106664e+281
5:   7.50141439e-114 2.99987975e-113 5.78515305e-114 6.66326040e-085
5:   5.47582069e-075 1.48189015e-074]
5:  [7.43093780e-085 9.73019004e-066 5.94904160e+281 1.25299765e-113
5:   1.36458378e-065 3.61842484e-104 1.09684292e-065 1.95450737e-152
5:   3.86940098e-065 1.27649033e-123]
5:  [            inf 5.05991956e-133             inf 2.07926429e-093
5:   2.74622507e-094             inf 1.64986164e-114             inf
5:   2.91746357e-123             inf]]
5:  eager grad out out tensor:
5: [[2.31983368e-052 2.31657916e-052 3.11087774e+179 3.93704495e-062
5:   2.62398069e+179 1.18295070e-076 2.31983531e-052 2.62459155e+179
5:   2.31983368e-052 6.75072234e-067]
5:  [3.67104616e-062 3.57289392e-062 7.39987884e-038 8.62005337e-043
5:   1.48541334e-076 1.23465258e-066 7.11860608e-038 2.95130515e+179
5:   6.81789405e-038 1.39642638e-076]
5:  [1.46230574e-066 5.53290585e-048 3.43822948e+179 3.57280048e-062
5:   1.08557686e-071 5.05460330e-038 8.62017106e-043 6.81789405e-038
5:   3.44353309e-086 4.42417126e-062]
5:  [7.10944485e-038 2.90195956e-057 3.31695976e-076 8.94213159e+130
5:   1.08621718e-042 4.90942075e-062 2.62395837e+179 1.96567189e-062
5:   2.62522473e+179 1.65717382e-047]
5:  [1.65690032e-047 3.76050095e+179 1.28446940e-057 3.43442555e+179
5:   2.90925473e-033 2.78859388e+179 3.67247049e-062 5.08076005e-067
5:   4.85636740e-033 1.68720913e-066]
5:  [3.59651350e+179 3.67104607e-062 2.08251660e-056 3.11088276e+179
5:   2.31983368e-052 8.44249653e-053 2.95004375e+179 3.57281160e-062
5:   2.90850242e-033 3.57280601e-062]
5:  [1.88149830e-071 3.27232023e+179 2.95130767e+179 2.79540547e-076
5:   1.39642638e-076 8.94213159e+130 9.21534464e-062 7.39991510e-038
5:   3.43949822e+179 7.10944485e-038]
5:  [5.05460330e-038 3.86667274e-047 5.34735575e-038 9.14388683e+140
5:   2.73887101e-057 5.47711580e-057 2.40523451e-057 8.16287964e-043
5:   7.39987884e-038 1.21732911e-037]
5:  [8.62028874e-043 3.11932525e-033 7.71300305e+140 3.53977069e-057
5:   3.69402731e-033 1.90221577e-052 3.31186189e-033 1.39803697e-076
5:   6.22045093e-033 3.57280048e-062]
5:  [2.62395837e+179 7.11331116e-067 3.92640282e+179 4.55989505e-047
5:   1.65717382e-047 3.92639532e+179 1.28446940e-057 3.76367425e+179
5:   5.40135499e-062 2.62395837e+179]]
5: 
5: +inf location mismatch:
5:  ACTUAL: array([[5.381628e-104, 5.366539e-104,           inf, 1.550032e-123,
5:                   inf, 1.399372e-152, 5.381636e-104,           inf,
5:         5.381628e-104, 4.557225e-133],...
5:  DESIRED: array([[2.319834e-052, 2.316579e-052, 3.110878e+179, 3.937045e-062,
5:         2.623981e+179, 1.182951e-076, 2.319835e-052, 2.624592e+179,
5:         2.319834e-052, 6.750722e-067],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestElementwiseMulOp_xsize_lessthan_ysize)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 57, in test_check_grad_normal
5:     self.check_grad(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3028, in check_grad_with_place
5:     prim_grad_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 890, in check
5:     self.check_static_comp()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/prim_op_test.py", line 1159, in check_static_comp
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-15, atol=1e-15
5: Check static comp grad out failed. Mismatch between static comp and eager on Place(cpu), when enable_fw_comp is True,enable_rev_comp is True,the forward api out tensor's index is : 0 
5: static comp grad out tensor:
5: [[3.50527829e-033 1.10510898e+141 1.58554837e-047 3.27677221e+179
5:   1.96900091e-062 3.43568935e+179 6.77451242e-043 3.43441805e+179
5:   9.06148984e-043 4.08657150e+179]
5:  [5.64926960e-038 1.97209278e-062 2.58334717e-057 5.37256920e-052
5:   7.85913746e-067 1.65529455e-047 1.45189403e-047 9.05269869e-043
5:   1.08570346e-042 4.46662990e-033]
5:  [3.11592827e+179 9.50111420e-043 3.92321465e+179 4.46662990e-033
5:   6.60479197e-033 3.11467183e+179 6.77013337e-043 3.92575482e+179
5:   1.03317402e-047 4.08530517e+179]
5:  [3.27486772e+179 3.43441069e+179 2.62523713e+179 2.34798882e-047
5:   3.27486772e+179 9.49765016e-043 3.92194833e+179 1.74653411e-076
5:   7.40560998e-038 4.46662990e-033]
5:  [7.26970385e-043 8.67496683e+140 4.43643275e-038 2.62587775e+179
5:   4.08372089e-033 3.11149608e+179 2.90782421e-033 7.68689628e+179
5:   1.35439357e-042 7.03536121e+179]
5:  [2.74012541e-057 7.69919362e-043 5.35309995e-038 5.35309995e-038
5:   2.13993785e-047 4.08657151e+179 3.93257229e-062 2.62839058e+179
5:   5.63890302e-038 2.78796072e+179]
5:  [2.94942548e+179 3.43757902e+179 3.43948589e+179 5.88838405e-062
5:   4.76418255e-038 2.63262612e-052 9.94968169e-043 2.57719664e-057
5:   6.81559368e-038 2.38210692e-038]
5:  [5.64351177e-038 1.55887477e-033 1.38240737e-047 1.31324539e-042
5:   8.60959137e-043 6.22670312e-038 3.69852312e-033 7.25416364e-043
5:   3.11848723e-033 1.10689755e-047]
5:  [8.94213087e+130 7.41250199e-038 8.11549968e-062 7.69907593e-043
5:   3.93248995e-062 3.18858605e-037 1.08604065e-042 5.64588946e-038
5:   7.86206826e-067 3.63216558e-043]
5:  [4.58459039e-076 9.04924834e-043 1.10952229e-047 5.64349841e-038
5:   4.01644637e-057 1.17554052e-047 6.01757907e-067 4.56347894e-072
5:   1.33522284e-037 8.45274804e-053]]
5:  eager grad out tensor:
5: [[4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]
5:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4.]]
5: 
5: Mismatched elements: 100 / 100 (100%)
5: Max absolute difference among violations: 7.68689628e+179
5: Max relative difference among violations: 1.92172407e+179
5:  ACTUAL: array([[3.505278e-033, 1.105109e+141, 1.585548e-047, 3.276772e+179,
5:         1.969001e-062, 3.435689e+179, 6.774512e-043, 3.434418e+179,
5:         9.061490e-043, 4.086572e+179],...
5:  DESIRED: array([[4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],
5:        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],
5:        [4., 4., 4., 4., 4., 4., 4., 4., 4., 4.],...
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestElementwiseMulOp_xsize_lessthan_ysize)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 50, in test_check_output
5:     self.check_output(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 779, in assert_array_compare
5:     flagged |= func_assert_same_pos(x, y,
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 745, in func_assert_same_pos
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=0
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: +inf location mismatch:
5:  ACTUAL: array([[[[0.000000e+000, 0.000000e+000, 0.000000e+000, 0.000000e+000,
5:           5.166620e+117, 4.064251e+103, 1.571575e-094,           inf,
5:           2.102115e-094, 2.758731e+113],...
5:  DESIRED: array([[[[2.178994e-01, 1.194586e-02, 6.852536e-02, 4.441244e-01,
5:           4.273088e-01, 3.985301e-01, 1.997818e-02, 2.010756e-01,
5:           7.018167e-01, 2.638406e-01],...
5: 
5: ======================================================================
5: FAIL: test_dygraph_mul (test_elementwise_mul_op.TestElementwiseMulop)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 575, in test_dygraph_mul
5:     np.testing.assert_allclose(actual_out, expect_out)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-07, atol=0
5: 
5: Mismatched elements: 24 / 24 (100%)
5: Max absolute difference among violations: 0.7814745
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[[0., 0., 0., 0.],
5:         [0., 0., 0., 0.],
5:         [0., 0., 0., 0.]],...
5:  DESIRED: array([[[0.781474, 0.283227, 0.240341, 0.270227],
5:         [0.661287, 0.157867, 0.066603, 0.2337  ],
5:         [0.106551, 0.047051, 0.300761, 0.656515]],...
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_x (test_elementwise_mul_op.TestRealComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 544, in test_check_grad_ingore_x
5:     self.check_grad(['Y'], 'Out', no_grad_set=set("X"), check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(6.906542818741903e+148) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable Y (shape: (2, 3, 4, 5), dtype: complex128) max gradient diff 6.906543e+148 over limit 5.000000e-03, the first error element is 8, expected 0.000000e+00, but got 0.000000e+00.
5: 
5: ======================================================================
5: FAIL: test_check_grad_ingore_y (test_elementwise_mul_op.TestRealComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 547, in test_check_grad_ingore_y
5:     self.check_grad(['X'], 'Out', no_grad_set=set('Y'), check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(3.849851935406858e+279) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 3, 4, 5), dtype: complex128) max gradient diff 3.849852e+279 over limit 5.000000e-03, the first error element is 70, expected 0.000000e+00, but got 6.830755e+245.
5: 
5: ======================================================================
5: FAIL: test_check_grad_normal (test_elementwise_mul_op.TestRealComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 541, in test_check_grad_normal
5:     self.check_grad(['X', 'Y'], 'Out', check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2969, in check_grad
5:     self.check_grad_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 3242, in check_grad_with_place
5:     self._assert_is_close(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2926, in _assert_is_close
5:     self.assertLessEqual(max_diff, max_relative_error, err_msg())
5: AssertionError: np.float64(3.849851790831488e+279) not less than or equal to 0.005 : Operator elementwise_mul error, Gradient Check On Place(cpu) variable X (shape: (2, 3, 4, 5), dtype: complex128) max gradient diff 3.849852e+279 over limit 5.000000e-03, the first error element is 16, expected 0.000000e+00, but got 2.949106e+148.
5: 
5: ======================================================================
5: FAIL: test_check_output (test_elementwise_mul_op.TestRealComplexElementwiseMulOp)
5: ----------------------------------------------------------------------
5: Traceback (most recent call last):
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/test_elementwise_mul_op.py", line 538, in test_check_output
5:     self.check_output(check_pir=True)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2763, in check_output
5:     res = self.check_output_with_place(
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2622, in check_output_with_place
5:     static_checker.check()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2193, in check
5:     self.compare_outputs_with_expects()
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2183, in compare_outputs_with_expects
5:     self.compare_single_output_with_expect(out_name, expect)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2161, in compare_single_output_with_expect
5:     self._compare_numpy(name, actual_np, expect_np)
5:   File "/home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/Paddle/test/legacy_test/op_test.py", line 2124, in _compare_numpy
5:     np.testing.assert_allclose(
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 1684, in assert_allclose
5:     assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
5:   File "/usr/lib/python3.10/contextlib.py", line 79, in inner
5:     return func(*args, **kwds)
5:   File "/home/mlx/.local/lib/python3.10/site-packages/numpy/testing/_private/utils.py", line 885, in assert_array_compare
5:     raise AssertionError(msg)
5: AssertionError: 
5: Not equal to tolerance rtol=1e-05, atol=1e-05
5: Operator (elementwise_mul) Output (Out) has diff at Place(cpu) in static checker
5: Mismatched elements: 120 / 120 (100%)
5: Max absolute difference among violations: 1.25101542
5: Max relative difference among violations: 1.
5:  ACTUAL: array([[[[ 3.674155e-053-3.674158e-053j,  1.925557e-158-9.863492e-157j,
5:            0.000000e+000-1.829743e-144j,  0.000000e+000-1.399026e-163j,
5:            1.657919e-144-2.368456e-145j],...
5:  DESIRED: array([[[[1.352085e-01+7.466591e-02j, 9.368077e-02+1.152415e-01j,
5:           6.566728e-01+1.245357e-02j, 5.151537e-02+4.055074e-02j,
5:           3.467623e-01+4.028211e-01j],...
5: 
5: ----------------------------------------------------------------------
5: Ran 85 tests in 10.235s
5: 
5: FAILED (failures=74, skipped=8)
5: 
1/1 Test #5: test_elementwise_mul_op ..........***Failed   13.49 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =  13.50 sec

The following tests FAILED:
	  5 - test_elementwise_mul_op (Failed)
Errors while running CTest
Output from these tests are in: /home/mlx/repos/paddle-onednn-dev/paddlecustomdevice-onednn/backends/sycl/build/Testing/Temporary/LastTest.log
Use "--rerun-failed --output-on-failure" to re-run the failed cases verbosely.
